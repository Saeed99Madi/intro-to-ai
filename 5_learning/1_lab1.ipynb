{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Introduction to Machine Learning\n",
    "\n",
    "Welcome to your first machine learning lab! In this lab, we'll explore the fundamentals of machine learning, starting with one of the simplest yet most powerful algorithms: linear regression.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will:\n",
    "- Understand the basic types of machine learning\n",
    "- Implement linear regression from scratch\n",
    "- Learn about gradient descent optimization\n",
    "- Explore polynomial regression and feature engineering\n",
    "- Use scikit-learn for practical ML tasks\n",
    "- Build a housing price prediction model\n",
    "\n",
    "## What is Machine Learning?\n",
    "\n",
    "Machine learning is the field of study that gives computers the ability to learn without being explicitly programmed. Instead of writing rules, we provide data and let algorithms discover patterns.\n",
    "\n",
    "### Types of Machine Learning\n",
    "\n",
    "1. **Supervised Learning**: Learning from labeled examples\n",
    "   - Regression: Predicting continuous values\n",
    "   - Classification: Predicting discrete categories\n",
    "\n",
    "2. **Unsupervised Learning**: Finding patterns in unlabeled data\n",
    "   - Clustering: Grouping similar examples\n",
    "   - Dimensionality reduction: Simplifying data\n",
    "\n",
    "3. **Reinforcement Learning**: Learning through interaction\n",
    "   - Agent learns by trial and error\n",
    "   - Maximizes cumulative reward\n",
    "\n",
    "This week focuses on **supervised learning**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple, List\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_regression, fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression as SKLinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Linear Regression from Scratch\n",
    "\n",
    "Linear regression finds the best-fitting straight line through data points. The model is:\n",
    "\n",
    "$$y = w_0 + w_1 x_1 + w_2 x_2 + ... + w_n x_n$$\n",
    "\n",
    "Or in vector notation: $y = \\mathbf{w}^T \\mathbf{x} + b$\n",
    "\n",
    "Where:\n",
    "- $y$ is the predicted output\n",
    "- $\\mathbf{x}$ are the input features\n",
    "- $\\mathbf{w}$ are the weights (parameters)\n",
    "- $b$ is the bias (intercept)\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "We measure prediction error using Mean Squared Error (MSE):\n",
    "\n",
    "$$MSE = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "Where $m$ is the number of training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    \"\"\"\n",
    "    Linear Regression using Gradient Descent.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    learning_rate : float\n",
    "        Step size for gradient descent\n",
    "    n_iterations : int\n",
    "        Number of training iterations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate: float = 0.01, n_iterations: int = 1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.loss_history = []\n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        Train the linear regression model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : np.ndarray, shape (m, n)\n",
    "            Training features\n",
    "        y : np.ndarray, shape (m,)\n",
    "            Target values\n",
    "        \"\"\"\n",
    "        m, n = X.shape\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.weights = np.zeros(n)\n",
    "        self.bias = 0\n",
    "        \n",
    "        # Gradient descent\n",
    "        for i in range(self.n_iterations):\n",
    "            # Forward pass\n",
    "            y_pred = self.predict(X)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = np.mean((y - y_pred) ** 2)\n",
    "            self.loss_history.append(loss)\n",
    "            \n",
    "            # Compute gradients\n",
    "            dw = -(2/m) * X.T.dot(y - y_pred)\n",
    "            db = -(2/m) * np.sum(y - y_pred)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Make predictions.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : np.ndarray, shape (m, n)\n",
    "            Input features\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        y_pred : np.ndarray, shape (m,)\n",
    "            Predictions\n",
    "        \"\"\"\n",
    "        return X.dot(self.weights) + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Linear Regression\n",
    "\n",
    "Let's test our implementation on synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X_train, y_train = make_regression(n_samples=100, n_features=1, noise=20, random_state=42)\n",
    "X_test, y_test = make_regression(n_samples=30, n_features=1, noise=20, random_state=43)\n",
    "\n",
    "# Train model\n",
    "model = LinearRegression(learning_rate=0.01, n_iterations=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Data and fit\n",
    "axes[0].scatter(X_train, y_train, alpha=0.6, label='Training data')\n",
    "axes[0].scatter(X_test, y_test, alpha=0.6, color='orange', label='Test data')\n",
    "axes[0].plot(X_train, y_pred_train, 'r-', label='Fitted line', linewidth=2)\n",
    "axes[0].set_xlabel('X')\n",
    "axes[0].set_ylabel('y')\n",
    "axes[0].set_title('Linear Regression Fit')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Loss history\n",
    "axes[1].plot(model.loss_history)\n",
    "axes[1].set_xlabel('Iteration')\n",
    "axes[1].set_ylabel('MSE Loss')\n",
    "axes[1].set_title('Training Loss Over Time')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Learned weights: {model.weights}\")\n",
    "print(f\"Learned bias: {model.bias:.2f}\")\n",
    "print(f\"\\nTraining MSE: {mean_squared_error(y_train, y_pred_train):.2f}\")\n",
    "print(f\"Test MSE: {mean_squared_error(y_test, y_pred_test):.2f}\")\n",
    "print(f\"Test RÂ² Score: {r2_score(y_test, y_pred_test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Closed-Form Solution\n",
    "\n",
    "Linear regression has a closed-form solution called the **Normal Equation**:\n",
    "\n",
    "$$\\mathbf{w} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}$$\n",
    "\n",
    "This directly computes the optimal weights without iteration.\n",
    "\n",
    "### Gradient Descent vs Normal Equation\n",
    "\n",
    "| Aspect | Gradient Descent | Normal Equation |\n",
    "|--------|-----------------|------------------|\n",
    "| Speed | Slow for large datasets | Fast for small datasets |\n",
    "| Memory | Low | High (needs to compute $X^T X$) |\n",
    "| Features | Works with many features | Slow with many features |\n",
    "| Generalization | Works for other models | Only for linear regression |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionNormalEq:\n",
    "    \"\"\"\n",
    "    Linear Regression using Normal Equation (closed-form solution).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        Train using normal equation.\n",
    "        \"\"\"\n",
    "        # Add bias term (column of ones)\n",
    "        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        \n",
    "        # Normal equation: theta = (X^T X)^-1 X^T y\n",
    "        theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "        \n",
    "        self.bias = theta[0]\n",
    "        self.weights = theta[1:]\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Make predictions.\n",
    "        \"\"\"\n",
    "        return X.dot(self.weights) + self.bias\n",
    "\n",
    "# Compare with gradient descent\n",
    "model_normal = LinearRegressionNormalEq()\n",
    "model_normal.fit(X_train, y_train)\n",
    "y_pred_normal = model_normal.predict(X_test)\n",
    "\n",
    "print(\"Normal Equation Results:\")\n",
    "print(f\"Weights: {model_normal.weights}\")\n",
    "print(f\"Bias: {model_normal.bias:.2f}\")\n",
    "print(f\"Test MSE: {mean_squared_error(y_test, y_pred_normal):.2f}\")\n",
    "print(f\"Test RÂ² Score: {r2_score(y_test, y_pred_normal):.3f}\")\n",
    "\n",
    "print(\"\\nGradient Descent Results:\")\n",
    "print(f\"Weights: {model.weights}\")\n",
    "print(f\"Bias: {model.bias:.2f}\")\n",
    "print(f\"Test MSE: {mean_squared_error(y_test, y_pred_test):.2f}\")\n",
    "print(f\"Test RÂ² Score: {r2_score(y_test, y_pred_test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Polynomial Regression\n",
    "\n",
    "Sometimes data doesn't follow a straight line. We can fit non-linear patterns by using **polynomial features**.\n",
    "\n",
    "For example, with $x$, we can create:\n",
    "- Linear: $y = w_0 + w_1 x$\n",
    "- Quadratic: $y = w_0 + w_1 x + w_2 x^2$\n",
    "- Cubic: $y = w_0 + w_1 x + w_2 x^2 + w_3 x^3$\n",
    "\n",
    "This is still linear regression (linear in the parameters), but with transformed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_polynomial_features(X: np.ndarray, degree: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create polynomial features up to the specified degree.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : np.ndarray, shape (m, 1)\n",
    "        Input features\n",
    "    degree : int\n",
    "        Maximum polynomial degree\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    X_poly : np.ndarray, shape (m, degree)\n",
    "        Polynomial features [x, x^2, x^3, ..., x^degree]\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    X_poly = np.zeros((m, degree))\n",
    "    \n",
    "    for i in range(1, degree + 1):\n",
    "        X_poly[:, i-1] = (X[:, 0] ** i)\n",
    "    \n",
    "    return X_poly\n",
    "\n",
    "# Generate non-linear data\n",
    "np.random.seed(42)\n",
    "X_nonlinear = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
    "y_nonlinear = 0.5 * X_nonlinear**3 - 2 * X_nonlinear**2 + X_nonlinear + 5 + np.random.randn(100, 1) * 3\n",
    "y_nonlinear = y_nonlinear.ravel()\n",
    "\n",
    "# Fit models with different polynomial degrees\n",
    "degrees = [1, 2, 3, 5, 10]\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, degree in enumerate(degrees):\n",
    "    # Create polynomial features\n",
    "    X_poly = create_polynomial_features(X_nonlinear, degree)\n",
    "    \n",
    "    # Fit model\n",
    "    model_poly = LinearRegressionNormalEq()\n",
    "    model_poly.fit(X_poly, y_nonlinear)\n",
    "    y_pred_poly = model_poly.predict(X_poly)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_nonlinear, y_pred_poly)\n",
    "    r2 = r2_score(y_nonlinear, y_pred_poly)\n",
    "    \n",
    "    # Plot\n",
    "    axes[idx].scatter(X_nonlinear, y_nonlinear, alpha=0.5, s=20)\n",
    "    axes[idx].plot(X_nonlinear, y_pred_poly, 'r-', linewidth=2)\n",
    "    axes[idx].set_title(f'Degree {degree}\\nMSE: {mse:.2f}, RÂ²: {r2:.3f}')\n",
    "    axes[idx].set_xlabel('X')\n",
    "    axes[idx].set_ylabel('y')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "# Remove extra subplot\n",
    "fig.delaxes(axes[-1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice how higher-degree polynomials fit the training data better,\")\n",
    "print(\"but may overfit (especially degree 10, which wiggles too much).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Feature Scaling\n",
    "\n",
    "When features have different scales (e.g., age in years vs income in dollars), gradient descent can be slow. Feature scaling helps:\n",
    "\n",
    "1. **Standardization (Z-score normalization)**:\n",
    "   $$x' = \\frac{x - \\mu}{\\sigma}$$\n",
    "   \n",
    "2. **Min-Max Normalization**:\n",
    "   $$x' = \\frac{x - x_{min}}{x_{max} - x_{min}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureScaler:\n",
    "    \"\"\"\n",
    "    Standardize features by removing mean and scaling to unit variance.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "    \n",
    "    def fit(self, X: np.ndarray):\n",
    "        \"\"\"\n",
    "        Compute mean and standard deviation.\n",
    "        \"\"\"\n",
    "        self.mean = np.mean(X, axis=0)\n",
    "        self.std = np.std(X, axis=0)\n",
    "    \n",
    "    def transform(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Standardize features.\n",
    "        \"\"\"\n",
    "        return (X - self.mean) / (self.std + 1e-8)  # Add epsilon to avoid division by zero\n",
    "    \n",
    "    def fit_transform(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Fit and transform in one step.\n",
    "        \"\"\"\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "# Generate data with different scales\n",
    "np.random.seed(42)\n",
    "X_unscaled = np.random.randn(100, 2)\n",
    "X_unscaled[:, 0] = X_unscaled[:, 0] * 1000  # Large scale\n",
    "X_unscaled[:, 1] = X_unscaled[:, 1] * 0.01  # Small scale\n",
    "y = 3 * X_unscaled[:, 0] + 5 * X_unscaled[:, 1] + np.random.randn(100) * 10\n",
    "\n",
    "# Train without scaling\n",
    "model_unscaled = LinearRegression(learning_rate=0.00001, n_iterations=1000)\n",
    "model_unscaled.fit(X_unscaled, y)\n",
    "\n",
    "# Train with scaling\n",
    "scaler = FeatureScaler()\n",
    "X_scaled = scaler.fit_transform(X_unscaled)\n",
    "model_scaled = LinearRegression(learning_rate=0.01, n_iterations=1000)\n",
    "model_scaled.fit(X_scaled, y)\n",
    "\n",
    "# Compare convergence\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(model_unscaled.loss_history)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Without Feature Scaling\\n(Learning Rate: 0.00001)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(model_scaled.loss_history)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('With Feature Scaling\\n(Learning Rate: 0.01)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Feature scaling allows us to use a larger learning rate and converge faster!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Using Scikit-Learn\n",
    "\n",
    "While implementing algorithms from scratch is educational, in practice we use well-tested libraries like scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Generate data\n",
    "np.random.seed(42)\n",
    "X = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
    "y = 0.5 * X**2 + X + 2 + np.random.randn(100, 1) * 0.5\n",
    "y = y.ravel()\n",
    "\n",
    "# Create a pipeline\n",
    "model_pipeline = Pipeline([\n",
    "    ('poly_features', PolynomialFeatures(degree=2)),\n",
    "    ('linear_regression', SKLinearRegression())\n",
    "])\n",
    "\n",
    "# Fit and predict\n",
    "model_pipeline.fit(X, y)\n",
    "y_pred = model_pipeline.predict(X)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, alpha=0.6, label='Data')\n",
    "plt.plot(X, y_pred, 'r-', linewidth=2, label='Polynomial Fit')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Polynomial Regression with Scikit-Learn')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"RÂ² Score: {r2_score(y, y_pred):.3f}\")\n",
    "print(f\"MSE: {mean_squared_error(y, y_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Real-World Application - Housing Price Prediction\n",
    "\n",
    "Let's apply what we've learned to predict housing prices using the California Housing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load California housing dataset\n",
    "housing = fetch_california_housing()\n",
    "X_housing = housing.data\n",
    "y_housing = housing.target\n",
    "feature_names = housing.feature_names\n",
    "\n",
    "print(\"Dataset Information:\")\n",
    "print(f\"Number of samples: {X_housing.shape[0]}\")\n",
    "print(f\"Number of features: {X_housing.shape[1]}\")\n",
    "print(f\"\\nFeatures: {feature_names}\")\n",
    "print(f\"\\nTarget: Median house value (in $100,000s)\")\n",
    "\n",
    "# Create DataFrame for easier exploration\n",
    "df = pd.DataFrame(X_housing, columns=feature_names)\n",
    "df['MedianHouseValue'] = y_housing\n",
    "\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nStatistical summary:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature distributions\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(feature_names):\n",
    "    axes[idx].hist(df[col], bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[idx].set_title(col)\n",
    "    axes[idx].set_xlabel('Value')\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "# Target distribution\n",
    "axes[-1].hist(y_housing, bins=50, edgecolor='black', alpha=0.7, color='red')\n",
    "axes[-1].set_title('MedianHouseValue')\n",
    "axes[-1].set_xlabel('Value ($100k)')\n",
    "axes[-1].set_ylabel('Frequency')\n",
    "axes[-1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature correlation with target\n",
    "correlations = df.corr()['MedianHouseValue'].sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "correlations[1:].plot(kind='barh')\n",
    "plt.xlabel('Correlation with Median House Value')\n",
    "plt.title('Feature Correlations')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Correlations with target:\")\n",
    "print(correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_housing, y_housing, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train model\n",
    "model_housing = SKLinearRegression()\n",
    "model_housing.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = model_housing.predict(X_train_scaled)\n",
    "y_test_pred = model_housing.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Model Performance:\")\n",
    "print(f\"\\nTraining Set:\")\n",
    "print(f\"  MSE: {mean_squared_error(y_train, y_train_pred):.3f}\")\n",
    "print(f\"  RMSE: {np.sqrt(mean_squared_error(y_train, y_train_pred)):.3f}\")\n",
    "print(f\"  RÂ² Score: {r2_score(y_train, y_train_pred):.3f}\")\n",
    "\n",
    "print(f\"\\nTest Set:\")\n",
    "print(f\"  MSE: {mean_squared_error(y_test, y_test_pred):.3f}\")\n",
    "print(f\"  RMSE: {np.sqrt(mean_squared_error(y_test, y_test_pred)):.3f}\")\n",
    "print(f\"  RÂ² Score: {r2_score(y_test, y_test_pred):.3f}\")\n",
    "\n",
    "print(\"\\nNote: RMSE is in units of $100k, so 0.7 means ~$70,000 average error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Scatter plot: Actual vs Predicted\n",
    "axes[0].scatter(y_test, y_test_pred, alpha=0.5)\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[0].set_xlabel('Actual Price ($100k)')\n",
    "axes[0].set_ylabel('Predicted Price ($100k)')\n",
    "axes[0].set_title('Actual vs Predicted Prices')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residual plot\n",
    "residuals = y_test - y_test_pred\n",
    "axes[1].scatter(y_test_pred, residuals, alpha=0.5)\n",
    "axes[1].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "axes[1].set_xlabel('Predicted Price ($100k)')\n",
    "axes[1].set_ylabel('Residuals')\n",
    "axes[1].set_title('Residual Plot')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance (coefficients)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'coefficient': model_housing.coef_\n",
    "}).sort_values('coefficient', key=abs, ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['feature'], feature_importance['coefficient'])\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.title('Feature Importance (Linear Regression Coefficients)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Feature Importance:\")\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Linear regression** models linear relationships between features and targets\n",
    "2. **Gradient descent** iteratively updates parameters to minimize loss\n",
    "3. **Normal equation** provides a direct solution but is computationally expensive for large datasets\n",
    "4. **Polynomial features** allow linear models to fit non-linear patterns\n",
    "5. **Feature scaling** is crucial for gradient descent convergence\n",
    "6. **Evaluation metrics** (MSE, RMSE, RÂ²) help assess model performance\n",
    "7. Real-world datasets require **preprocessing** and **feature engineering**\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. **Learning Rate Experiment**: Try different learning rates (0.001, 0.01, 0.1, 1.0) and observe convergence. What happens when the learning rate is too large?\n",
    "\n",
    "2. **Polynomial Degree Selection**: For the non-linear data, which polynomial degree gives the best test performance? How do you detect overfitting?\n",
    "\n",
    "3. **Feature Engineering**: Create new features for the housing dataset (e.g., rooms_per_household = AveRooms / AveOccup). Does this improve performance?\n",
    "\n",
    "4. **Regularization Preview**: Add L2 regularization to prevent overfitting:\n",
    "   - Modify the loss: $MSE + \\lambda \\sum w_i^2$\n",
    "   - Update the gradient: $dw = -(2/m) X^T(y - \\hat{y}) + 2\\lambda w$\n",
    "\n",
    "5. **Mini-batch Gradient Descent**: Implement mini-batch GD that updates weights using random subsets of data instead of the full dataset.\n",
    "\n",
    "6. **Multi-output Regression**: Extend LinearRegression to predict multiple targets simultaneously.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next lab, we'll explore:\n",
    "- Classification algorithms (Logistic Regression, k-NN, Decision Trees)\n",
    "- Different types of supervised learning problems\n",
    "- More complex decision boundaries\n",
    "- Algorithm comparison and selection\n",
    "\n",
    "Great job completing Lab 1! You now understand the fundamentals of machine learning and regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
