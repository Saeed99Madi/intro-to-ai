{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Model Evaluation and Validation\n",
    "\n",
    "Building a model is only half the battle. Evaluating its performance properly is crucial for understanding whether it will work well on new, unseen data.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will:\n",
    "- Use various evaluation metrics for classification and regression\n",
    "- Understand and implement cross-validation\n",
    "- Recognize and address overfitting and underfitting\n",
    "- Apply regularization techniques\n",
    "- Tune hyperparameters systematically\n",
    "- Use learning curves to diagnose model problems\n",
    "- Build robust, well-validated models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from typing import Tuple, List\n",
    "from sklearn.datasets import load_breast_cancer, make_classification, fetch_california_housing\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, cross_val_score, KFold, StratifiedKFold,\n",
    "    GridSearchCV, RandomizedSearchCV, learning_curve, validation_curve\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_curve, roc_auc_score,\n",
    "    mean_squared_error, mean_absolute_error, r2_score\n",
    ")\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Classification Metrics\n",
    "\n",
    "Accuracy is not always the best metric. Let's explore various classification metrics.\n",
    "\n",
    "### Confusion Matrix\n",
    "\n",
    "|                | Predicted Positive | Predicted Negative |\n",
    "|----------------|-------------------|-------------------|\n",
    "| **Actual Positive** | True Positive (TP) | False Negative (FN) |\n",
    "| **Actual Negative** | False Positive (FP) | True Negative (TN) |\n",
    "\n",
    "### Key Metrics\n",
    "\n",
    "- **Accuracy**: $\\frac{TP + TN}{TP + TN + FP + FN}$ - Overall correctness\n",
    "- **Precision**: $\\frac{TP}{TP + FP}$ - Of predicted positives, how many are correct?\n",
    "- **Recall (Sensitivity)**: $\\frac{TP}{TP + FN}$ - Of actual positives, how many did we find?\n",
    "- **F1-Score**: $2 \\times \\frac{Precision \\times Recall}{Precision + Recall}$ - Harmonic mean of precision and recall\n",
    "- **Specificity**: $\\frac{TN}{TN + FP}$ - Of actual negatives, how many did we correctly identify?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split and scale\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train model\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "print(\"Classification Metrics:\")\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred):.3f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred):.3f}\")\n",
    "print(f\"Recall:    {recall_score(y_test, y_pred):.3f}\")\n",
    "print(f\"F1-Score:  {f1_score(y_test, y_pred):.3f}\")\n",
    "print(f\"ROC-AUC:   {roc_auc_score(y_test, y_proba):.3f}\")\n",
    "\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=data.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Confusion matrix\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "           xticklabels=data.target_names, yticklabels=data.target_names)\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_title('Confusion Matrix')\n",
    "\n",
    "# Normalized confusion matrix\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues', ax=axes[1],\n",
    "           xticklabels=data.target_names, yticklabels=data.target_names)\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "axes[1].set_title('Normalized Confusion Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve and AUC\n",
    "\n",
    "The **ROC (Receiver Operating Characteristic)** curve plots:\n",
    "- True Positive Rate (Recall) on y-axis\n",
    "- False Positive Rate on x-axis\n",
    "\n",
    "**AUC (Area Under Curve)** measures the entire area under the ROC curve:\n",
    "- AUC = 1.0: Perfect classifier\n",
    "- AUC = 0.5: Random guessing\n",
    "- AUC < 0.5: Worse than random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, linewidth=2, label=f'ROC Curve (AUC = {auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate (Recall)')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"The closer the curve is to the top-left corner, the better the model.\")\n",
    "print(f\"AUC = {auc:.3f} indicates excellent performance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision-Recall Trade-off\n",
    "\n",
    "By adjusting the classification threshold, we can trade precision for recall:\n",
    "- **High threshold** → Higher precision, lower recall (fewer false positives)\n",
    "- **Low threshold** → Higher recall, lower precision (fewer false negatives)\n",
    "\n",
    "Choose based on the cost of false positives vs false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall trade-off\n",
    "thresholds_test = np.linspace(0, 1, 100)\n",
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "for threshold in thresholds_test:\n",
    "    y_pred_thresh = (y_proba >= threshold).astype(int)\n",
    "    if len(np.unique(y_pred_thresh)) > 1:\n",
    "        precisions.append(precision_score(y_test, y_pred_thresh))\n",
    "        recalls.append(recall_score(y_test, y_pred_thresh))\n",
    "    else:\n",
    "        precisions.append(1.0 if np.unique(y_pred_thresh)[0] == 1 else 0.0)\n",
    "        recalls.append(1.0 if np.unique(y_pred_thresh)[0] == 1 else 0.0)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Precision and Recall vs Threshold\n",
    "axes[0].plot(thresholds_test, precisions, label='Precision', linewidth=2)\n",
    "axes[0].plot(thresholds_test, recalls, label='Recall', linewidth=2)\n",
    "axes[0].axvline(x=0.5, color='k', linestyle='--', alpha=0.5, label='Default threshold')\n",
    "axes[0].set_xlabel('Threshold')\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].set_title('Precision and Recall vs Threshold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Precision-Recall Curve\n",
    "axes[1].plot(recalls, precisions, linewidth=2)\n",
    "axes[1].set_xlabel('Recall')\n",
    "axes[1].set_ylabel('Precision')\n",
    "axes[1].set_title('Precision-Recall Curve')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Cross-Validation\n",
    "\n",
    "A single train-test split can be misleading. **Cross-validation** provides a more robust estimate of model performance.\n",
    "\n",
    "### k-Fold Cross-Validation\n",
    "\n",
    "1. Split data into k folds\n",
    "2. For each fold:\n",
    "   - Train on k-1 folds\n",
    "   - Validate on the remaining fold\n",
    "3. Average the k validation scores\n",
    "\n",
    "### Stratified k-Fold\n",
    "\n",
    "Maintains class distribution in each fold (important for imbalanced datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossValidator:\n",
    "    \"\"\"\n",
    "    Custom k-Fold Cross-Validation implementation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_splits: int = 5):\n",
    "        self.n_splits = n_splits\n",
    "    \n",
    "    def split(self, X: np.ndarray) -> List[Tuple[np.ndarray, np.ndarray]]:\n",
    "        \"\"\"\n",
    "        Generate train/test indices for k-fold CV.\n",
    "        \"\"\"\n",
    "        n_samples = len(X)\n",
    "        indices = np.arange(n_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        fold_size = n_samples // self.n_splits\n",
    "        \n",
    "        for i in range(self.n_splits):\n",
    "            start = i * fold_size\n",
    "            end = start + fold_size if i < self.n_splits - 1 else n_samples\n",
    "            \n",
    "            test_indices = indices[start:end]\n",
    "            train_indices = np.concatenate([indices[:start], indices[end:]])\n",
    "            \n",
    "            yield train_indices, test_indices\n",
    "    \n",
    "    def cross_val_score(self, model, X: np.ndarray, y: np.ndarray) -> List[float]:\n",
    "        \"\"\"\n",
    "        Perform cross-validation and return scores.\n",
    "        \"\"\"\n",
    "        scores = []\n",
    "        \n",
    "        for train_idx, test_idx in self.split(X):\n",
    "            X_train_fold, X_test_fold = X[train_idx], X[test_idx]\n",
    "            y_train_fold, y_test_fold = y[train_idx], y[test_idx]\n",
    "            \n",
    "            # Train model\n",
    "            model.fit(X_train_fold, y_train_fold)\n",
    "            \n",
    "            # Evaluate\n",
    "            y_pred = model.predict(X_test_fold)\n",
    "            score = accuracy_score(y_test_fold, y_pred)\n",
    "            scores.append(score)\n",
    "        \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test custom cross-validation\n",
    "cv = CrossValidator(n_splits=5)\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Scale entire dataset\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "scores = cv.cross_val_score(model, X_scaled, y)\n",
    "\n",
    "print(\"Custom 5-Fold Cross-Validation Scores:\")\n",
    "for i, score in enumerate(scores, 1):\n",
    "    print(f\"Fold {i}: {score:.3f}\")\n",
    "print(f\"\\nMean: {np.mean(scores):.3f} (+/- {np.std(scores):.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with scikit-learn\n",
    "from sklearn.model_selection import cross_val_score as sk_cv_score\n",
    "\n",
    "# Regular k-fold\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores_kfold = sk_cv_score(model, X_scaled, y, cv=kfold, scoring='accuracy')\n",
    "\n",
    "# Stratified k-fold\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores_stratified = sk_cv_score(model, X_scaled, y, cv=stratified_kfold, scoring='accuracy')\n",
    "\n",
    "print(\"Scikit-learn Cross-Validation:\")\n",
    "print(f\"\\nk-Fold CV:\")\n",
    "print(f\"Mean: {scores_kfold.mean():.3f} (+/- {scores_kfold.std():.3f})\")\n",
    "print(f\"\\nStratified k-Fold CV:\")\n",
    "print(f\"Mean: {scores_stratified.mean():.3f} (+/- {scores_stratified.std():.3f})\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "positions = [1, 2]\n",
    "bp = plt.boxplot([scores_kfold, scores_stratified], positions=positions, \n",
    "                 labels=['k-Fold', 'Stratified k-Fold'], patch_artist=True)\n",
    "for patch in bp['boxes']:\n",
    "    patch.set_facecolor('lightblue')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Cross-Validation Comparison')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Overfitting and Underfitting\n",
    "\n",
    "- **Underfitting**: Model is too simple, performs poorly on both training and test data\n",
    "- **Overfitting**: Model is too complex, performs well on training but poorly on test data\n",
    "- **Good fit**: Model generalizes well to unseen data\n",
    "\n",
    "### Bias-Variance Trade-off\n",
    "\n",
    "- **High Bias** (Underfitting): Model makes strong assumptions, misses relevant patterns\n",
    "- **High Variance** (Overfitting): Model is too sensitive to training data, captures noise\n",
    "- **Optimal**: Balance between bias and variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate overfitting with decision trees\n",
    "X, y = make_classification(n_samples=500, n_features=2, n_informative=2, \n",
    "                          n_redundant=0, n_clusters_per_class=1, \n",
    "                          random_state=42, flip_y=0.1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "max_depths = [1, 2, 3, 5, 10, 20]\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for depth in max_depths:\n",
    "    model = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    train_scores.append(model.score(X_train, y_train))\n",
    "    test_scores.append(model.score(X_test, y_test))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(max_depths, train_scores, 'o-', label='Training Score', linewidth=2)\n",
    "plt.plot(max_depths, test_scores, 'o-', label='Test Score', linewidth=2)\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training vs Test Scores (Overfitting Demonstration)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Observations:\")\n",
    "print(\"- Shallow trees (depth 1-2): Underfit - low scores on both sets\")\n",
    "print(\"- Medium trees (depth 3-5): Good fit - balanced performance\")\n",
    "print(\"- Deep trees (depth 10+): Overfit - high training score, lower test score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Regularization\n",
    "\n",
    "Regularization adds a penalty term to prevent overfitting by constraining model complexity.\n",
    "\n",
    "### Ridge Regression (L2 Regularization)\n",
    "\n",
    "$$Loss = MSE + \\alpha \\sum_{i=1}^{n} w_i^2$$\n",
    "\n",
    "- Penalizes large weights\n",
    "- Shrinks coefficients toward zero\n",
    "- All features retained\n",
    "\n",
    "### Lasso Regression (L1 Regularization)\n",
    "\n",
    "$$Loss = MSE + \\alpha \\sum_{i=1}^{n} |w_i|$$\n",
    "\n",
    "- Can set coefficients exactly to zero\n",
    "- Performs feature selection\n",
    "\n",
    "### Elastic Net\n",
    "\n",
    "$$Loss = MSE + \\alpha_1 \\sum_{i=1}^{n} |w_i| + \\alpha_2 \\sum_{i=1}^{n} w_i^2$$\n",
    "\n",
    "- Combines L1 and L2 penalties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load housing data\n",
    "housing = fetch_california_housing()\n",
    "X, y = housing.data, housing.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Test different alpha values\n",
    "alphas = np.logspace(-3, 3, 50)\n",
    "\n",
    "ridge_train_scores = []\n",
    "ridge_test_scores = []\n",
    "lasso_train_scores = []\n",
    "lasso_test_scores = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    # Ridge\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    ridge.fit(X_train_scaled, y_train)\n",
    "    ridge_train_scores.append(ridge.score(X_train_scaled, y_train))\n",
    "    ridge_test_scores.append(ridge.score(X_test_scaled, y_test))\n",
    "    \n",
    "    # Lasso\n",
    "    lasso = Lasso(alpha=alpha, max_iter=5000)\n",
    "    lasso.fit(X_train_scaled, y_train)\n",
    "    lasso_train_scores.append(lasso.score(X_train_scaled, y_train))\n",
    "    lasso_test_scores.append(lasso.score(X_test_scaled, y_test))\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Ridge\n",
    "axes[0].semilogx(alphas, ridge_train_scores, label='Training', linewidth=2)\n",
    "axes[0].semilogx(alphas, ridge_test_scores, label='Test', linewidth=2)\n",
    "axes[0].set_xlabel('Alpha (Regularization Strength)')\n",
    "axes[0].set_ylabel('R² Score')\n",
    "axes[0].set_title('Ridge Regression')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Lasso\n",
    "axes[1].semilogx(alphas, lasso_train_scores, label='Training', linewidth=2)\n",
    "axes[1].semilogx(alphas, lasso_test_scores, label='Test', linewidth=2)\n",
    "axes[1].set_xlabel('Alpha (Regularization Strength)')\n",
    "axes[1].set_ylabel('R² Score')\n",
    "axes[1].set_title('Lasso Regression')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observations:\")\n",
    "print(\"- Low alpha: Little regularization, potential overfitting\")\n",
    "print(\"- High alpha: Strong regularization, potential underfitting\")\n",
    "print(\"- Optimal alpha: Balance between training and test performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare feature selection\n",
    "ridge = Ridge(alpha=1.0)\n",
    "lasso = Lasso(alpha=0.1, max_iter=5000)\n",
    "\n",
    "ridge.fit(X_train_scaled, y_train)\n",
    "lasso.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Compare coefficients\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].barh(housing.feature_names, ridge.coef_)\n",
    "axes[0].set_xlabel('Coefficient Value')\n",
    "axes[0].set_title('Ridge Coefficients (All Non-Zero)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].barh(housing.feature_names, lasso.coef_)\n",
    "axes[1].set_xlabel('Coefficient Value')\n",
    "axes[1].set_title(f'Lasso Coefficients ({np.sum(lasso.coef_ != 0)} Non-Zero)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Feature Selection with Lasso:\")\n",
    "for name, coef in zip(housing.feature_names, lasso.coef_):\n",
    "    if coef != 0:\n",
    "        print(f\"{name:15s}: {coef:8.3f}\")\n",
    "    else:\n",
    "        print(f\"{name:15s}: Eliminated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Hyperparameter Tuning\n",
    "\n",
    "Hyperparameters are settings that control the learning process (not learned from data).\n",
    "\n",
    "### Grid Search\n",
    "- Exhaustively try all combinations\n",
    "- Guaranteed to find best combination in grid\n",
    "- Can be computationally expensive\n",
    "\n",
    "### Random Search\n",
    "- Sample random combinations\n",
    "- Often finds good solutions faster\n",
    "- Better for large parameter spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load classification data\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Grid Search\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    DecisionTreeClassifier(random_state=42),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Grid Search Results:\")\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV Score: {grid_search.best_score_:.3f}\")\n",
    "print(f\"Test Score: {grid_search.score(X_test_scaled, y_test):.3f}\")\n",
    "print(f\"Total combinations tested: {len(grid_search.cv_results_['params'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Search\n",
    "from scipy.stats import randint\n",
    "\n",
    "param_dist = {\n",
    "    'max_depth': randint(3, 20),\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 10)\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    DecisionTreeClassifier(random_state=42),\n",
    "    param_dist,\n",
    "    n_iter=50,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"\\nRandom Search Results:\")\n",
    "print(f\"Best Parameters: {random_search.best_params_}\")\n",
    "print(f\"Best CV Score: {random_search.best_score_:.3f}\")\n",
    "print(f\"Test Score: {random_search.score(X_test_scaled, y_test):.3f}\")\n",
    "print(f\"Combinations tested: {len(random_search.cv_results_['params'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Learning Curves\n",
    "\n",
    "Learning curves show how model performance changes with training set size.\n",
    "\n",
    "**Interpreting Learning Curves:**\n",
    "- **High bias (underfitting)**: Training and validation scores are both low and converge\n",
    "- **High variance (overfitting)**: Large gap between training and validation scores\n",
    "- **Good fit**: Both scores are high and close together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(model, X, y, title=\"Learning Curves\"):\n",
    "    \"\"\"\n",
    "    Plot learning curves for a model.\n",
    "    \"\"\"\n",
    "    train_sizes, train_scores, val_scores = learning_curve(\n",
    "        model, X, y, cv=5, n_jobs=-1,\n",
    "        train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "        scoring='accuracy'\n",
    "    )\n",
    "    \n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    val_mean = np.mean(val_scores, axis=1)\n",
    "    val_std = np.std(val_scores, axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_sizes, train_mean, 'o-', label='Training Score', linewidth=2)\n",
    "    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1)\n",
    "    plt.plot(train_sizes, val_mean, 'o-', label='Validation Score', linewidth=2)\n",
    "    plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1)\n",
    "    plt.xlabel('Training Set Size')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Compare different models\n",
    "models = [\n",
    "    (DecisionTreeClassifier(max_depth=2, random_state=42), \"Shallow Tree (Underfit)\"),\n",
    "    (DecisionTreeClassifier(max_depth=5, random_state=42), \"Medium Tree (Good Fit)\"),\n",
    "    (DecisionTreeClassifier(max_depth=20, random_state=42), \"Deep Tree (Overfit)\")\n",
    "]\n",
    "\n",
    "for model, title in models:\n",
    "    plot_learning_curves(model, X_train_scaled, y_train, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Validation Curves\n",
    "\n",
    "Validation curves show how model performance changes with a single hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation curve for max_depth\n",
    "param_range = range(1, 21)\n",
    "train_scores, val_scores = validation_curve(\n",
    "    DecisionTreeClassifier(random_state=42),\n",
    "    X_train_scaled, y_train,\n",
    "    param_name='max_depth',\n",
    "    param_range=param_range,\n",
    "    cv=5,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "val_mean = np.mean(val_scores, axis=1)\n",
    "val_std = np.std(val_scores, axis=1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(param_range, train_mean, 'o-', label='Training Score', linewidth=2)\n",
    "plt.fill_between(param_range, train_mean - train_std, train_mean + train_std, alpha=0.1)\n",
    "plt.plot(param_range, val_mean, 'o-', label='Validation Score', linewidth=2)\n",
    "plt.fill_between(param_range, val_mean - val_std, val_mean + val_std, alpha=0.1)\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Validation Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "optimal_depth = param_range[np.argmax(val_mean)]\n",
    "print(f\"Optimal max_depth: {optimal_depth}\")\n",
    "print(f\"Best validation score: {val_mean[optimal_depth-1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Choose appropriate metrics** for your problem (accuracy, precision, recall, F1, ROC-AUC)\n",
    "2. **Use cross-validation** for robust performance estimates\n",
    "3. **Watch for overfitting** - training score much higher than validation score\n",
    "4. **Regularization** helps prevent overfitting by constraining model complexity\n",
    "5. **Hyperparameter tuning** is essential for optimal performance\n",
    "6. **Learning curves** diagnose bias-variance issues\n",
    "7. **Validation curves** help choose optimal hyperparameters\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. **Imbalanced Dataset**: Create a dataset with 95% class 0 and 5% class 1. Show why accuracy is misleading. Use precision, recall, and F1 instead.\n",
    "\n",
    "2. **Custom Scoring**: Implement custom scoring functions (e.g., weighted F1, custom cost function).\n",
    "\n",
    "3. **Nested Cross-Validation**: Implement nested CV for unbiased hyperparameter tuning.\n",
    "\n",
    "4. **Regularization Path**: Plot how coefficients change with regularization strength (regularization path).\n",
    "\n",
    "5. **Early Stopping**: Implement early stopping for iterative algorithms to prevent overfitting.\n",
    "\n",
    "6. **Feature Selection**: Compare different feature selection methods (filter, wrapper, embedded).\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In Lab 4, we'll explore:\n",
    "- Ensemble methods that combine multiple models\n",
    "- Random Forests and Gradient Boosting\n",
    "- Advanced techniques for improving model performance\n",
    "\n",
    "Excellent work! You now know how to properly evaluate and validate machine learning models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
