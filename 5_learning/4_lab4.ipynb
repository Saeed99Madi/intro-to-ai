{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Ensemble Methods\n",
    "\n",
    "Ensemble methods combine multiple models to create a more powerful predictor. The principle is simple: many weak learners together can form a strong learner.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will:\n",
    "- Understand ensemble learning principles\n",
    "- Implement bagging and bootstrap aggregating\n",
    "- Build Random Forests from scratch\n",
    "- Understand boosting algorithms (AdaBoost, Gradient Boosting)\n",
    "- Use XGBoost and LightGBM\n",
    "- Apply stacking and blending\n",
    "- Analyze feature importance\n",
    "\n",
    "## Why Ensembles Work\n",
    "\n",
    "**Wisdom of Crowds**: Multiple diverse models make better predictions than any single model.\n",
    "\n",
    "**Key Requirements:**\n",
    "1. Models should be diverse (make different errors)\n",
    "2. Models should perform better than random guessing\n",
    "3. Errors should be relatively independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification, load_breast_cancer, load_wine\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (\n",
    "    BaggingClassifier, RandomForestClassifier, AdaBoostClassifier,\n",
    "    GradientBoostingClassifier, VotingClassifier, StackingClassifier\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Bagging (Bootstrap Aggregating)\n",
    "\n",
    "**Bagging** trains multiple models on different random subsets of the training data (with replacement).\n",
    "\n",
    "### Algorithm:\n",
    "1. Create k bootstrap samples from training data\n",
    "2. Train a model on each bootstrap sample\n",
    "3. For prediction:\n",
    "   - Classification: Majority vote\n",
    "   - Regression: Average predictions\n",
    "\n",
    "**Benefits:**\n",
    "- Reduces variance\n",
    "- Helps prevent overfitting\n",
    "- Particularly effective for high-variance models (e.g., decision trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaggingClassifierScratch:\n",
    "    \"\"\"\n",
    "    Bagging Classifier implemented from scratch.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    base_estimator : object\n",
    "        Base model to use\n",
    "    n_estimators : int\n",
    "        Number of base models\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_estimator, n_estimators: int = 10):\n",
    "        self.base_estimator = base_estimator\n",
    "        self.n_estimators = n_estimators\n",
    "        self.estimators = []\n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        Train bagging ensemble.\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        self.estimators = []\n",
    "        \n",
    "        for _ in range(self.n_estimators):\n",
    "            # Bootstrap sample\n",
    "            indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "            X_bootstrap = X[indices]\n",
    "            y_bootstrap = y[indices]\n",
    "            \n",
    "            # Train model on bootstrap sample\n",
    "            # Import the class to create a new instance\n",
    "            from sklearn.base import clone\n",
    "            estimator = clone(self.base_estimator)\n",
    "            estimator.fit(X_bootstrap, y_bootstrap)\n",
    "            self.estimators.append(estimator)\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict using majority vote.\n",
    "        \"\"\"\n",
    "        predictions = np.array([estimator.predict(X) for estimator in self.estimators])\n",
    "        # Majority vote for each sample\n",
    "        return np.array([Counter(predictions[:, i]).most_common(1)[0][0] \n",
    "                        for i in range(X.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "X, y = make_classification(n_samples=500, n_features=20, n_informative=15,\n",
    "                          n_redundant=5, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Compare single tree vs bagging\n",
    "single_tree = DecisionTreeClassifier(random_state=42)\n",
    "single_tree.fit(X_train, y_train)\n",
    "\n",
    "bagging_scratch = BaggingClassifierScratch(\n",
    "    base_estimator=DecisionTreeClassifier(random_state=42),\n",
    "    n_estimators=10\n",
    ")\n",
    "bagging_scratch.fit(X_train, y_train)\n",
    "\n",
    "bagging_sklearn = BaggingClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(random_state=42),\n",
    "    n_estimators=10,\n",
    "    random_state=42\n",
    ")\n",
    "bagging_sklearn.fit(X_train, y_train)\n",
    "\n",
    "print(\"Single Decision Tree:\")\n",
    "print(f\"Training Accuracy: {single_tree.score(X_train, y_train):.3f}\")\n",
    "print(f\"Test Accuracy: {single_tree.score(X_test, y_test):.3f}\")\n",
    "\n",
    "print(\"\\nBagging (From Scratch):\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, bagging_scratch.predict(X_test)):.3f}\")\n",
    "\n",
    "print(\"\\nBagging (Scikit-learn):\")\n",
    "print(f\"Training Accuracy: {bagging_sklearn.score(X_train, y_train):.3f}\")\n",
    "print(f\"Test Accuracy: {bagging_sklearn.score(X_test, y_test):.3f}\")\n",
    "\n",
    "print(\"\\nBagging reduces overfitting and improves test performance!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of number of estimators\n",
    "n_estimators_range = range(1, 51)\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for n in n_estimators_range:\n",
    "    bagging = BaggingClassifier(\n",
    "        base_estimator=DecisionTreeClassifier(random_state=42),\n",
    "        n_estimators=n,\n",
    "        random_state=42\n",
    "    )\n",
    "    bagging.fit(X_train, y_train)\n",
    "    train_scores.append(bagging.score(X_train, y_train))\n",
    "    test_scores.append(bagging.score(X_test, y_test))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(n_estimators_range, train_scores, label='Training', linewidth=2)\n",
    "plt.plot(n_estimators_range, test_scores, label='Test', linewidth=2)\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Bagging: Effect of Number of Estimators')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Random Forests\n",
    "\n",
    "**Random Forest** is an extension of bagging that adds an additional layer of randomness:\n",
    "- Uses bagging (bootstrap samples)\n",
    "- At each split, considers only a random subset of features\n",
    "\n",
    "### Key Hyperparameters:\n",
    "- `n_estimators`: Number of trees\n",
    "- `max_features`: Number of features to consider for each split\n",
    "- `max_depth`: Maximum depth of trees\n",
    "- `min_samples_split`: Minimum samples required to split\n",
    "\n",
    "### Advantages:\n",
    "- Very high accuracy\n",
    "- Handles high-dimensional data well\n",
    "- Provides feature importance\n",
    "- Resistant to overfitting\n",
    "- Works with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRandomForest:\n",
    "    \"\"\"\n",
    "    Simplified Random Forest implementation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_estimators: int = 10, max_features: str = 'sqrt',\n",
    "                max_depth: int = None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_features = max_features\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "        self.n_features_per_tree = None\n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        Train random forest.\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Determine number of features per split\n",
    "        if self.max_features == 'sqrt':\n",
    "            self.n_features_per_tree = int(np.sqrt(n_features))\n",
    "        elif self.max_features == 'log2':\n",
    "            self.n_features_per_tree = int(np.log2(n_features))\n",
    "        else:\n",
    "            self.n_features_per_tree = n_features\n",
    "        \n",
    "        self.trees = []\n",
    "        \n",
    "        for _ in range(self.n_estimators):\n",
    "            # Bootstrap sample\n",
    "            indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "            X_bootstrap = X[indices]\n",
    "            y_bootstrap = y[indices]\n",
    "            \n",
    "            # Train tree with feature subsampling\n",
    "            tree = DecisionTreeClassifier(\n",
    "                max_depth=self.max_depth,\n",
    "                max_features=self.n_features_per_tree,\n",
    "                random_state=None\n",
    "            )\n",
    "            tree.fit(X_bootstrap, y_bootstrap)\n",
    "            self.trees.append(tree)\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict using majority vote.\n",
    "        \"\"\"\n",
    "        predictions = np.array([tree.predict(X) for tree in self.trees])\n",
    "        return np.array([Counter(predictions[:, i]).most_common(1)[0][0] \n",
    "                        for i in range(X.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                     random_state=42, stratify=y)\n",
    "\n",
    "# Compare implementations\n",
    "rf_scratch = SimpleRandomForest(n_estimators=100, max_features='sqrt', max_depth=10)\n",
    "rf_scratch.fit(X_train, y_train)\n",
    "\n",
    "rf_sklearn = RandomForestClassifier(n_estimators=100, max_features='sqrt',\n",
    "                                   max_depth=10, random_state=42)\n",
    "rf_sklearn.fit(X_train, y_train)\n",
    "\n",
    "print(\"Random Forest (From Scratch):\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, rf_scratch.predict(X_test)):.3f}\")\n",
    "\n",
    "print(\"\\nRandom Forest (Scikit-learn):\")\n",
    "print(f\"Training Accuracy: {rf_sklearn.score(X_train, y_train):.3f}\")\n",
    "print(f\"Test Accuracy: {rf_sklearn.score(X_test, y_test):.3f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, rf_sklearn.predict(X_test), \n",
    "                          target_names=data.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': data.feature_names,\n",
    "    'importance': rf_sklearn.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(feature_importance['feature'][:15], feature_importance['importance'][:15])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 15 Feature Importances (Random Forest)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Boosting\n",
    "\n",
    "**Boosting** trains models sequentially, with each model focusing on the errors of previous models.\n",
    "\n",
    "### AdaBoost (Adaptive Boosting)\n",
    "\n",
    "1. Start with equal weights for all samples\n",
    "2. Train a weak learner\n",
    "3. Increase weights for misclassified samples\n",
    "4. Repeat, giving more weight to models that perform better\n",
    "\n",
    "### Key Idea:\n",
    "- Forces subsequent models to focus on hard-to-classify examples\n",
    "- Reduces bias (unlike bagging which reduces variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost\n",
    "adaboost = AdaBoostClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=1),  # Weak learners (stumps)\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "adaboost.fit(X_train, y_train)\n",
    "\n",
    "print(\"AdaBoost Results:\")\n",
    "print(f\"Training Accuracy: {adaboost.score(X_train, y_train):.3f}\")\n",
    "print(f\"Test Accuracy: {adaboost.score(X_test, y_test):.3f}\")\n",
    "\n",
    "# Effect of number of estimators\n",
    "n_estimators_range = range(1, 101, 5)\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for n in n_estimators_range:\n",
    "    ada = AdaBoostClassifier(\n",
    "        base_estimator=DecisionTreeClassifier(max_depth=1),\n",
    "        n_estimators=n,\n",
    "        random_state=42\n",
    "    )\n",
    "    ada.fit(X_train, y_train)\n",
    "    train_scores.append(ada.score(X_train, y_train))\n",
    "    test_scores.append(ada.score(X_test, y_test))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(n_estimators_range, train_scores, label='Training', linewidth=2)\n",
    "plt.plot(n_estimators_range, test_scores, label='Test', linewidth=2)\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('AdaBoost: Effect of Number of Estimators')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting\n",
    "\n",
    "More sophisticated than AdaBoost:\n",
    "1. Start with a simple model (often just the mean)\n",
    "2. Compute residuals (errors)\n",
    "3. Train next model to predict residuals\n",
    "4. Add predictions to ensemble\n",
    "5. Repeat\n",
    "\n",
    "**Key Hyperparameters:**\n",
    "- `learning_rate`: Shrinks contribution of each tree (lower = more robust, needs more trees)\n",
    "- `n_estimators`: Number of boosting stages\n",
    "- `max_depth`: Tree depth (typically shallow, 3-8)\n",
    "- `subsample`: Fraction of samples to use for each tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting\n",
    "gb = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=42\n",
    ")\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "print(\"Gradient Boosting Results:\")\n",
    "print(f\"Training Accuracy: {gb.score(X_train, y_train):.3f}\")\n",
    "print(f\"Test Accuracy: {gb.score(X_test, y_test):.3f}\")\n",
    "\n",
    "# Learning rate effect\n",
    "learning_rates = [0.01, 0.05, 0.1, 0.2, 0.5]\n",
    "test_scores_lr = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    gb_lr = GradientBoostingClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=lr,\n",
    "        max_depth=3,\n",
    "        random_state=42\n",
    "    )\n",
    "    gb_lr.fit(X_train, y_train)\n",
    "    test_scores_lr.append(gb_lr.score(X_test, y_test))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(learning_rates, test_scores_lr, 'o-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title('Gradient Boosting: Effect of Learning Rate')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: XGBoost and LightGBM\n",
    "\n",
    "Modern, highly optimized implementations of gradient boosting.\n",
    "\n",
    "### XGBoost (Extreme Gradient Boosting)\n",
    "- Regularization to prevent overfitting\n",
    "- Parallel processing\n",
    "- Handling missing values\n",
    "- Tree pruning\n",
    "\n",
    "### LightGBM\n",
    "- Faster training speed\n",
    "- Lower memory usage\n",
    "- Better accuracy\n",
    "- Leaf-wise tree growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import xgboost as xgb\n",
    "    from lightgbm import LGBMClassifier\n",
    "    \n",
    "    # XGBoost\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=3,\n",
    "        random_state=42,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    \n",
    "    # LightGBM\n",
    "    lgbm_model = LGBMClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=3,\n",
    "        random_state=42\n",
    "    )\n",
    "    lgbm_model.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"XGBoost Results:\")\n",
    "    print(f\"Test Accuracy: {xgb_model.score(X_test, y_test):.3f}\")\n",
    "    \n",
    "    print(\"\\nLightGBM Results:\")\n",
    "    print(f\"Test Accuracy: {lgbm_model.score(X_test, y_test):.3f}\")\n",
    "    \n",
    "    # Feature importance comparison\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # XGBoost importance\n",
    "    xgb_importance = pd.DataFrame({\n",
    "        'feature': data.feature_names,\n",
    "        'importance': xgb_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False).head(10)\n",
    "    \n",
    "    axes[0].barh(xgb_importance['feature'], xgb_importance['importance'])\n",
    "    axes[0].set_xlabel('Importance')\n",
    "    axes[0].set_title('XGBoost Feature Importance')\n",
    "    axes[0].invert_yaxis()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # LightGBM importance\n",
    "    lgbm_importance = pd.DataFrame({\n",
    "        'feature': data.feature_names,\n",
    "        'importance': lgbm_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False).head(10)\n",
    "    \n",
    "    axes[1].barh(lgbm_importance['feature'], lgbm_importance['importance'])\n",
    "    axes[1].set_xlabel('Importance')\n",
    "    axes[1].set_title('LightGBM Feature Importance')\n",
    "    axes[1].invert_yaxis()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"XGBoost or LightGBM not installed. Install with:\")\n",
    "    print(\"pip install xgboost lightgbm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Voting and Stacking\n",
    "\n",
    "### Voting Classifier\n",
    "\n",
    "Combines predictions from multiple different algorithms:\n",
    "- **Hard voting**: Majority vote\n",
    "- **Soft voting**: Average probabilities (usually better)\n",
    "\n",
    "### Stacking (Stacked Generalization)\n",
    "\n",
    "Uses a meta-learner to combine base model predictions:\n",
    "1. Train base models on training data\n",
    "2. Use base model predictions as features\n",
    "3. Train meta-model on these features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voting Classifier\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', LogisticRegression(max_iter=1000, random_state=42)),\n",
    "        ('rf', RandomForestClassifier(n_estimators=50, random_state=42)),\n",
    "        ('svc', SVC(probability=True, random_state=42))\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Voting Classifier Results:\")\n",
    "print(f\"Test Accuracy: {voting_clf.score(X_test, y_test):.3f}\")\n",
    "\n",
    "# Compare with individual models\n",
    "print(\"\\nIndividual Model Scores:\")\n",
    "for name, model in voting_clf.named_estimators_.items():\n",
    "    print(f\"{name}: {model.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking Classifier\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', LogisticRegression(max_iter=1000, random_state=42)),\n",
    "        ('rf', RandomForestClassifier(n_estimators=50, random_state=42)),\n",
    "        ('knn', KNeighborsClassifier(n_neighbors=5))\n",
    "    ],\n",
    "    final_estimator=GradientBoostingClassifier(n_estimators=50, random_state=42),\n",
    "    cv=5\n",
    ")\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Stacking Classifier Results:\")\n",
    "print(f\"Test Accuracy: {stacking_clf.score(X_test, y_test):.3f}\")\n",
    "\n",
    "print(\"\\nBase Estimator Scores:\")\n",
    "for name, model in stacking_clf.named_estimators_.items():\n",
    "    print(f\"{name}: {model.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Comprehensive Model Comparison\n",
    "\n",
    "Let's compare all ensemble methods on a challenging dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load wine dataset (multi-class)\n",
    "wine = load_wine()\n",
    "X, y = wine.data, wine.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Models to compare\n",
    "models = {\n",
    "    'Single Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Bagging': BaggingClassifier(n_estimators=100, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'AdaBoost': AdaBoostClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'Voting': VotingClassifier(\n",
    "        estimators=[\n",
    "            ('rf', RandomForestClassifier(n_estimators=50, random_state=42)),\n",
    "            ('gb', GradientBoostingClassifier(n_estimators=50, random_state=42))\n",
    "        ],\n",
    "        voting='soft'\n",
    "    )\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)\n",
    "    \n",
    "    # Train and test\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    test_score = model.score(X_test_scaled, y_test)\n",
    "    \n",
    "    results[name] = {\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'test_score': test_score\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  CV Score: {cv_scores.mean():.3f} (+/- {cv_scores.std():.3f})\")\n",
    "    print(f\"  Test Score: {test_score:.3f}\")\n",
    "\n",
    "# Visualize comparison\n",
    "results_df = pd.DataFrame(results).T\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(results_df))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, results_df['cv_mean'], width, label='CV Score',\n",
    "      yerr=results_df['cv_std'], capsize=5)\n",
    "ax.bar(x + width/2, results_df['test_score'], width, label='Test Score')\n",
    "\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Ensemble Methods Comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(results_df.index, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Bagging** reduces variance, works well with high-variance models (deep trees)\n",
    "2. **Random Forests** combine bagging with feature randomness for excellent performance\n",
    "3. **Boosting** reduces bias, builds models sequentially\n",
    "4. **AdaBoost** focuses on misclassified samples\n",
    "5. **Gradient Boosting** is more sophisticated, typically more accurate\n",
    "6. **XGBoost/LightGBM** are state-of-the-art implementations\n",
    "7. **Voting** combines diverse models through averaging\n",
    "8. **Stacking** uses a meta-learner to combine predictions\n",
    "9. **Feature importance** helps understand what the model learned\n",
    "10. Ensemble methods typically outperform single models\n",
    "\n",
    "## When to Use What?\n",
    "\n",
    "- **Random Forest**: Great default choice, interpretable, robust\n",
    "- **Gradient Boosting/XGBoost**: When you need maximum accuracy\n",
    "- **AdaBoost**: When you have simple weak learners\n",
    "- **Bagging**: When variance is the main issue\n",
    "- **Stacking**: When you want to squeeze out last bit of performance\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. **Out-of-Bag (OOB) Score**: Implement OOB evaluation for bagging.\n",
    "\n",
    "2. **Ensemble Diversity**: Measure and visualize diversity among ensemble members.\n",
    "\n",
    "3. **Partial Dependence Plots**: Visualize how features affect predictions.\n",
    "\n",
    "4. **Hyperparameter Tuning**: Use GridSearchCV to find optimal hyperparameters for Random Forest.\n",
    "\n",
    "5. **Custom Voting**: Implement weighted voting with custom weights.\n",
    "\n",
    "6. **Multi-level Stacking**: Create a 3-level stacking ensemble.\n",
    "\n",
    "7. **Feature Selection with RF**: Use Random Forest feature importance for feature selection.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "You've completed Week 5! Next week:\n",
    "- Week 6: Neural Networks - Deep learning fundamentals\n",
    "- Week 7: Language - NLP and transformers\n",
    "\n",
    "Congratulations! You now understand both individual ML algorithms and how to combine them for superior performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
