{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Markov Models and Applications\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will:\n",
    "- Understand Markov chains and the Markov property\n",
    "- Build and analyze Hidden Markov Models (HMMs)\n",
    "- Implement forward-backward algorithm\n",
    "- Apply Viterbi algorithm for sequence decoding\n",
    "- Use HMMs for time series prediction\n",
    "- Apply to real-world sequential data\n",
    "\n",
    "## What are Markov Models?\n",
    "\n",
    "**Markov Models** handle sequential data where:\n",
    "- Current state depends on previous state(s)\n",
    "- Future is independent of past given present\n",
    "\n",
    "**Markov Property**: \"The future is independent of the past given the present\"\n",
    "\n",
    "$$P(X_t | X_1, ..., X_{t-1}) = P(X_t | X_{t-1})$$\n",
    "\n",
    "## Types of Markov Models\n",
    "\n",
    "1. **Markov Chain**: States are fully observable\n",
    "2. **Hidden Markov Model (HMM)**: States are hidden, we only see observations\n",
    "\n",
    "## Real-World Applications\n",
    "\n",
    "- üó£Ô∏è **Speech Recognition**: Audio ‚Üí words\n",
    "- üå§Ô∏è **Weather Prediction**: Today's weather ‚Üí tomorrow's\n",
    "- üìà **Finance**: Market states and price movements\n",
    "- üß¨ **Bioinformatics**: DNA sequences and gene finding\n",
    "- üìù **Natural Language**: Part-of-speech tagging\n",
    "- ü§ñ **Robotics**: Robot localization and tracking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plot settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Markov Chains\n",
    "\n",
    "### Components of a Markov Chain\n",
    "\n",
    "1. **States**: S = {s‚ÇÅ, s‚ÇÇ, ..., s‚Çô}\n",
    "2. **Initial Distribution**: œÄ = P(X‚ÇÄ = s·µ¢)\n",
    "3. **Transition Matrix**: A where A·µ¢‚±º = P(X‚Çú = s‚±º | X‚Çú‚Çã‚ÇÅ = s·µ¢)\n",
    "\n",
    "**Properties**:\n",
    "- Each row of A sums to 1 (probability distribution)\n",
    "- A·µ¢‚±º ‚â• 0 for all i, j\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovChain:\n",
    "    \"\"\"A simple Markov Chain implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, states: List[str], \n",
    "                 transition_matrix: np.ndarray,\n",
    "                 initial_distribution: np.ndarray = None):\n",
    "        \"\"\"\n",
    "        Initialize Markov Chain.\n",
    "        \n",
    "        Args:\n",
    "            states: List of state names\n",
    "            transition_matrix: Transition probability matrix\n",
    "            initial_distribution: Initial state probabilities\n",
    "        \"\"\"\n",
    "        self.states = states\n",
    "        self.n_states = len(states)\n",
    "        self.transition_matrix = transition_matrix\n",
    "        \n",
    "        if initial_distribution is None:\n",
    "            self.initial_distribution = np.ones(self.n_states) / self.n_states\n",
    "        else:\n",
    "            self.initial_distribution = initial_distribution\n",
    "        \n",
    "        # Validate\n",
    "        assert transition_matrix.shape == (self.n_states, self.n_states)\n",
    "        assert np.allclose(transition_matrix.sum(axis=1), 1.0)\n",
    "        assert np.allclose(self.initial_distribution.sum(), 1.0)\n",
    "    \n",
    "    def next_state(self, current_state: str) -> str:\n",
    "        \"\"\"Sample next state given current state.\"\"\"\n",
    "        current_idx = self.states.index(current_state)\n",
    "        next_idx = np.random.choice(self.n_states, p=self.transition_matrix[current_idx])\n",
    "        return self.states[next_idx]\n",
    "    \n",
    "    def generate_sequence(self, length: int) -> List[str]:\n",
    "        \"\"\"Generate a sequence of states.\"\"\"\n",
    "        sequence = []\n",
    "        \n",
    "        # Sample initial state\n",
    "        current = np.random.choice(self.states, p=self.initial_distribution)\n",
    "        sequence.append(current)\n",
    "        \n",
    "        # Generate sequence\n",
    "        for _ in range(length - 1):\n",
    "            current = self.next_state(current)\n",
    "            sequence.append(current)\n",
    "        \n",
    "        return sequence\n",
    "    \n",
    "    def state_distribution(self, steps: int) -> np.ndarray:\n",
    "        \"\"\"Calculate state distribution after n steps.\"\"\"\n",
    "        distribution = self.initial_distribution.copy()\n",
    "        for _ in range(steps):\n",
    "            distribution = distribution @ self.transition_matrix\n",
    "        return distribution\n",
    "    \n",
    "    def stationary_distribution(self, max_iter: int = 1000) -> np.ndarray:\n",
    "        \"\"\"Find stationary distribution (if it exists).\"\"\"\n",
    "        distribution = self.initial_distribution.copy()\n",
    "        for _ in range(max_iter):\n",
    "            new_distribution = distribution @ self.transition_matrix\n",
    "            if np.allclose(distribution, new_distribution):\n",
    "                return new_distribution\n",
    "            distribution = new_distribution\n",
    "        return distribution\n",
    "    \n",
    "    def visualize_transition_matrix(self):\n",
    "        \"\"\"Visualize transition probabilities.\"\"\"\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(self.transition_matrix, annot=True, fmt='.2f',\n",
    "                   xticklabels=self.states, yticklabels=self.states,\n",
    "                   cmap='YlOrRd', cbar_kws={'label': 'Probability'})\n",
    "        plt.title('Transition Probability Matrix', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Next State', fontsize=12, fontweight='bold')\n",
    "        plt.ylabel('Current State', fontsize=12, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Example: Weather Markov Chain\n",
    "print(\"Weather Markov Chain Example\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "states = ['Sunny', 'Cloudy', 'Rainy']\n",
    "\n",
    "# Transition matrix\n",
    "# Rows: current state, Columns: next state\n",
    "transition = np.array([\n",
    "    [0.7, 0.2, 0.1],  # Sunny ‚Üí Sunny, Cloudy, Rainy\n",
    "    [0.3, 0.4, 0.3],  # Cloudy ‚Üí Sunny, Cloudy, Rainy\n",
    "    [0.2, 0.3, 0.5]   # Rainy ‚Üí Sunny, Cloudy, Rainy\n",
    "])\n",
    "\n",
    "initial = np.array([0.5, 0.3, 0.2])\n",
    "\n",
    "weather_mc = MarkovChain(states, transition, initial)\n",
    "\n",
    "# Generate a sequence\n",
    "sequence = weather_mc.generate_sequence(30)\n",
    "print(f\"Generated weather sequence (30 days):\")\n",
    "print(' ‚Üí '.join(sequence[:15]))\n",
    "print(' ‚Üí '.join(sequence[15:]))\n",
    "print()\n",
    "\n",
    "# Calculate distribution after steps\n",
    "for steps in [1, 5, 10, 50]:\n",
    "    dist = weather_mc.state_distribution(steps)\n",
    "    print(f\"After {steps:2d} steps:\")\n",
    "    for state, prob in zip(states, dist):\n",
    "        print(f\"  P({state}) = {prob:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Stationary distribution\n",
    "stationary = weather_mc.stationary_distribution()\n",
    "print(\"Stationary distribution (long-run behavior):\")\n",
    "for state, prob in zip(states, stationary):\n",
    "    print(f\"  P({state}) = {prob:.4f}\")\n",
    "\n",
    "# Visualize\n",
    "weather_mc.visualize_transition_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Hidden Markov Models (HMMs)\n",
    "\n",
    "### What makes it \"Hidden\"?\n",
    "\n",
    "In HMMs:\n",
    "- **States** are hidden (not directly observable)\n",
    "- **Observations** are emitted from states (what we see)\n",
    "\n",
    "### HMM Components\n",
    "\n",
    "1. **Hidden States**: S = {s‚ÇÅ, ..., s‚Çô}\n",
    "2. **Observations**: O = {o‚ÇÅ, ..., o‚Çò}\n",
    "3. **Initial Distribution**: œÄ\n",
    "4. **Transition Matrix**: A (between hidden states)\n",
    "5. **Emission Matrix**: B where B·µ¢‚±º = P(observe o‚±º | state s·µ¢)\n",
    "\n",
    "### Three Fundamental Problems\n",
    "\n",
    "1. **Evaluation**: Given observations, what's P(observations | model)?\n",
    "2. **Decoding**: Given observations, what's the most likely state sequence?\n",
    "3. **Learning**: Given observations, what are the best model parameters?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenMarkovModel:\n",
    "    \"\"\"Hidden Markov Model implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, states: List[str], observations: List[str],\n",
    "                 transition_matrix: np.ndarray,\n",
    "                 emission_matrix: np.ndarray,\n",
    "                 initial_distribution: np.ndarray = None):\n",
    "        \"\"\"\n",
    "        Initialize HMM.\n",
    "        \n",
    "        Args:\n",
    "            states: Hidden state names\n",
    "            observations: Observation names\n",
    "            transition_matrix: State transition probabilities\n",
    "            emission_matrix: Observation emission probabilities\n",
    "            initial_distribution: Initial state probabilities\n",
    "        \"\"\"\n",
    "        self.states = states\n",
    "        self.observations = observations\n",
    "        self.n_states = len(states)\n",
    "        self.n_obs = len(observations)\n",
    "        \n",
    "        self.A = transition_matrix  # Transition\n",
    "        self.B = emission_matrix    # Emission\n",
    "        \n",
    "        if initial_distribution is None:\n",
    "            self.pi = np.ones(self.n_states) / self.n_states\n",
    "        else:\n",
    "            self.pi = initial_distribution\n",
    "        \n",
    "        # Validate\n",
    "        assert self.A.shape == (self.n_states, self.n_states)\n",
    "        assert self.B.shape == (self.n_states, self.n_obs)\n",
    "        assert np.allclose(self.A.sum(axis=1), 1.0)\n",
    "        assert np.allclose(self.B.sum(axis=1), 1.0)\n",
    "        assert np.allclose(self.pi.sum(), 1.0)\n",
    "    \n",
    "    def generate_sequence(self, length: int) -> Tuple[List[str], List[str]]:\n",
    "        \"\"\"\n",
    "        Generate a sequence of states and observations.\n",
    "        \n",
    "        Returns:\n",
    "            (state_sequence, observation_sequence)\n",
    "        \"\"\"\n",
    "        states_seq = []\n",
    "        obs_seq = []\n",
    "        \n",
    "        # Initial state\n",
    "        state_idx = np.random.choice(self.n_states, p=self.pi)\n",
    "        states_seq.append(self.states[state_idx])\n",
    "        \n",
    "        # Initial observation\n",
    "        obs_idx = np.random.choice(self.n_obs, p=self.B[state_idx])\n",
    "        obs_seq.append(self.observations[obs_idx])\n",
    "        \n",
    "        # Generate sequence\n",
    "        for _ in range(length - 1):\n",
    "            # Next state\n",
    "            state_idx = np.random.choice(self.n_states, p=self.A[state_idx])\n",
    "            states_seq.append(self.states[state_idx])\n",
    "            \n",
    "            # Observation from new state\n",
    "            obs_idx = np.random.choice(self.n_obs, p=self.B[state_idx])\n",
    "            obs_seq.append(self.observations[obs_idx])\n",
    "        \n",
    "        return states_seq, obs_seq\n",
    "    \n",
    "    def forward(self, observations: List[str]) -> Tuple[np.ndarray, float]:\n",
    "        \"\"\"\n",
    "        Forward algorithm to compute P(observations).\n",
    "        \n",
    "        Args:\n",
    "            observations: Sequence of observations\n",
    "        \n",
    "        Returns:\n",
    "            (alpha matrix, total probability)\n",
    "        \"\"\"\n",
    "        T = len(observations)\n",
    "        alpha = np.zeros((T, self.n_states))\n",
    "        \n",
    "        # Initialization\n",
    "        obs_idx = self.observations.index(observations[0])\n",
    "        alpha[0] = self.pi * self.B[:, obs_idx]\n",
    "        \n",
    "        # Recursion\n",
    "        for t in range(1, T):\n",
    "            obs_idx = self.observations.index(observations[t])\n",
    "            for j in range(self.n_states):\n",
    "                alpha[t, j] = np.sum(alpha[t-1] * self.A[:, j]) * self.B[j, obs_idx]\n",
    "        \n",
    "        # Termination\n",
    "        prob = np.sum(alpha[T-1])\n",
    "        \n",
    "        return alpha, prob\n",
    "    \n",
    "    def viterbi(self, observations: List[str]) -> Tuple[List[str], float]:\n",
    "        \"\"\"\n",
    "        Viterbi algorithm to find most likely state sequence.\n",
    "        \n",
    "        Args:\n",
    "            observations: Sequence of observations\n",
    "        \n",
    "        Returns:\n",
    "            (most_likely_states, probability)\n",
    "        \"\"\"\n",
    "        T = len(observations)\n",
    "        delta = np.zeros((T, self.n_states))\n",
    "        psi = np.zeros((T, self.n_states), dtype=int)\n",
    "        \n",
    "        # Initialization\n",
    "        obs_idx = self.observations.index(observations[0])\n",
    "        delta[0] = self.pi * self.B[:, obs_idx]\n",
    "        \n",
    "        # Recursion\n",
    "        for t in range(1, T):\n",
    "            obs_idx = self.observations.index(observations[t])\n",
    "            for j in range(self.n_states):\n",
    "                probs = delta[t-1] * self.A[:, j]\n",
    "                psi[t, j] = np.argmax(probs)\n",
    "                delta[t, j] = np.max(probs) * self.B[j, obs_idx]\n",
    "        \n",
    "        # Backtracking\n",
    "        path = [0] * T\n",
    "        path[T-1] = np.argmax(delta[T-1])\n",
    "        \n",
    "        for t in range(T-2, -1, -1):\n",
    "            path[t] = psi[t+1, path[t+1]]\n",
    "        \n",
    "        # Convert to state names\n",
    "        state_path = [self.states[i] for i in path]\n",
    "        prob = np.max(delta[T-1])\n",
    "        \n",
    "        return state_path, prob\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"HMM({self.n_states} states, {self.n_obs} observations)\"\n",
    "\n",
    "\n",
    "# Example: Dishonest Casino\n",
    "print(\"Dishonest Casino HMM\")\n",
    "print(\"=\" * 60)\n",
    "print(\"A casino has two dice:\")\n",
    "print(\"- Fair die: Equal probability for all numbers\")\n",
    "print(\"- Loaded die: Biased towards 6\")\n",
    "print(\"The dealer secretly switches between them!\")\n",
    "print()\n",
    "\n",
    "states = ['Fair', 'Loaded']\n",
    "observations = ['1', '2', '3', '4', '5', '6']\n",
    "\n",
    "# Transition: dealer switches dice occasionally\n",
    "transition = np.array([\n",
    "    [0.95, 0.05],  # Fair ‚Üí Fair, Loaded\n",
    "    [0.10, 0.90]   # Loaded ‚Üí Fair, Loaded\n",
    "])\n",
    "\n",
    "# Emission: probability of each number\n",
    "emission = np.array([\n",
    "    [1/6, 1/6, 1/6, 1/6, 1/6, 1/6],  # Fair: uniform\n",
    "    [1/10, 1/10, 1/10, 1/10, 1/10, 1/2]  # Loaded: biased to 6\n",
    "])\n",
    "\n",
    "initial = np.array([0.5, 0.5])\n",
    "\n",
    "casino_hmm = HiddenMarkovModel(states, observations, transition, emission, initial)\n",
    "\n",
    "# Generate a sequence\n",
    "true_states, rolls = casino_hmm.generate_sequence(30)\n",
    "print(\"Generated rolls:\")\n",
    "print(' '.join(rolls))\n",
    "print()\n",
    "print(\"True states (hidden):\")\n",
    "print(' '.join([s[0] for s in true_states]))\n",
    "print()\n",
    "\n",
    "# Use Viterbi to infer states\n",
    "inferred_states, prob = casino_hmm.viterbi(rolls)\n",
    "print(\"Inferred states:\")\n",
    "print(' '.join([s[0] for s in inferred_states]))\n",
    "print(f\"\\nProbability: {prob:.2e}\")\n",
    "\n",
    "# Calculate accuracy\n",
    "correct = sum(1 for t, i in zip(true_states, inferred_states) if t == i)\n",
    "accuracy = correct / len(true_states)\n",
    "print(f\"Accuracy: {accuracy*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Weather Prediction with HMM\n",
    "\n",
    "Let's build a practical weather prediction system!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Weather Prediction HMM\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Hidden states: actual weather conditions\n",
    "weather_states = ['Sunny', 'Rainy']\n",
    "\n",
    "# Observations: what we see (temperature, humidity)\n",
    "weather_obs = ['Hot', 'Warm', 'Cold']\n",
    "\n",
    "# State transitions\n",
    "weather_transition = np.array([\n",
    "    [0.8, 0.2],  # Sunny ‚Üí Sunny, Rainy\n",
    "    [0.4, 0.6]   # Rainy ‚Üí Sunny, Rainy\n",
    "])\n",
    "\n",
    "# Emission probabilities\n",
    "weather_emission = np.array([\n",
    "    [0.6, 0.3, 0.1],  # Sunny ‚Üí Hot, Warm, Cold\n",
    "    [0.1, 0.4, 0.5]   # Rainy ‚Üí Hot, Warm, Cold\n",
    "])\n",
    "\n",
    "weather_initial = np.array([0.6, 0.4])\n",
    "\n",
    "weather_hmm = HiddenMarkovModel(\n",
    "    weather_states, weather_obs,\n",
    "    weather_transition, weather_emission, weather_initial\n",
    ")\n",
    "\n",
    "# Simulate a week of observations\n",
    "true_weather, observations = weather_hmm.generate_sequence(7)\n",
    "\n",
    "print(\"Week of observations:\")\n",
    "for day, obs in enumerate(observations, 1):\n",
    "    print(f\"  Day {day}: Temperature is {obs}\")\n",
    "print()\n",
    "\n",
    "# Infer actual weather\n",
    "inferred_weather, _ = weather_hmm.viterbi(observations)\n",
    "\n",
    "print(\"Comparison:\")\n",
    "print(\"Day | Observation | True Weather | Inferred\")\n",
    "print(\"-\" * 50)\n",
    "for day, (obs, true, inf) in enumerate(zip(observations, true_weather, inferred_weather), 1):\n",
    "    match = \"‚úì\" if true == inf else \"‚úó\"\n",
    "    print(f\" {day}  |    {obs:5s}    |   {true:6s}    | {inf:6s} {match}\")\n",
    "\n",
    "# Calculate forward probabilities\n",
    "alpha, total_prob = weather_hmm.forward(observations)\n",
    "\n",
    "print(f\"\\nP(observations | model) = {total_prob:.6e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Part-of-Speech Tagging\n",
    "\n",
    "A classic NLP application of HMMs!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Part-of-Speech Tagging with HMM\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Hidden states: POS tags\n",
    "pos_tags = ['NOUN', 'VERB', 'ADJ']\n",
    "\n",
    "# Observations: words\n",
    "words = ['dog', 'cat', 'run', 'jump', 'big', 'small']\n",
    "\n",
    "# Tag transitions (simplified)\n",
    "pos_transition = np.array([\n",
    "    [0.3, 0.5, 0.2],  # NOUN ‚Üí NOUN, VERB, ADJ\n",
    "    [0.6, 0.1, 0.3],  # VERB ‚Üí NOUN, VERB, ADJ\n",
    "    [0.8, 0.1, 0.1]   # ADJ ‚Üí NOUN, VERB, ADJ\n",
    "])\n",
    "\n",
    "# Emission probabilities: P(word | tag)\n",
    "pos_emission = np.array([\n",
    "    [0.4, 0.4, 0.05, 0.05, 0.05, 0.05],  # NOUN\n",
    "    [0.05, 0.05, 0.4, 0.4, 0.05, 0.05],  # VERB\n",
    "    [0.05, 0.05, 0.05, 0.05, 0.4, 0.4]   # ADJ\n",
    "])\n",
    "\n",
    "pos_initial = np.array([0.5, 0.3, 0.2])\n",
    "\n",
    "pos_hmm = HiddenMarkovModel(\n",
    "    pos_tags, words,\n",
    "    pos_transition, pos_emission, pos_initial\n",
    ")\n",
    "\n",
    "# Tag some sentences\n",
    "sentences = [\n",
    "    ['big', 'dog', 'run'],\n",
    "    ['small', 'cat', 'jump'],\n",
    "    ['dog', 'jump']\n",
    "]\n",
    "\n",
    "print(\"Tagging sentences:\\n\")\n",
    "for sentence in sentences:\n",
    "    tags, prob = pos_hmm.viterbi(sentence)\n",
    "    print(\" \".join(sentence))\n",
    "    print(\" \".join(tags))\n",
    "    print(f\"Log probability: {np.log(prob):.2f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Time Series Analysis\n",
    "\n",
    "Using HMMs for market regime detection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Market Regime Detection\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Hidden states: market regimes\n",
    "regimes = ['Bull', 'Bear', 'Sideways']\n",
    "\n",
    "# Observations: daily returns (simplified)\n",
    "returns = ['Up', 'Down', 'Flat']\n",
    "\n",
    "# Regime transitions\n",
    "regime_transition = np.array([\n",
    "    [0.7, 0.2, 0.1],   # Bull\n",
    "    [0.2, 0.7, 0.1],   # Bear\n",
    "    [0.3, 0.3, 0.4]    # Sideways\n",
    "])\n",
    "\n",
    "# Return distributions per regime\n",
    "regime_emission = np.array([\n",
    "    [0.6, 0.2, 0.2],   # Bull: mostly up\n",
    "    [0.2, 0.6, 0.2],   # Bear: mostly down\n",
    "    [0.3, 0.3, 0.4]    # Sideways: mostly flat\n",
    "])\n",
    "\n",
    "regime_initial = np.array([0.4, 0.3, 0.3])\n",
    "\n",
    "market_hmm = HiddenMarkovModel(\n",
    "    regimes, returns,\n",
    "    regime_transition, regime_emission, regime_initial\n",
    ")\n",
    "\n",
    "# Generate market data\n",
    "true_regimes, observed_returns = market_hmm.generate_sequence(50)\n",
    "\n",
    "# Infer regimes\n",
    "inferred_regimes, _ = market_hmm.viterbi(observed_returns)\n",
    "\n",
    "# Visualize\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 8), sharex=True)\n",
    "\n",
    "# Encode states as numbers for plotting\n",
    "regime_map = {'Bull': 2, 'Sideways': 1, 'Bear': 0}\n",
    "true_encoded = [regime_map[r] for r in true_regimes]\n",
    "inferred_encoded = [regime_map[r] for r in inferred_regimes]\n",
    "\n",
    "days = range(len(true_regimes))\n",
    "\n",
    "# True regimes\n",
    "ax1.plot(days, true_encoded, 'o-', linewidth=2, markersize=6, label='True Regime')\n",
    "ax1.set_yticks([0, 1, 2])\n",
    "ax1.set_yticklabels(['Bear', 'Sideways', 'Bull'])\n",
    "ax1.set_ylabel('Market Regime', fontweight='bold')\n",
    "ax1.set_title('True Market Regimes', fontweight='bold', fontsize=13)\n",
    "ax1.grid(alpha=0.3)\n",
    "ax1.legend()\n",
    "\n",
    "# Inferred regimes\n",
    "ax2.plot(days, inferred_encoded, 's-', linewidth=2, markersize=6, \n",
    "        color='orange', label='Inferred Regime')\n",
    "ax2.set_yticks([0, 1, 2])\n",
    "ax2.set_yticklabels(['Bear', 'Sideways', 'Bull'])\n",
    "ax2.set_xlabel('Day', fontweight='bold')\n",
    "ax2.set_ylabel('Market Regime', fontweight='bold')\n",
    "ax2.set_title('Inferred Market Regimes (Viterbi)', fontweight='bold', fontsize=13)\n",
    "ax2.grid(alpha=0.3)\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = sum(1 for t, i in zip(true_regimes, inferred_regimes) if t == i) / len(true_regimes)\n",
    "print(f\"\\nRegime detection accuracy: {accuracy*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Using hmmlearn Library\n",
    "\n",
    "Let's use a professional library for more sophisticated HMMs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from hmmlearn import hmm\n",
    "    HMMLEARN_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"hmmlearn not installed. Run: pip install hmmlearn\")\n",
    "    HMMLEARN_AVAILABLE = False\n",
    "\n",
    "if HMMLEARN_AVAILABLE:\n",
    "    print(\"Stock Price Modeling with hmmlearn\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Generate synthetic stock returns\n",
    "    np.random.seed(42)\n",
    "    n_samples = 200\n",
    "    \n",
    "    # Two regimes: high volatility and low volatility\n",
    "    regime = np.random.choice([0, 1], size=n_samples, p=[0.7, 0.3])\n",
    "    returns = np.zeros(n_samples)\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        if regime[i] == 0:  # Low volatility\n",
    "            returns[i] = np.random.normal(0.001, 0.01)\n",
    "        else:  # High volatility\n",
    "            returns[i] = np.random.normal(-0.002, 0.05)\n",
    "    \n",
    "    # Reshape for hmmlearn\n",
    "    X = returns.reshape(-1, 1)\n",
    "    \n",
    "    # Fit Gaussian HMM\n",
    "    model = hmm.GaussianHMM(n_components=2, covariance_type=\"full\", n_iter=100)\n",
    "    model.fit(X)\n",
    "    \n",
    "    # Predict hidden states\n",
    "    hidden_states = model.predict(X)\n",
    "    \n",
    "    print(\"Model Parameters:\")\n",
    "    print(f\"Transition matrix:\\n{model.transmat_}\")\n",
    "    print(f\"\\nMeans: {model.means_.flatten()}\")\n",
    "    print(f\"Variances: {np.sqrt(model.covars_.flatten())}\")\n",
    "    print()\n",
    "    \n",
    "    # Visualize\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 8), sharex=True)\n",
    "    \n",
    "    # Returns\n",
    "    ax1.plot(returns, alpha=0.7)\n",
    "    ax1.set_ylabel('Returns', fontweight='bold')\n",
    "    ax1.set_title('Stock Returns', fontweight='bold', fontsize=13)\n",
    "    ax1.grid(alpha=0.3)\n",
    "    \n",
    "    # Hidden states\n",
    "    ax2.plot(hidden_states, drawstyle='steps-post', linewidth=2)\n",
    "    ax2.set_xlabel('Time', fontweight='bold')\n",
    "    ax2.set_ylabel('Hidden State', fontweight='bold')\n",
    "    ax2.set_title('Inferred Volatility Regime', fontweight='bold', fontsize=13)\n",
    "    ax2.set_yticks([0, 1])\n",
    "    ax2.set_yticklabels(['Low Vol', 'High Vol'])\n",
    "    ax2.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Install hmmlearn to run this section: pip install hmmlearn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Robot Localization\n",
    "Build an HMM where:\n",
    "- Hidden states: Robot's actual position (Left, Center, Right)\n",
    "- Observations: Noisy sensor readings\n",
    "Use Viterbi to track the robot's path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build robot localization HMM\n",
    "# Your code here\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Sentiment Analysis\n",
    "Create an HMM for sentiment tracking:\n",
    "- Hidden states: Sentiment (Positive, Negative, Neutral)\n",
    "- Observations: Words in reviews\n",
    "Track sentiment evolution in a sequence of reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build sentiment tracking HMM\n",
    "# Your code here\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Compare Algorithms\n",
    "Compare forward algorithm vs Viterbi on the same observation sequence.\n",
    "Explain when each is appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare forward and Viterbi algorithms\n",
    "# Your code here\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Markov Property** - Future independent of past given present\n",
    "2. **Markov Chains** - Simple sequential models with observable states\n",
    "3. **Hidden Markov Models** - States hidden, observations visible\n",
    "4. **Forward Algorithm** - Calculate probability of observation sequence\n",
    "5. **Viterbi Algorithm** - Find most likely state sequence\n",
    "6. **Real Applications** - Speech, NLP, finance, robotics\n",
    "\n",
    "### When to Use HMMs\n",
    "\n",
    "**Good for**:\n",
    "- Sequential data with temporal dependencies\n",
    "- Hidden structure you want to infer\n",
    "- Well-defined state spaces\n",
    "- Time series with regime changes\n",
    "\n",
    "**Not ideal for**:\n",
    "- Long-range dependencies (use RNNs/LSTMs)\n",
    "- Very high-dimensional observations\n",
    "- Non-Markovian processes\n",
    "\n",
    "### Looking Ahead\n",
    "\n",
    "You've now completed Week 3: Uncertainty!\n",
    "\n",
    "Next steps:\n",
    "- **Week 4**: Optimization - Local search, genetic algorithms\n",
    "- **Week 5**: Machine Learning - Supervised and unsupervised learning\n",
    "- Apply probabilistic reasoning to learning algorithms\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
