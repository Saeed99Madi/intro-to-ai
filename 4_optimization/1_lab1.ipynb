{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Introduction to Optimization\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will:\n",
    "- Understand what optimization means in AI\n",
    "- Formulate optimization problems\n",
    "- Identify objective functions and constraints\n",
    "- Distinguish between local and global optima\n",
    "- Visualize optimization landscapes\n",
    "- Implement basic optimization strategies\n",
    "\n",
    "## What is Optimization?\n",
    "\n",
    "**Optimization** is the process of finding the best solution from all feasible solutions.\n",
    "\n",
    "**General form**:\n",
    "```\n",
    "maximize/minimize f(x)\n",
    "subject to: g_i(x) ≤ 0  (inequality constraints)\n",
    "            h_j(x) = 0  (equality constraints)\n",
    "            x ∈ X       (domain constraints)\n",
    "```\n",
    "\n",
    "Where:\n",
    "- **f(x)** = objective function (what we want to optimize)\n",
    "- **x** = decision variables (what we control)\n",
    "- **g_i, h_j** = constraints (what we must satisfy)\n",
    "\n",
    "## Why Optimization in AI?\n",
    "\n",
    "Optimization is **everywhere** in AI:\n",
    "- 🎯 **Training ML models**: Minimize loss function\n",
    "- 🗺️ **Path planning**: Minimize distance/time\n",
    "- 📅 **Scheduling**: Minimize conflicts/costs\n",
    "- 🎮 **Game AI**: Maximize score/win probability\n",
    "- 🤖 **Robot control**: Minimize energy/time\n",
    "- 📊 **Resource allocation**: Maximize utility\n",
    "\n",
    "## Real-World Applications\n",
    "\n",
    "- 🚗 **Uber/Lyft**: Route optimization, driver assignment\n",
    "- 📦 **Amazon**: Warehouse layout, delivery routes\n",
    "- ✈️ **Airlines**: Crew scheduling, flight routing\n",
    "- 🏭 **Manufacturing**: Production planning, supply chains\n",
    "- 💰 **Finance**: Portfolio optimization, trading strategies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from typing import Callable, List, Tuple, Optional\n",
    "import time\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plot settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Formulating Optimization Problems\n",
    "\n",
    "### Example 1: The Knapsack Problem\n",
    "\n",
    "**Problem**: You have a knapsack with limited capacity. Choose items to maximize value.\n",
    "\n",
    "**Formulation**:\n",
    "- **Decision variables**: x_i ∈ {0, 1} (take item i or not)\n",
    "- **Objective**: maximize Σ v_i × x_i (total value)\n",
    "- **Constraint**: Σ w_i × x_i ≤ W (weight limit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnapsackProblem:\n",
    "    \"\"\"The 0-1 Knapsack Problem.\"\"\"\n",
    "    \n",
    "    def __init__(self, values: List[float], weights: List[float], capacity: float):\n",
    "        \"\"\"\n",
    "        Initialize knapsack problem.\n",
    "        \n",
    "        Args:\n",
    "            values: Value of each item\n",
    "            weights: Weight of each item\n",
    "            capacity: Maximum weight capacity\n",
    "        \"\"\"\n",
    "        self.values = np.array(values)\n",
    "        self.weights = np.array(weights)\n",
    "        self.capacity = capacity\n",
    "        self.n_items = len(values)\n",
    "    \n",
    "    def is_feasible(self, solution: np.ndarray) -> bool:\n",
    "        \"\"\"Check if solution satisfies constraints.\"\"\"\n",
    "        total_weight = np.sum(solution * self.weights)\n",
    "        return total_weight <= self.capacity\n",
    "    \n",
    "    def objective(self, solution: np.ndarray) -> float:\n",
    "        \"\"\"Calculate objective function value.\"\"\"\n",
    "        if not self.is_feasible(solution):\n",
    "            return -np.inf  # Infeasible solution\n",
    "        return np.sum(solution * self.values)\n",
    "    \n",
    "    def greedy_solution(self) -> np.ndarray:\n",
    "        \"\"\"Simple greedy heuristic: best value-to-weight ratio.\"\"\"\n",
    "        # Calculate value per unit weight\n",
    "        ratios = self.values / self.weights\n",
    "        \n",
    "        # Sort items by ratio (descending)\n",
    "        sorted_indices = np.argsort(-ratios)\n",
    "        \n",
    "        solution = np.zeros(self.n_items, dtype=int)\n",
    "        remaining_capacity = self.capacity\n",
    "        \n",
    "        for idx in sorted_indices:\n",
    "            if self.weights[idx] <= remaining_capacity:\n",
    "                solution[idx] = 1\n",
    "                remaining_capacity -= self.weights[idx]\n",
    "        \n",
    "        return solution\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"KnapsackProblem({self.n_items} items, capacity={self.capacity})\"\n",
    "\n",
    "\n",
    "# Example knapsack problem\n",
    "print(\"Knapsack Problem Example\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "values = [60, 100, 120, 80, 90]\n",
    "weights = [10, 20, 30, 15, 25]\n",
    "capacity = 50\n",
    "\n",
    "knapsack = KnapsackProblem(values, weights, capacity)\n",
    "\n",
    "print(f\"Items: {knapsack.n_items}\")\n",
    "print(f\"Capacity: {capacity}\")\n",
    "print()\n",
    "print(\"Item | Value | Weight | Ratio\")\n",
    "print(\"-\" * 40)\n",
    "for i, (v, w) in enumerate(zip(values, weights)):\n",
    "    ratio = v / w\n",
    "    print(f\"  {i}  |  {v:3d}  |   {w:2d}   | {ratio:.2f}\")\n",
    "\n",
    "# Greedy solution\n",
    "greedy_sol = knapsack.greedy_solution()\n",
    "greedy_value = knapsack.objective(greedy_sol)\n",
    "greedy_weight = np.sum(greedy_sol * knapsack.weights)\n",
    "\n",
    "print()\n",
    "print(\"Greedy Solution:\")\n",
    "print(f\"Selected items: {np.where(greedy_sol == 1)[0].tolist()}\")\n",
    "print(f\"Total value: {greedy_value}\")\n",
    "print(f\"Total weight: {greedy_weight}/{capacity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Objective Functions and Landscapes\n",
    "\n",
    "The **objective function** defines what \"best\" means.\n",
    "\n",
    "### Visualization: Optimization Landscapes\n",
    "\n",
    "Let's visualize different types of optimization landscapes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_landscapes():\n",
    "    \"\"\"Create different optimization landscape examples.\"\"\"\n",
    "    \n",
    "    x = np.linspace(-5, 5, 1000)\n",
    "    \n",
    "    # 1. Convex (easy - single global optimum)\n",
    "    y1 = x**2\n",
    "    \n",
    "    # 2. Non-convex with local minima\n",
    "    y2 = x**4 - 4*x**3 + 3*x**2 + 2*x\n",
    "    \n",
    "    # 3. Multimodal (many local optima)\n",
    "    y3 = x * np.sin(3*x) + 0.5*x**2\n",
    "    \n",
    "    # 4. Noisy (real-world)\n",
    "    y4 = x**2 + 0.5*np.random.randn(len(x))\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    landscapes = [\n",
    "        (y1, \"Convex (Easy)\", \"Single global minimum\"),\n",
    "        (y2, \"Non-Convex\", \"Multiple local minima\"),\n",
    "        (y3, \"Multimodal\", \"Many local optima\"),\n",
    "        (y4, \"Noisy\", \"Real-world complexity\")\n",
    "    ]\n",
    "    \n",
    "    for ax, (y, title, subtitle) in zip(axes.flat, landscapes):\n",
    "        ax.plot(x, y, linewidth=2, color='blue')\n",
    "        \n",
    "        # Mark global minimum\n",
    "        min_idx = np.argmin(y)\n",
    "        ax.plot(x[min_idx], y[min_idx], 'r*', markersize=20, \n",
    "               label='Global minimum', zorder=5)\n",
    "        \n",
    "        ax.set_xlabel('x', fontweight='bold')\n",
    "        ax.set_ylabel('f(x)', fontweight='bold')\n",
    "        ax.set_title(f'{title}\\n{subtitle}', fontweight='bold')\n",
    "        ax.legend()\n",
    "        ax.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Optimization Landscape Types\")\n",
    "print(\"=\" * 60)\n",
    "create_landscapes()\n",
    "\n",
    "print(\"\\nKey Insights:\")\n",
    "print(\"- Convex: Gradient descent works perfectly\")\n",
    "print(\"- Non-convex: May get stuck in local minima\")\n",
    "print(\"- Multimodal: Need global search strategies\")\n",
    "print(\"- Noisy: Need robust optimization methods\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D Optimization Landscape\n",
    "\n",
    "Let's visualize a 2D optimization problem:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rastrigin(x, y):\n",
    "    \"\"\"Rastrigin function - multimodal test function.\"\"\"\n",
    "    A = 10\n",
    "    return (A * 2 + (x**2 - A * np.cos(2 * np.pi * x)) + \n",
    "           (y**2 - A * np.cos(2 * np.pi * y)))\n",
    "\n",
    "def ackley(x, y):\n",
    "    \"\"\"Ackley function - another multimodal test function.\"\"\"\n",
    "    return (-20 * np.exp(-0.2 * np.sqrt(0.5 * (x**2 + y**2))) - \n",
    "           np.exp(0.5 * (np.cos(2 * np.pi * x) + np.cos(2 * np.pi * y))) + \n",
    "           np.e + 20)\n",
    "\n",
    "# Create meshgrid\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y = np.linspace(-5, 5, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "# Calculate function values\n",
    "Z_rastrigin = rastrigin(X, Y)\n",
    "Z_ackley = ackley(X, Y)\n",
    "\n",
    "# 3D surface plots\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "\n",
    "# Rastrigin\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "surf1 = ax1.plot_surface(X, Y, Z_rastrigin, cmap='viridis', alpha=0.8)\n",
    "ax1.set_xlabel('x', fontweight='bold')\n",
    "ax1.set_ylabel('y', fontweight='bold')\n",
    "ax1.set_zlabel('f(x, y)', fontweight='bold')\n",
    "ax1.set_title('Rastrigin Function\\n(Many local minima)', fontweight='bold')\n",
    "fig.colorbar(surf1, ax=ax1, shrink=0.5)\n",
    "\n",
    "# Ackley\n",
    "ax2 = fig.add_subplot(122, projection='3d')\n",
    "surf2 = ax2.plot_surface(X, Y, Z_ackley, cmap='plasma', alpha=0.8)\n",
    "ax2.set_xlabel('x', fontweight='bold')\n",
    "ax2.set_ylabel('y', fontweight='bold')\n",
    "ax2.set_zlabel('f(x, y)', fontweight='bold')\n",
    "ax2.set_title('Ackley Function\\n(Complex landscape)', fontweight='bold')\n",
    "fig.colorbar(surf2, ax=ax2, shrink=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"These are challenging optimization problems!\")\n",
    "print(\"Global minimum is at (0, 0) but there are many local minima.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Local vs Global Optima\n",
    "\n",
    "### Definitions\n",
    "\n",
    "**Local Optimum**: Best solution in a neighborhood\n",
    "- f(x*) ≤ f(x) for all x in neighborhood of x*\n",
    "\n",
    "**Global Optimum**: Best solution overall\n",
    "- f(x*) ≤ f(x) for all x in the entire domain\n",
    "\n",
    "**Challenge**: Many optimization algorithms can get stuck in local optima!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_local_vs_global():\n",
    "    \"\"\"Demonstrate the difference between local and global optima.\"\"\"\n",
    "    \n",
    "    # Create a function with multiple local minima\n",
    "    x = np.linspace(0, 10, 1000)\n",
    "    y = np.sin(x) * x + 0.1 * x**2\n",
    "    \n",
    "    # Find local minima using simple approach\n",
    "    local_minima = []\n",
    "    for i in range(1, len(y) - 1):\n",
    "        if y[i] < y[i-1] and y[i] < y[i+1]:\n",
    "            # Check if it's a significant local minimum\n",
    "            if i % 50 == 0:  # Sample to avoid too many points\n",
    "                local_minima.append((x[i], y[i]))\n",
    "    \n",
    "    # Global minimum\n",
    "    global_min_idx = np.argmin(y)\n",
    "    global_min = (x[global_min_idx], y[global_min_idx])\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(x, y, 'b-', linewidth=2, label='Objective function')\n",
    "    \n",
    "    # Mark local minima\n",
    "    for xm, ym in local_minima:\n",
    "        plt.plot(xm, ym, 'yo', markersize=12, markeredgecolor='orange',\n",
    "                markeredgewidth=2, label='Local minimum' if xm == local_minima[0][0] else '')\n",
    "    \n",
    "    # Mark global minimum\n",
    "    plt.plot(global_min[0], global_min[1], 'r*', markersize=25,\n",
    "            markeredgecolor='darkred', markeredgewidth=2, label='Global minimum')\n",
    "    \n",
    "    plt.xlabel('x', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('f(x)', fontsize=12, fontweight='bold')\n",
    "    plt.title('Local vs Global Optima', fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=11)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Global minimum: f({global_min[0]:.2f}) = {global_min[1]:.2f}\")\n",
    "    print(f\"Found {len(local_minima)} local minima\")\n",
    "    print()\n",
    "    print(\"Problem: If we start at the wrong place, we might find a local\")\n",
    "    print(\"minimum instead of the global minimum!\")\n",
    "\n",
    "demonstrate_local_vs_global()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Random Search Baseline\n",
    "\n",
    "Before implementing sophisticated algorithms, let's establish a baseline:\n",
    "**Random Search** - Simply try random solutions!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomSearch:\n",
    "    \"\"\"Simple random search optimizer.\"\"\"\n",
    "    \n",
    "    def __init__(self, objective_fn: Callable, bounds: List[Tuple[float, float]]):\n",
    "        \"\"\"\n",
    "        Initialize random search.\n",
    "        \n",
    "        Args:\n",
    "            objective_fn: Function to minimize\n",
    "            bounds: List of (min, max) for each dimension\n",
    "        \"\"\"\n",
    "        self.objective_fn = objective_fn\n",
    "        self.bounds = bounds\n",
    "        self.n_dims = len(bounds)\n",
    "    \n",
    "    def optimize(self, n_iterations: int) -> Tuple[np.ndarray, float, List[float]]:\n",
    "        \"\"\"\n",
    "        Run random search.\n",
    "        \n",
    "        Returns:\n",
    "            (best_solution, best_value, history)\n",
    "        \"\"\"\n",
    "        best_solution = None\n",
    "        best_value = np.inf\n",
    "        history = []\n",
    "        \n",
    "        for _ in range(n_iterations):\n",
    "            # Generate random solution\n",
    "            solution = np.array([np.random.uniform(low, high) \n",
    "                               for low, high in self.bounds])\n",
    "            \n",
    "            # Evaluate\n",
    "            value = self.objective_fn(solution)\n",
    "            \n",
    "            # Update best\n",
    "            if value < best_value:\n",
    "                best_value = value\n",
    "                best_solution = solution.copy()\n",
    "            \n",
    "            history.append(best_value)\n",
    "        \n",
    "        return best_solution, best_value, history\n",
    "\n",
    "\n",
    "# Test on sphere function (simple convex)\n",
    "def sphere(x):\n",
    "    \"\"\"Sphere function: simple convex function.\"\"\"\n",
    "    return np.sum(x**2)\n",
    "\n",
    "print(\"Random Search Example\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Minimizing sphere function: f(x) = sum(x_i^2)\")\n",
    "print(\"Global minimum: f(0, 0) = 0\")\n",
    "print()\n",
    "\n",
    "# 2D sphere function\n",
    "bounds = [(-5, 5), (-5, 5)]\n",
    "rs = RandomSearch(sphere, bounds)\n",
    "\n",
    "# Run random search\n",
    "best_sol, best_val, history = rs.optimize(n_iterations=1000)\n",
    "\n",
    "print(f\"Best solution: ({best_sol[0]:.4f}, {best_sol[1]:.4f})\")\n",
    "print(f\"Best value: {best_val:.6f}\")\n",
    "print()\n",
    "\n",
    "# Plot convergence\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history, linewidth=2)\n",
    "plt.xlabel('Iteration', fontweight='bold')\n",
    "plt.ylabel('Best Value Found', fontweight='bold')\n",
    "plt.title('Random Search Convergence', fontweight='bold')\n",
    "plt.yscale('log')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Visualize search space\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y = np.linspace(-5, 5, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = X**2 + Y**2\n",
    "\n",
    "plt.contourf(X, Y, Z, levels=20, cmap='viridis', alpha=0.6)\n",
    "plt.colorbar(label='f(x, y)')\n",
    "plt.plot(best_sol[0], best_sol[1], 'r*', markersize=20, \n",
    "        markeredgecolor='white', markeredgewidth=2, label='Best found')\n",
    "plt.plot(0, 0, 'go', markersize=15, markeredgecolor='white', \n",
    "        markeredgewidth=2, label='True optimum')\n",
    "plt.xlabel('x', fontweight='bold')\n",
    "plt.ylabel('y', fontweight='bold')\n",
    "plt.title('Solution in Search Space', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Optimization Problem Types\n",
    "\n",
    "### Classification of Optimization Problems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification of Optimization Problems\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "classification = {\n",
    "    \"By Variables\": {\n",
    "        \"Continuous\": \"Variables can take any real value (e.g., robot position)\",\n",
    "        \"Discrete\": \"Variables from finite set (e.g., scheduling decisions)\",\n",
    "        \"Mixed\": \"Combination of continuous and discrete\"\n",
    "    },\n",
    "    \"By Objective\": {\n",
    "        \"Single-objective\": \"One objective to optimize\",\n",
    "        \"Multi-objective\": \"Multiple conflicting objectives (Pareto optimization)\"\n",
    "    },\n",
    "    \"By Constraints\": {\n",
    "        \"Unconstrained\": \"No constraints on variables\",\n",
    "        \"Constrained\": \"Must satisfy constraints\"\n",
    "    },\n",
    "    \"By Convexity\": {\n",
    "        \"Convex\": \"Easy - local optimum is global\",\n",
    "        \"Non-convex\": \"Hard - many local optima\"\n",
    "    },\n",
    "    \"By Determinism\": {\n",
    "        \"Deterministic\": \"Objective function is deterministic\",\n",
    "        \"Stochastic\": \"Objective has randomness/noise\"\n",
    "    }\n",
    "}\n",
    "\n",
    "for category, types in classification.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for type_name, description in types.items():\n",
    "        print(f\"  • {type_name}: {description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Benchmark Functions\n",
    "\n",
    "Standard test functions for comparing optimization algorithms:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BenchmarkFunctions:\n",
    "    \"\"\"Common optimization benchmark functions.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def sphere(x):\n",
    "        \"\"\"Sphere: f(x) = sum(x_i^2). Global min at origin.\"\"\"\n",
    "        return np.sum(x**2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def rosenbrock(x):\n",
    "        \"\"\"Rosenbrock: narrow valley to global optimum.\"\"\"\n",
    "        return np.sum(100 * (x[1:] - x[:-1]**2)**2 + (1 - x[:-1])**2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def rastrigin(x):\n",
    "        \"\"\"Rastrigin: highly multimodal.\"\"\"\n",
    "        A = 10\n",
    "        n = len(x)\n",
    "        return A * n + np.sum(x**2 - A * np.cos(2 * np.pi * x))\n",
    "    \n",
    "    @staticmethod\n",
    "    def ackley(x):\n",
    "        \"\"\"Ackley: many local minima.\"\"\"\n",
    "        n = len(x)\n",
    "        return (-20 * np.exp(-0.2 * np.sqrt(np.sum(x**2) / n)) -\n",
    "               np.exp(np.sum(np.cos(2 * np.pi * x)) / n) + 20 + np.e)\n",
    "    \n",
    "    @staticmethod\n",
    "    def griewank(x):\n",
    "        \"\"\"Griewank: many widespread local minima.\"\"\"\n",
    "        sum_term = np.sum(x**2) / 4000\n",
    "        prod_term = np.prod(np.cos(x / np.sqrt(np.arange(1, len(x) + 1))))\n",
    "        return sum_term - prod_term + 1\n",
    "\n",
    "\n",
    "# Test all benchmark functions\n",
    "print(\"Benchmark Function Comparison\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "bench = BenchmarkFunctions()\n",
    "functions = [\n",
    "    ('Sphere', bench.sphere, 'Convex, unimodal'),\n",
    "    ('Rosenbrock', bench.rosenbrock, 'Non-convex valley'),\n",
    "    ('Rastrigin', bench.rastrigin, 'Highly multimodal'),\n",
    "    ('Ackley', bench.ackley, 'Many local minima'),\n",
    "    ('Griewank', bench.griewank, 'Multimodal')\n",
    "]\n",
    "\n",
    "# Evaluate at different points\n",
    "test_points = [\n",
    "    np.array([0.0, 0.0]),\n",
    "    np.array([1.0, 1.0]),\n",
    "    np.array([2.0, -2.0])\n",
    "]\n",
    "\n",
    "print(\"\\nFunction values at test points:\")\n",
    "print(\"Function      | (0,0)  | (1,1)  | (2,-2)  | Type\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for name, func, func_type in functions:\n",
    "    values = [func(pt) for pt in test_points]\n",
    "    print(f\"{name:13s} | {values[0]:6.2f} | {values[1]:6.2f} | {values[2]:7.2f} | {func_type}\")\n",
    "\n",
    "print(\"\\nAll functions have global minimum at or near origin.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Comparing Random Search on Benchmarks\n",
    "\n",
    "Let's compare random search performance on different landscapes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Random Search Performance on Benchmark Functions\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "bench = BenchmarkFunctions()\n",
    "bounds = [(-5, 5), (-5, 5)]\n",
    "n_iterations = 1000\n",
    "\n",
    "functions = [\n",
    "    ('Sphere', bench.sphere),\n",
    "    ('Rosenbrock', bench.rosenbrock),\n",
    "    ('Rastrigin', bench.rastrigin),\n",
    "    ('Ackley', bench.ackley)\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "for ax, (name, func) in zip(axes.flat, functions):\n",
    "    # Run random search\n",
    "    rs = RandomSearch(func, bounds)\n",
    "    best_sol, best_val, history = rs.optimize(n_iterations)\n",
    "    \n",
    "    # Plot convergence\n",
    "    ax.plot(history, linewidth=2, color='blue')\n",
    "    ax.set_xlabel('Iteration', fontweight='bold')\n",
    "    ax.set_ylabel('Best Value', fontweight='bold')\n",
    "    ax.set_title(f'{name} Function\\nFinal: {best_val:.6f}', fontweight='bold')\n",
    "    ax.set_yscale('log')\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"- Sphere: Fast convergence (easy problem)\")\n",
    "print(\"- Rosenbrock: Slower (narrow valley)\")\n",
    "print(\"- Rastrigin: Difficult (many local minima)\")\n",
    "print(\"- Ackley: Challenging (complex landscape)\")\n",
    "print(\"\\nRandom search is not efficient - we need smarter algorithms!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Custom Objective Function\n",
    "Create your own 2D objective function with at least 2 local minima.\n",
    "Visualize it and test random search on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create and test custom objective function\n",
    "# Your code here\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Knapsack Variants\n",
    "Implement and compare different greedy strategies for the knapsack problem:\n",
    "- Sort by value\n",
    "- Sort by weight\n",
    "- Sort by value/weight ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare greedy strategies\n",
    "# Your code here\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Random Search Analysis\n",
    "Run random search 10 times on the same problem with different random seeds.\n",
    "Plot the distribution of final values and analyze variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze random search variance\n",
    "# Your code here\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Optimization** - Finding the best solution among many possibilities\n",
    "2. **Objective Function** - Defines what \"best\" means\n",
    "3. **Constraints** - What solutions are feasible\n",
    "4. **Local vs Global** - Getting stuck vs finding the true optimum\n",
    "5. **Problem Types** - Different problems need different algorithms\n",
    "6. **Benchmarks** - Standard functions to test algorithms\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "- **Universal**: Almost every AI problem involves optimization\n",
    "- **Practical**: Real-world problems are often optimization problems\n",
    "- **Foundation**: Understanding needed for ML, deep learning, RL\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In Lab 2, we'll learn **Local Search Algorithms** that are much smarter than random search:\n",
    "- Hill Climbing\n",
    "- Simulated Annealing\n",
    "- And more!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
