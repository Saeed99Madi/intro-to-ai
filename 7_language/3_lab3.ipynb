{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Sequence Models and Attention\n",
    "\n",
    "In this lab, we'll explore sequence-to-sequence models and attention mechanisms that revolutionized NLP. These techniques enable neural machine translation, text summarization, and more.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will:\n",
    "- Understand encoder-decoder architecture\n",
    "- Implement sequence-to-sequence models\n",
    "- Build attention mechanisms from scratch\n",
    "- Understand self-attention\n",
    "- Implement multi-head attention\n",
    "- Build a neural machine translation system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import re\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Sequence-to-Sequence (Seq2Seq) Architecture\n",
    "\n",
    "**Seq2Seq** models transform one sequence into another:\n",
    "\n",
    "### Architecture:\n",
    "1. **Encoder**: Processes input sequence â†’ context vector\n",
    "2. **Context Vector**: Fixed-size representation of input\n",
    "3. **Decoder**: Generates output sequence from context\n",
    "\n",
    "### Problem with Basic Seq2Seq:\n",
    "- Fixed-size context vector is a bottleneck\n",
    "- Information loss for long sequences\n",
    "- All input must be compressed into single vector\n",
    "\n",
    "### Solution: Attention!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple translation task: English numbers to French\n",
    "data_pairs = [\n",
    "    ('one', 'un'),\n",
    "    ('two', 'deux'),\n",
    "    ('three', 'trois'),\n",
    "    ('four', 'quatre'),\n",
    "    ('five', 'cinq'),\n",
    "    ('six', 'six'),\n",
    "    ('seven', 'sept'),\n",
    "    ('eight', 'huit'),\n",
    "    ('nine', 'neuf'),\n",
    "    ('ten', 'dix'),\n",
    "]\n",
    "\n",
    "# Build vocabularies\n",
    "input_texts = [pair[0] for pair in data_pairs]\n",
    "target_texts = [pair[1] for pair in data_pairs]\n",
    "\n",
    "# Add special tokens\n",
    "input_chars = sorted(set(''.join(input_texts)))\n",
    "target_chars = sorted(set(''.join(target_texts)))\n",
    "target_chars = ['\\t'] + target_chars + ['\\n']  # Start and end tokens\n",
    "\n",
    "input_char_to_idx = {char: idx for idx, char in enumerate(input_chars)}\n",
    "target_char_to_idx = {char: idx for idx, char in enumerate(target_chars)}\n",
    "\n",
    "print(f\"Input vocabulary: {input_chars}\")\n",
    "print(f\"Target vocabulary: {target_chars}\")\n",
    "print(f\"\\nInput vocab size: {len(input_chars)}\")\n",
    "print(f\"Target vocab size: {len(target_chars)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts]) + 2  # +2 for start/end\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, len(input_chars)),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, len(target_chars)),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, len(target_chars)),\n",
    "    dtype='float32')\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    # Encoder input\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_char_to_idx[char]] = 1.0\n",
    "    \n",
    "    # Decoder input (with start token)\n",
    "    decoder_input_data[i, 0, target_char_to_idx['\\t']] = 1.0\n",
    "    for t, char in enumerate(target_text):\n",
    "        decoder_input_data[i, t + 1, target_char_to_idx[char]] = 1.0\n",
    "    \n",
    "    # Decoder target (shifted by one)\n",
    "    for t, char in enumerate(target_text):\n",
    "        decoder_target_data[i, t, target_char_to_idx[char]] = 1.0\n",
    "    decoder_target_data[i, len(target_text), target_char_to_idx['\\n']] = 1.0\n",
    "\n",
    "print(f\"Encoder input shape: {encoder_input_data.shape}\")\n",
    "print(f\"Decoder input shape: {decoder_input_data.shape}\")\n",
    "print(f\"Decoder target shape: {decoder_target_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Seq2Seq model\n",
    "latent_dim = 32\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = layers.Input(shape=(None, len(input_chars)))\n",
    "encoder = layers.LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = layers.Input(shape=(None, len(target_chars)))\n",
    "decoder_lstm = layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = layers.Dense(len(target_chars), activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Model\n",
    "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "history = model.fit(\n",
    "    [encoder_input_data, decoder_input_data],\n",
    "    decoder_target_data,\n",
    "    batch_size=2,\n",
    "    epochs=100,\n",
    "    validation_split=0.2,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(f\"Training complete!\")\n",
    "print(f\"Final accuracy: {history.history['accuracy'][-1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Attention Mechanism\n",
    "\n",
    "**Attention** allows the decoder to focus on relevant parts of the input.\n",
    "\n",
    "### Key Idea:\n",
    "Instead of compressing entire input into fixed vector:\n",
    "- Keep all encoder hidden states\n",
    "- At each decoding step, compute attention weights\n",
    "- Create context vector as weighted sum of encoder states\n",
    "\n",
    "### Attention Formula:\n",
    "$$\\text{attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "Where:\n",
    "- $Q$: Query (decoder state)\n",
    "- $K$: Keys (encoder states)\n",
    "- $V$: Values (encoder states)\n",
    "- $d_k$: Dimension of keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement attention layer\n",
    "class AttentionLayer(layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.W1 = layers.Dense(units)\n",
    "        self.W2 = layers.Dense(units)\n",
    "        self.V = layers.Dense(1)\n",
    "    \n",
    "    def call(self, query, values):\n",
    "        # query shape: (batch, hidden_dim)\n",
    "        # values shape: (batch, seq_len, hidden_dim)\n",
    "        \n",
    "        # Expand query to match values shape\n",
    "        query_with_time = tf.expand_dims(query, 1)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(query_with_time) + self.W2(values)\n",
    "        ))\n",
    "        \n",
    "        # Attention weights\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        \n",
    "        # Context vector\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        return context_vector, attention_weights\n",
    "\n",
    "print(\"Attention layer implemented!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Seq2Seq with Attention\n",
    "class Seq2SeqAttention(keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, units):\n",
    "        super(Seq2SeqAttention, self).__init__()\n",
    "        self.units = units\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder_embedding = layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.encoder_lstm = layers.LSTM(units, return_sequences=True, return_state=True)\n",
    "        \n",
    "        # Attention\n",
    "        self.attention = AttentionLayer(units)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder_embedding = layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.decoder_lstm = layers.LSTM(units, return_sequences=True, return_state=True)\n",
    "        self.decoder_dense = layers.Dense(vocab_size)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        encoder_input, decoder_input = inputs\n",
    "        \n",
    "        # Encode\n",
    "        encoder_emb = self.encoder_embedding(encoder_input)\n",
    "        encoder_output, state_h, state_c = self.encoder_lstm(encoder_emb)\n",
    "        \n",
    "        # Decode\n",
    "        decoder_emb = self.decoder_embedding(decoder_input)\n",
    "        decoder_output, _, _ = self.decoder_lstm(\n",
    "            decoder_emb, initial_state=[state_h, state_c]\n",
    "        )\n",
    "        \n",
    "        # Apply attention (simplified for demonstration)\n",
    "        # In full implementation, would apply at each decoder step\n",
    "        output = self.decoder_dense(decoder_output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "print(\"Seq2Seq with Attention model defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Self-Attention\n",
    "\n",
    "**Self-attention** computes attention within the same sequence.\n",
    "\n",
    "### Process:\n",
    "1. Create Query (Q), Key (K), Value (V) from input\n",
    "2. Compute attention scores: $QK^T$\n",
    "3. Scale by $\\sqrt{d_k}$\n",
    "4. Apply softmax\n",
    "5. Multiply by Values (V)\n",
    "\n",
    "### Result:\n",
    "Each position attends to all other positions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Scaled Dot-Product Attention\n",
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    \"\"\"\n",
    "    Calculate attention weights.\n",
    "    q, k, v: shape (batch, seq_len, d_k)\n",
    "    \"\"\"\n",
    "    # Compute attention scores\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "    \n",
    "    # Scale\n",
    "    d_k = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(d_k)\n",
    "    \n",
    "    # Apply mask (if provided)\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "    \n",
    "    # Softmax\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "    \n",
    "    # Apply to values\n",
    "    output = tf.matmul(attention_weights, v)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# Test\n",
    "seq_len = 5\n",
    "d_k = 8\n",
    "batch_size = 2\n",
    "\n",
    "q = tf.random.normal((batch_size, seq_len, d_k))\n",
    "k = tf.random.normal((batch_size, seq_len, d_k))\n",
    "v = tf.random.normal((batch_size, seq_len, d_k))\n",
    "\n",
    "output, weights = scaled_dot_product_attention(q, k, v)\n",
    "\n",
    "print(f\"Input shape: {q.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {weights.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention weights\n",
    "sample_weights = weights[0].numpy()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(sample_weights, annot=True, fmt='.2f', cmap='viridis',\n",
    "           xticklabels=range(seq_len), yticklabels=range(seq_len))\n",
    "plt.xlabel('Key Position')\n",
    "plt.ylabel('Query Position')\n",
    "plt.title('Self-Attention Weights')\n",
    "plt.show()\n",
    "\n",
    "print(\"Each row shows which positions a query attends to.\")\n",
    "print(\"Brighter colors = stronger attention.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Multi-Head Attention\n",
    "\n",
    "**Multi-head attention** runs multiple attention operations in parallel.\n",
    "\n",
    "### Benefits:\n",
    "- Attend to different aspects simultaneously\n",
    "- Learn different relationships\n",
    "- More expressive than single attention\n",
    "\n",
    "### Process:\n",
    "1. Linear projections for each head\n",
    "2. Apply attention in parallel\n",
    "3. Concatenate outputs\n",
    "4. Final linear projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Multi-Head Attention\n",
    "class MultiHeadAttention(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.depth = d_model // num_heads\n",
    "        \n",
    "        self.wq = layers.Dense(d_model)\n",
    "        self.wk = layers.Dense(d_model)\n",
    "        self.wv = layers.Dense(d_model)\n",
    "        \n",
    "        self.dense = layers.Dense(d_model)\n",
    "    \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split last dimension into (num_heads, depth).\"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, v, k, q, mask=None):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        \n",
    "        # Linear projections\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "        \n",
    "        # Split into multiple heads\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "        \n",
    "        # Attention\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask\n",
    "        )\n",
    "        \n",
    "        # Concatenate heads\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                     (batch_size, -1, self.d_model))\n",
    "        \n",
    "        # Final linear\n",
    "        output = self.dense(concat_attention)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Test\n",
    "d_model = 64\n",
    "num_heads = 8\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "x = tf.random.normal((2, 10, d_model))\n",
    "output, weights = mha(x, x, x)\n",
    "\n",
    "print(f\"Multi-Head Attention output shape: {output.shape}\")\n",
    "print(f\"Number of heads: {num_heads}\")\n",
    "print(f\"Depth per head: {d_model // num_heads}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Positional Encoding\n",
    "\n",
    "**Positional encoding** adds position information to embeddings.\n",
    "\n",
    "### Why needed?\n",
    "- Attention has no notion of order\n",
    "- Need to inject position information\n",
    "\n",
    "### Formula:\n",
    "$$PE_{(pos, 2i)} = \\sin(pos / 10000^{2i/d})$$\n",
    "$$PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i/d})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "    \"\"\"\n",
    "    Create positional encoding.\n",
    "    \"\"\"\n",
    "    angle_rads = np.arange(position)[:, np.newaxis] / np.power(\n",
    "        10000, (2 * (np.arange(d_model)[np.newaxis, :] // 2)) / np.float32(d_model)\n",
    "    )\n",
    "    \n",
    "    # Apply sin to even indices\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    \n",
    "    # Apply cos to odd indices\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    \n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "# Visualize\n",
    "pos_encoding = positional_encoding(50, 128)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
    "plt.xlabel('Depth')\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.title('Positional Encoding')\n",
    "plt.show()\n",
    "\n",
    "print(\"Positional encoding adds unique patterns for each position.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Seq2Seq** uses encoder-decoder architecture\n",
    "2. **Attention** allows focusing on relevant input parts\n",
    "3. **Self-attention** relates different positions in sequence\n",
    "4. **Scaled dot-product** prevents gradients from vanishing\n",
    "5. **Multi-head attention** captures different relationships\n",
    "6. **Positional encoding** adds order information\n",
    "7. **Attention is foundation** for transformers\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. **Bidirectional Encoder**: Implement bidirectional LSTM encoder\n",
    "2. **Beam Search**: Add beam search decoding\n",
    "3. **Attention Visualization**: Visualize attention for translation\n",
    "4. **Different Attention**: Implement Luong attention\n",
    "5. **Longer Sequences**: Train on longer translation pairs\n",
    "6. **Transformer Block**: Combine MHA with feed-forward\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In Lab 4, we'll explore:\n",
    "- Complete Transformer architecture\n",
    "- BERT and GPT models\n",
    "- Pre-training and fine-tuning\n",
    "- Using Hugging Face Transformers\n",
    "\n",
    "Excellent work! You now understand attention mechanisms that power modern NLP."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
