{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Introduction to Natural Language Processing\n",
    "\n",
    "Welcome to NLP! In this lab, we'll explore the fundamentals of text processing and traditional NLP techniques that form the foundation for modern language understanding.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will:\n",
    "- Preprocess text data effectively\n",
    "- Understand tokenization strategies\n",
    "- Implement Bag of Words and TF-IDF\n",
    "- Build text classifiers with traditional methods\n",
    "- Perform basic NLP tasks (POS tagging, NER)\n",
    "- Build a spam detection system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag, ne_chunk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Download required NLTK data\n",
    "for resource in ['punkt', 'stopwords', 'averaged_perceptron_tagger', \n",
    "                'wordnet', 'maxent_ne_chunker', 'words']:\n",
    "    try:\n",
    "        nltk.download(resource, quiet=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Text Preprocessing\n",
    "\n",
    "Text preprocessing is crucial for NLP. Common steps:\n",
    "\n",
    "1. **Lowercasing**: Normalize case\n",
    "2. **Tokenization**: Split into words/sentences\n",
    "3. **Remove punctuation**: Clean special characters\n",
    "4. **Remove stop words**: Filter common words\n",
    "5. **Stemming/Lemmatization**: Reduce to root form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text\n",
    "text = \"\"\"Natural Language Processing (NLP) is a fascinating field of AI! \n",
    "It enables computers to understand, interpret, and generate human language. \n",
    "Modern NLP systems are incredibly powerful, aren't they?\"\"\"\n",
    "\n",
    "print(\"Original text:\")\n",
    "print(text)\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# 1. Lowercasing\n",
    "text_lower = text.lower()\n",
    "print(\"1. Lowercased:\")\n",
    "print(text_lower)\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# 2. Tokenization\n",
    "tokens = word_tokenize(text)\n",
    "print(\"2. Tokens (words):\")\n",
    "print(tokens[:20])\n",
    "print(f\"Total tokens: {len(tokens)}\")\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# 3. Remove punctuation\n",
    "tokens_no_punct = [token for token in tokens if token.isalnum()]\n",
    "print(\"3. After removing punctuation:\")\n",
    "print(tokens_no_punct)\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# 4. Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens_no_stop = [token.lower() for token in tokens_no_punct \n",
    "                  if token.lower() not in stop_words]\n",
    "print(\"4. After removing stop words:\")\n",
    "print(tokens_no_stop)\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# 5a. Stemming (aggressive)\n",
    "stemmer = PorterStemmer()\n",
    "stems = [stemmer.stem(token) for token in tokens_no_stop]\n",
    "print(\"5a. Stemming:\")\n",
    "print(stems)\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# 5b. Lemmatization (smarter)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmas = [lemmatizer.lemmatize(token) for token in tokens_no_stop]\n",
    "print(\"5b. Lemmatization:\")\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete preprocessing function\n",
    "def preprocess_text(text, remove_stopwords=True, use_lemmatization=True):\n",
    "    \"\"\"\n",
    "    Complete text preprocessing pipeline.\n",
    "    \"\"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove punctuation and non-alphanumeric\n",
    "    tokens = [token for token in tokens if token.isalnum()]\n",
    "    \n",
    "    # Remove stop words\n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Lemmatization or stemming\n",
    "    if use_lemmatization:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    else:\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Test\n",
    "processed = preprocess_text(text)\n",
    "print(\"Fully processed text:\")\n",
    "print(processed)\n",
    "print(f\"\\nOriginal tokens: {len(word_tokenize(text))}\")\n",
    "print(f\"Processed tokens: {len(processed)}\")\n",
    "print(f\"Reduction: {(1 - len(processed)/len(word_tokenize(text)))*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Bag of Words (BoW)\n",
    "\n",
    "**Bag of Words** represents text as word occurrence counts, ignoring grammar and order.\n",
    "\n",
    "### Process:\n",
    "1. Create vocabulary from all documents\n",
    "2. Count word occurrences in each document\n",
    "3. Represent document as vector of counts\n",
    "\n",
    "### Example:\n",
    "- Doc1: \"I love NLP\"\n",
    "- Doc2: \"I love AI\"\n",
    "- Vocabulary: [\"I\", \"love\", \"NLP\", \"AI\"]\n",
    "- Doc1 vector: [1, 1, 1, 0]\n",
    "- Doc2 vector: [1, 1, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement BoW from scratch\n",
    "class BagOfWords:\n",
    "    def __init__(self):\n",
    "        self.vocabulary = {}\n",
    "        self.vocab_list = []\n",
    "    \n",
    "    def fit(self, documents):\n",
    "        \"\"\"Build vocabulary from documents.\"\"\"\n",
    "        all_words = set()\n",
    "        for doc in documents:\n",
    "            tokens = preprocess_text(doc)\n",
    "            all_words.update(tokens)\n",
    "        \n",
    "        self.vocab_list = sorted(list(all_words))\n",
    "        self.vocabulary = {word: idx for idx, word in enumerate(self.vocab_list)}\n",
    "    \n",
    "    def transform(self, documents):\n",
    "        \"\"\"Convert documents to BoW vectors.\"\"\"\n",
    "        vectors = []\n",
    "        \n",
    "        for doc in documents:\n",
    "            tokens = preprocess_text(doc)\n",
    "            vector = np.zeros(len(self.vocabulary))\n",
    "            \n",
    "            for token in tokens:\n",
    "                if token in self.vocabulary:\n",
    "                    vector[self.vocabulary[token]] += 1\n",
    "            \n",
    "            vectors.append(vector)\n",
    "        \n",
    "        return np.array(vectors)\n",
    "    \n",
    "    def fit_transform(self, documents):\n",
    "        \"\"\"Fit and transform in one step.\"\"\"\n",
    "        self.fit(documents)\n",
    "        return self.transform(documents)\n",
    "\n",
    "# Test BoW\n",
    "documents = [\n",
    "    \"I love machine learning\",\n",
    "    \"Machine learning is amazing\",\n",
    "    \"I love deep learning\",\n",
    "    \"Deep learning and machine learning are related\"\n",
    "]\n",
    "\n",
    "bow = BagOfWords()\n",
    "bow_vectors = bow.fit_transform(documents)\n",
    "\n",
    "print(\"Vocabulary:\")\n",
    "print(bow.vocab_list)\n",
    "print(f\"\\nVocabulary size: {len(bow.vocab_list)}\")\n",
    "print(f\"\\nBoW vectors shape: {bow_vectors.shape}\")\n",
    "print(\"\\nBoW representation:\")\n",
    "bow_df = pd.DataFrame(bow_vectors, columns=bow.vocab_list)\n",
    "print(bow_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize BoW\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(bow_df, annot=True, fmt='.0f', cmap='YlOrRd', cbar_kws={'label': 'Count'})\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Documents')\n",
    "plt.title('Bag of Words Representation')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nBoW Characteristics:\")\n",
    "print(\"✓ Simple and interpretable\")\n",
    "print(\"✓ Works well for short texts\")\n",
    "print(\"✗ Loses word order\")\n",
    "print(\"✗ High dimensionality\")\n",
    "print(\"✗ Doesn't capture semantics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "\n",
    "TF-IDF weighs words by importance:\n",
    "- Common words (\"the\", \"is\") get low weight\n",
    "- Rare but meaningful words get high weight\n",
    "\n",
    "### Formula:\n",
    "$$\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t)$$\n",
    "\n",
    "Where:\n",
    "- $\\text{TF}(t, d) = \\frac{\\text{count of } t \\text{ in } d}{\\text{total words in } d}$\n",
    "- $\\text{IDF}(t) = \\log\\frac{\\text{total documents}}{\\text{documents containing } t}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement TF-IDF from scratch\n",
    "class TFIDF:\n",
    "    def __init__(self):\n",
    "        self.vocabulary = {}\n",
    "        self.vocab_list = []\n",
    "        self.idf = None\n",
    "    \n",
    "    def fit(self, documents):\n",
    "        \"\"\"Calculate IDF values.\"\"\"\n",
    "        # Build vocabulary\n",
    "        all_words = set()\n",
    "        for doc in documents:\n",
    "            tokens = preprocess_text(doc)\n",
    "            all_words.update(tokens)\n",
    "        \n",
    "        self.vocab_list = sorted(list(all_words))\n",
    "        self.vocabulary = {word: idx for idx, word in enumerate(self.vocab_list)}\n",
    "        \n",
    "        # Calculate IDF\n",
    "        n_docs = len(documents)\n",
    "        doc_counts = np.zeros(len(self.vocabulary))\n",
    "        \n",
    "        for doc in documents:\n",
    "            tokens = set(preprocess_text(doc))\n",
    "            for token in tokens:\n",
    "                if token in self.vocabulary:\n",
    "                    doc_counts[self.vocabulary[token]] += 1\n",
    "        \n",
    "        self.idf = np.log(n_docs / (doc_counts + 1))  # +1 for smoothing\n",
    "    \n",
    "    def transform(self, documents):\n",
    "        \"\"\"Convert documents to TF-IDF vectors.\"\"\"\n",
    "        vectors = []\n",
    "        \n",
    "        for doc in documents:\n",
    "            tokens = preprocess_text(doc)\n",
    "            \n",
    "            # Calculate TF\n",
    "            tf = np.zeros(len(self.vocabulary))\n",
    "            for token in tokens:\n",
    "                if token in self.vocabulary:\n",
    "                    tf[self.vocabulary[token]] += 1\n",
    "            \n",
    "            if len(tokens) > 0:\n",
    "                tf = tf / len(tokens)\n",
    "            \n",
    "            # Calculate TF-IDF\n",
    "            tfidf = tf * self.idf\n",
    "            vectors.append(tfidf)\n",
    "        \n",
    "        return np.array(vectors)\n",
    "    \n",
    "    def fit_transform(self, documents):\n",
    "        \"\"\"Fit and transform in one step.\"\"\"\n",
    "        self.fit(documents)\n",
    "        return self.transform(documents)\n",
    "\n",
    "# Test TF-IDF\n",
    "tfidf = TFIDF()\n",
    "tfidf_vectors = tfidf.fit_transform(documents)\n",
    "\n",
    "print(\"TF-IDF vectors shape:\", tfidf_vectors.shape)\n",
    "print(\"\\nTF-IDF representation:\")\n",
    "tfidf_df = pd.DataFrame(tfidf_vectors, columns=tfidf.vocab_list)\n",
    "print(tfidf_df.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare BoW vs TF-IDF\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "sns.heatmap(bow_df, annot=True, fmt='.0f', cmap='YlOrRd', ax=axes[0])\n",
    "axes[0].set_title('Bag of Words')\n",
    "axes[0].set_xlabel('Words')\n",
    "axes[0].set_ylabel('Documents')\n",
    "\n",
    "sns.heatmap(tfidf_df, annot=True, fmt='.2f', cmap='YlGnBu', ax=axes[1])\n",
    "axes[1].set_title('TF-IDF')\n",
    "axes[1].set_xlabel('Words')\n",
    "axes[1].set_ylabel('Documents')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTF-IDF Benefits:\")\n",
    "print(\"✓ Reduces weight of common words\")\n",
    "print(\"✓ Highlights distinctive words\")\n",
    "print(\"✓ Better for text classification\")\n",
    "print(\"✓ More informative than raw counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Text Classification - Spam Detection\n",
    "\n",
    "Let's build a spam detector using traditional NLP methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample spam dataset\n",
    "spam_data = [\n",
    "    (\"Win a free iPhone now! Click here!\", \"spam\"),\n",
    "    (\"Meeting tomorrow at 3pm\", \"ham\"),\n",
    "    (\"Congratulations! You've won $1000!\", \"spam\"),\n",
    "    (\"Can you send me the report?\", \"ham\"),\n",
    "    (\"FREE MONEY!!! Act now!!!\", \"spam\"),\n",
    "    (\"Let's grab lunch tomorrow\", \"ham\"),\n",
    "    (\"You are the lucky winner!\", \"spam\"),\n",
    "    (\"Project deadline is Friday\", \"ham\"),\n",
    "    (\"Click here for amazing deals!!!\", \"spam\"),\n",
    "    (\"Thanks for your help\", \"ham\"),\n",
    "    (\"Limited time offer! Buy now!\", \"spam\"),\n",
    "    (\"See you at the conference\", \"ham\"),\n",
    "    (\"Get rich quick scheme!\", \"spam\"),\n",
    "    (\"Please review the document\", \"ham\"),\n",
    "    (\"Claim your prize immediately!!!\", \"spam\"),\n",
    "    (\"Coffee meeting at 10am?\", \"ham\"),\n",
    "]\n",
    "\n",
    "# Add more examples\n",
    "more_spam = [\n",
    "    (\"Earn money from home!\", \"spam\"),\n",
    "    (\"Weekend plans?\", \"ham\"),\n",
    "    (\"You've been selected for a special offer\", \"spam\"),\n",
    "    (\"Meeting notes attached\", \"ham\"),\n",
    "]\n",
    "\n",
    "spam_data.extend(more_spam)\n",
    "\n",
    "texts = [text for text, label in spam_data]\n",
    "labels = [label for text, label in spam_data]\n",
    "\n",
    "print(f\"Total messages: {len(texts)}\")\n",
    "print(f\"Spam: {labels.count('spam')}\")\n",
    "print(f\"Ham (not spam): {labels.count('ham')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    texts, labels, test_size=0.3, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "# Create TF-IDF features using sklearn\n",
    "vectorizer = TfidfVectorizer(max_features=100, stop_words='english')\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"Training set: {X_train_tfidf.shape}\")\n",
    "print(f\"Test set: {X_test_tfidf.shape}\")\n",
    "print(f\"Vocabulary size: {len(vectorizer.vocabulary_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Train Logistic Regression\n",
    "lr_classifier = LogisticRegression(max_iter=1000)\n",
    "lr_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predictions\n",
    "nb_pred = nb_classifier.predict(X_test_tfidf)\n",
    "lr_pred = lr_classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Naive Bayes Results:\")\n",
    "print(classification_report(y_test, nb_pred))\n",
    "\n",
    "print(\"\\nLogistic Regression Results:\")\n",
    "print(classification_report(y_test, lr_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on new messages\n",
    "new_messages = [\n",
    "    \"Congratulations! You won a free vacation!\",\n",
    "    \"Can we reschedule our meeting?\",\n",
    "    \"Click here to claim your reward!!!\",\n",
    "    \"Thanks for the update\"\n",
    "]\n",
    "\n",
    "new_tfidf = vectorizer.transform(new_messages)\n",
    "predictions = nb_classifier.predict(new_tfidf)\n",
    "probabilities = nb_classifier.predict_proba(new_tfidf)\n",
    "\n",
    "print(\"Predictions on new messages:\\n\")\n",
    "for msg, pred, prob in zip(new_messages, predictions, probabilities):\n",
    "    spam_prob = prob[1] if pred == 'spam' else prob[0]\n",
    "    print(f\"Message: {msg}\")\n",
    "    print(f\"Prediction: {pred.upper()}\")\n",
    "    print(f\"Confidence: {spam_prob:.2%}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: POS Tagging and NER\n",
    "\n",
    "**Part-of-Speech (POS) Tagging**: Identify word types (noun, verb, etc.)\n",
    "\n",
    "**Named Entity Recognition (NER)**: Identify entities (person, organization, location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS Tagging\n",
    "sample_text = \"Apple Inc. was founded by Steve Jobs in California. The company released the iPhone in 2007.\"\n",
    "\n",
    "tokens = word_tokenize(sample_text)\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "print(\"POS Tagging:\")\n",
    "for word, tag in pos_tags:\n",
    "    print(f\"{word:15s} -> {tag}\")\n",
    "\n",
    "print(\"\\nCommon POS tags:\")\n",
    "print(\"NN: Noun, VB: Verb, JJ: Adjective\")\n",
    "print(\"RB: Adverb, DT: Determiner, IN: Preposition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named Entity Recognition\n",
    "named_entities = ne_chunk(pos_tags)\n",
    "\n",
    "print(\"\\nNamed Entities:\")\n",
    "for chunk in named_entities:\n",
    "    if hasattr(chunk, 'label'):\n",
    "        entity_text = ' '.join(c[0] for c in chunk)\n",
    "        entity_label = chunk.label()\n",
    "        print(f\"{entity_text:20s} -> {entity_label}\")\n",
    "\n",
    "print(\"\\nEntity types:\")\n",
    "print(\"PERSON: People\")\n",
    "print(\"ORGANIZATION: Companies, agencies\")\n",
    "print(\"GPE: Geopolitical entities (countries, cities)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Preprocessing** is crucial for NLP success\n",
    "2. **Tokenization** converts text to processable units\n",
    "3. **Stop words** removal reduces noise\n",
    "4. **Stemming** is fast but crude, **lemmatization** is smarter\n",
    "5. **Bag of Words** is simple but loses word order\n",
    "6. **TF-IDF** weighs words by importance\n",
    "7. **Traditional methods** work well for simple tasks\n",
    "8. **POS tagging** identifies grammatical roles\n",
    "9. **NER** extracts entities from text\n",
    "10. **Feature engineering** matters for classic ML\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. **Custom Tokenizer**: Build a tokenizer that handles contractions\n",
    "2. **N-grams**: Implement bigrams and trigrams for better context\n",
    "3. **Sentiment Analysis**: Build a movie review classifier\n",
    "4. **Language Detection**: Classify text by language\n",
    "5. **Topic Modeling**: Use TF-IDF to find document topics\n",
    "6. **Text Similarity**: Compute cosine similarity between documents\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In Lab 2, we'll explore:\n",
    "- Word embeddings (Word2Vec, GloVe)\n",
    "- Dense representations\n",
    "- Semantic relationships\n",
    "- Neural language models\n",
    "\n",
    "Great work! You now understand the foundations of NLP."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
