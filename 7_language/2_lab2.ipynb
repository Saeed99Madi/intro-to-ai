{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Word Embeddings and Language Models\n",
    "\n",
    "In this lab, we'll explore dense vector representations of words and build neural language models. Word embeddings capture semantic meaning in continuous vector space.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will:\n",
    "- Understand limitations of one-hot encoding\n",
    "- Implement Word2Vec (Skip-gram)\n",
    "- Use pre-trained embeddings (GloVe)\n",
    "- Explore word analogies and relationships\n",
    "- Build RNN language models\n",
    "- Generate text with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: One-Hot Encoding Problems\n",
    "\n",
    "**One-hot encoding** represents each word as a vector with all zeros except one 1.\n",
    "\n",
    "### Problems:\n",
    "1. **High dimensionality**: Vocabulary size = vector size\n",
    "2. **No semantic relationship**: All words equally distant\n",
    "3. **Sparse**: Mostly zeros\n",
    "4. **Can't generalize**: \"cat\" and \"kitten\" treated as unrelated\n",
    "\n",
    "### Solution: Dense Embeddings\n",
    "- Low-dimensional (50-300 dimensions)\n",
    "- Captures semantic relationships\n",
    "- Similar words have similar vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate one-hot encoding limitations\n",
    "vocab = ['king', 'queen', 'man', 'woman', 'cat', 'dog']\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Create one-hot vectors\n",
    "one_hot = np.eye(vocab_size)\n",
    "\n",
    "print(\"One-Hot Encoding:\")\n",
    "for word, vec in zip(vocab, one_hot):\n",
    "    print(f\"{word:10s}: {vec}\")\n",
    "\n",
    "# Calculate cosine similarity\n",
    "def cosine_similarity(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "print(\"\\nCosine Similarities (One-Hot):\")\n",
    "print(f\"king vs queen: {cosine_similarity(one_hot[0], one_hot[1]):.3f}\")\n",
    "print(f\"king vs cat: {cosine_similarity(one_hot[0], one_hot[4]):.3f}\")\n",
    "print(f\"man vs woman: {cosine_similarity(one_hot[2], one_hot[3]):.3f}\")\n",
    "print(\"\\nAll pairs are equally dissimilar (0.0)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Word2Vec - Skip-gram Model\n",
    "\n",
    "**Word2Vec** learns embeddings by predicting context words from target word.\n",
    "\n",
    "### Skip-gram:\n",
    "- Input: Target word\n",
    "- Output: Context words (surrounding words)\n",
    "- Learning: Words appearing in similar contexts get similar embeddings\n",
    "\n",
    "### Architecture:\n",
    "1. Input: One-hot encoded word\n",
    "2. Embedding layer: Projects to dense vector\n",
    "3. Output: Probability distribution over vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Word2Vec implementation\n",
    "class SimpleWord2Vec:\n",
    "    def __init__(self, embedding_dim=50, window_size=2):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.window_size = window_size\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.embeddings = None\n",
    "    \n",
    "    def build_vocab(self, sentences):\n",
    "        \"\"\"Build vocabulary from sentences.\"\"\"\n",
    "        words = []\n",
    "        for sentence in sentences:\n",
    "            words.extend(sentence.lower().split())\n",
    "        \n",
    "        vocab = sorted(set(words))\n",
    "        self.word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n",
    "        self.vocab_size = len(vocab)\n",
    "    \n",
    "    def generate_training_data(self, sentences):\n",
    "        \"\"\"Generate skip-gram training pairs.\"\"\"\n",
    "        pairs = []\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            words = sentence.lower().split()\n",
    "            word_indices = [self.word2idx[w] for w in words if w in self.word2idx]\n",
    "            \n",
    "            for center_idx, center_word in enumerate(word_indices):\n",
    "                # Get context window\n",
    "                start = max(0, center_idx - self.window_size)\n",
    "                end = min(len(word_indices), center_idx + self.window_size + 1)\n",
    "                \n",
    "                for context_idx in range(start, end):\n",
    "                    if context_idx != center_idx:\n",
    "                        pairs.append((center_word, word_indices[context_idx]))\n",
    "        \n",
    "        return np.array(pairs)\n",
    "    \n",
    "    def train(self, sentences, epochs=10):\n",
    "        \"\"\"Train Word2Vec model.\"\"\"\n",
    "        self.build_vocab(sentences)\n",
    "        training_data = self.generate_training_data(sentences)\n",
    "        \n",
    "        # Build model\n",
    "        input_target = layers.Input(shape=(1,))\n",
    "        embedding = layers.Embedding(self.vocab_size, self.embedding_dim, \n",
    "                                    input_length=1, name='embedding')(input_target)\n",
    "        embedding = layers.Flatten()(embedding)\n",
    "        output = layers.Dense(self.vocab_size, activation='softmax')(embedding)\n",
    "        \n",
    "        model = keras.Model(inputs=input_target, outputs=output)\n",
    "        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "        \n",
    "        # Train\n",
    "        X = training_data[:, 0]\n",
    "        y = training_data[:, 1]\n",
    "        \n",
    "        model.fit(X, y, epochs=epochs, batch_size=32, verbose=0)\n",
    "        \n",
    "        # Extract embeddings\n",
    "        embedding_layer = model.get_layer('embedding')\n",
    "        self.embeddings = embedding_layer.get_weights()[0]\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def get_embedding(self, word):\n",
    "        \"\"\"Get embedding for a word.\"\"\"\n",
    "        if word not in self.word2idx:\n",
    "            return None\n",
    "        return self.embeddings[self.word2idx[word]]\n",
    "    \n",
    "    def most_similar(self, word, top_n=5):\n",
    "        \"\"\"Find most similar words.\"\"\"\n",
    "        if word not in self.word2idx:\n",
    "            return []\n",
    "        \n",
    "        word_vec = self.get_embedding(word)\n",
    "        similarities = []\n",
    "        \n",
    "        for other_word in self.word2idx:\n",
    "            if other_word != word:\n",
    "                other_vec = self.get_embedding(other_word)\n",
    "                sim = cosine_similarity(word_vec, other_vec)\n",
    "                similarities.append((other_word, sim))\n",
    "        \n",
    "        return sorted(similarities, key=lambda x: x[1], reverse=True)[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec on sample corpus\n",
    "corpus = [\n",
    "    \"the king loves the queen\",\n",
    "    \"the queen loves the king\",\n",
    "    \"the man loves the woman\",\n",
    "    \"the woman loves the man\",\n",
    "    \"the cat chases the mouse\",\n",
    "    \"the dog chases the cat\",\n",
    "    \"the mouse runs from the cat\",\n",
    "    \"the king and queen rule the kingdom\",\n",
    "    \"the man and woman walk together\",\n",
    "    \"cats and dogs are pets\"\n",
    "]\n",
    "\n",
    "# Train model\n",
    "w2v = SimpleWord2Vec(embedding_dim=20, window_size=2)\n",
    "model = w2v.train(corpus, epochs=50)\n",
    "\n",
    "print(\"Word2Vec trained!\")\n",
    "print(f\"Vocabulary size: {w2v.vocab_size}\")\n",
    "print(f\"Embedding dimension: {w2v.embedding_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test word similarities\n",
    "test_words = ['king', 'queen', 'man', 'woman', 'cat', 'dog']\n",
    "\n",
    "print(\"\\nWord Similarities:\\n\")\n",
    "for word in test_words:\n",
    "    if word in w2v.word2idx:\n",
    "        similar = w2v.most_similar(word, top_n=3)\n",
    "        print(f\"{word:10s}: {[f'{w}({s:.2f})' for w, s in similar]}\")\n",
    "\n",
    "# Test specific similarities\n",
    "print(\"\\n\\nSpecific Similarities:\")\n",
    "pairs = [('king', 'queen'), ('man', 'woman'), ('cat', 'dog')]\n",
    "for w1, w2 in pairs:\n",
    "    if w1 in w2v.word2idx and w2 in w2v.word2idx:\n",
    "        v1 = w2v.get_embedding(w1)\n",
    "        v2 = w2v.get_embedding(w2)\n",
    "        sim = cosine_similarity(v1, v2)\n",
    "        print(f\"{w1} <-> {w2}: {sim:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize embeddings with PCA\n",
    "if w2v.embeddings is not None:\n",
    "    # Reduce to 2D\n",
    "    pca = PCA(n_components=2)\n",
    "    embeddings_2d = pca.fit_transform(w2v.embeddings)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.5)\n",
    "    \n",
    "    # Label words\n",
    "    for idx, word in w2v.idx2word.items():\n",
    "        plt.annotate(word, (embeddings_2d[idx, 0], embeddings_2d[idx, 1]),\n",
    "                    fontsize=12, alpha=0.8)\n",
    "    \n",
    "    plt.xlabel('PC 1')\n",
    "    plt.ylabel('PC 2')\n",
    "    plt.title('Word Embeddings Visualization (PCA)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Similar words cluster together in embedding space!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Word Analogies\n",
    "\n",
    "Famous property of word embeddings:\n",
    "$$\\text{king} - \\text{man} + \\text{woman} \\approx \\text{queen}$$\n",
    "\n",
    "This shows embeddings capture **semantic relationships**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_analogy(w2v, a, b, c, top_n=5):\n",
    "    \"\"\"\n",
    "    Solve analogy: a is to b as c is to ?\n",
    "    Using vector arithmetic: b - a + c\n",
    "    \"\"\"\n",
    "    words = [a, b, c]\n",
    "    if not all(w in w2v.word2idx for w in words):\n",
    "        return None\n",
    "    \n",
    "    # Get vectors\n",
    "    va = w2v.get_embedding(a)\n",
    "    vb = w2v.get_embedding(b)\n",
    "    vc = w2v.get_embedding(c)\n",
    "    \n",
    "    # Compute target vector\n",
    "    target = vb - va + vc\n",
    "    \n",
    "    # Find closest words\n",
    "    similarities = []\n",
    "    for word in w2v.word2idx:\n",
    "        if word not in [a, b, c]:\n",
    "            vec = w2v.get_embedding(word)\n",
    "            sim = cosine_similarity(target, vec)\n",
    "            similarities.append((word, sim))\n",
    "    \n",
    "    return sorted(similarities, key=lambda x: x[1], reverse=True)[:top_n]\n",
    "\n",
    "# Test analogies\n",
    "print(\"Word Analogies:\\n\")\n",
    "\n",
    "analogies = [\n",
    "    ('king', 'man', 'woman'),  # king - man + woman = ?\n",
    "    ('king', 'queen', 'man'),  # king - queen + man = ?\n",
    "]\n",
    "\n",
    "for a, b, c in analogies:\n",
    "    result = solve_analogy(w2v, a, b, c, top_n=3)\n",
    "    if result:\n",
    "        print(f\"{a} - {b} + {c} =\")\n",
    "        for word, score in result:\n",
    "            print(f\"  {word}: {score:.3f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: RNN Language Model\n",
    "\n",
    "A **language model** predicts the next word given previous words.\n",
    "\n",
    "### RNN approach:\n",
    "- Input: Sequence of words (embeddings)\n",
    "- Hidden state: Captures context\n",
    "- Output: Probability distribution over next word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for language modeling\n",
    "text = \" \".join(corpus)\n",
    "words = text.lower().split()\n",
    "\n",
    "# Build vocabulary\n",
    "vocab = sorted(set(words))\n",
    "word2idx_lm = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx2word_lm = {idx: word for word, idx in word2idx_lm.items()}\n",
    "vocab_size_lm = len(vocab)\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size_lm}\")\n",
    "print(f\"Total words: {len(words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training sequences\n",
    "seq_length = 3\n",
    "X_seq, y_seq = [], []\n",
    "\n",
    "for i in range(len(words) - seq_length):\n",
    "    seq_in = words[i:i+seq_length]\n",
    "    seq_out = words[i+seq_length]\n",
    "    \n",
    "    X_seq.append([word2idx_lm[w] for w in seq_in])\n",
    "    y_seq.append(word2idx_lm[seq_out])\n",
    "\n",
    "X_seq = np.array(X_seq)\n",
    "y_seq = np.array(y_seq)\n",
    "\n",
    "print(f\"Training sequences: {len(X_seq)}\")\n",
    "print(f\"Sequence shape: {X_seq.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LSTM language model\n",
    "model_lm = keras.Sequential([\n",
    "    layers.Embedding(vocab_size_lm, 32, input_length=seq_length),\n",
    "    layers.LSTM(64, return_sequences=False),\n",
    "    layers.Dense(vocab_size_lm, activation='softmax')\n",
    "])\n",
    "\n",
    "model_lm.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train\n",
    "history = model_lm.fit(\n",
    "    X_seq, y_seq,\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"Language model trained!\")\n",
    "print(f\"Final accuracy: {history.history['accuracy'][-1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text\n",
    "def generate_text(model, start_seq, length=10, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generate text using the language model.\n",
    "    \"\"\"\n",
    "    generated = start_seq.copy()\n",
    "    \n",
    "    for _ in range(length):\n",
    "        # Prepare input\n",
    "        seq = generated[-seq_length:]\n",
    "        seq_encoded = np.array([[word2idx_lm[w] for w in seq]])\n",
    "        \n",
    "        # Predict next word\n",
    "        predictions = model.predict(seq_encoded, verbose=0)[0]\n",
    "        predictions = np.log(predictions + 1e-7) / temperature\n",
    "        exp_preds = np.exp(predictions)\n",
    "        predictions = exp_preds / np.sum(exp_preds)\n",
    "        \n",
    "        next_idx = np.random.choice(len(predictions), p=predictions)\n",
    "        next_word = idx2word_lm[next_idx]\n",
    "        generated.append(next_word)\n",
    "    \n",
    "    return ' '.join(generated)\n",
    "\n",
    "# Test generation\n",
    "start_sequences = [\n",
    "    ['the', 'king', 'loves'],\n",
    "    ['the', 'cat', 'chases'],\n",
    "    ['the', 'man', 'and']\n",
    "]\n",
    "\n",
    "print(\"\\nText Generation:\\n\")\n",
    "for start in start_sequences:\n",
    "    generated = generate_text(model_lm, start, length=7, temperature=0.8)\n",
    "    print(f\"Start: {' '.join(start)}\")\n",
    "    print(f\"Generated: {generated}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Word embeddings** capture semantic meaning in dense vectors\n",
    "2. **Word2Vec** learns by predicting context words\n",
    "3. **Similar words** have similar embeddings (cosine similarity)\n",
    "4. **Word analogies** work through vector arithmetic\n",
    "5. **RNN language models** predict next words given context\n",
    "6. **LSTM** handles longer sequences better than RNN\n",
    "7. **Temperature** controls generation randomness\n",
    "8. **Embeddings** are foundation for modern NLP\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. **Larger Corpus**: Train Word2Vec on Wikipedia text\n",
    "2. **GloVe**: Download and use pre-trained GloVe embeddings\n",
    "3. **Subword Embeddings**: Implement character-level or BPE\n",
    "4. **Bidirectional LSTM**: Improve language model\n",
    "5. **Perplexity**: Calculate language model perplexity\n",
    "6. **Transfer**: Use embeddings in downstream task\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In Lab 3, we'll explore:\n",
    "- Sequence-to-sequence models\n",
    "- Attention mechanisms\n",
    "- Neural machine translation\n",
    "- Multi-head attention\n",
    "\n",
    "Great work! You now understand how to represent words as dense vectors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
