{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Lab 4: Transformers and Modern NLP\n",
    "\n",
    "In this lab, we'll explore the transformer architecture that revolutionized NLP and learn to use state-of-the-art pre-trained models like BERT and GPT.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will:\n",
    "- Understand the complete transformer architecture\n",
    "- Implement transformer encoder and decoder blocks\n",
    "- Learn about BERT and GPT architectures\n",
    "- Use Hugging Face Transformers library\n",
    "- Fine-tune pre-trained models for specific tasks\n",
    "- Perform text classification, generation, and Q&A\n",
    "- Evaluate models with perplexity, BLEU, and ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Part 1: Building a Complete Transformer\n",
    "\n",
    "The **Transformer** architecture (\"Attention is All You Need\", 2017) uses self-attention exclusively, without recurrence.\n",
    "\n",
    "### Key Components:\n",
    "1. **Multi-Head Attention**: Parallel attention mechanisms\n",
    "2. **Feed-Forward Networks**: Position-wise fully connected layers\n",
    "3. **Layer Normalization**: Normalize layer outputs\n",
    "4. **Residual Connections**: Skip connections for gradient flow\n",
    "5. **Positional Encoding**: Inject sequence order information\n",
    "\n",
    "### Architecture:\n",
    "```\n",
    "Encoder:                    Decoder:\n",
    "Input Embedding             Output Embedding\n",
    "+ Positional Encoding       + Positional Encoding\n",
    "â†“                           â†“\n",
    "Multi-Head Self-Attention   Masked Multi-Head Self-Attention\n",
    "+ Residual + Norm           + Residual + Norm\n",
    "â†“                           â†“\n",
    "Feed Forward                Cross-Attention (to encoder)\n",
    "+ Residual + Norm           + Residual + Norm\n",
    "                            â†“\n",
    "                            Feed Forward\n",
    "                            + Residual + Norm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "    \"\"\"\n",
    "    Create positional encoding matrix.\n",
    "    \n",
    "    PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n",
    "    PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "    \"\"\"\n",
    "    angle_rads = np.arange(position)[:, np.newaxis] / np.power(\n",
    "        10000, (2 * (np.arange(d_model)[np.newaxis, :] // 2)) / np.float32(d_model)\n",
    "    )\n",
    "    \n",
    "    # Apply sin to even indices\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    \n",
    "    # Apply cos to odd indices\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    \n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "# Visualize positional encoding\n",
    "pos_enc = positional_encoding(50, 128)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.pcolormesh(pos_enc[0], cmap='RdBu')\n",
    "plt.xlabel('Dimension')\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.title('Positional Encoding (50 positions, 128 dimensions)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Positional encoding allows the model to use sequence order!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    \"\"\"\n",
    "    Calculate scaled dot-product attention.\n",
    "    \n",
    "    Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) V\n",
    "    \"\"\"\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "    \n",
    "    # Scale by sqrt(d_k)\n",
    "    d_k = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(d_k)\n",
    "    \n",
    "    # Apply mask (if provided)\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "    \n",
    "    # Softmax over last axis\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "    \n",
    "    output = tf.matmul(attention_weights, v)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "\n",
    "class MultiHeadAttention(layers.Layer):\n",
    "    \"\"\"Multi-head attention layer.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.depth = d_model // num_heads\n",
    "        \n",
    "        self.wq = layers.Dense(d_model)\n",
    "        self.wk = layers.Dense(d_model)\n",
    "        self.wv = layers.Dense(d_model)\n",
    "        \n",
    "        self.dense = layers.Dense(d_model)\n",
    "    \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, v, k, q, mask=None):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        \n",
    "        # Linear projections\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "        \n",
    "        # Split into multiple heads\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask\n",
    "        )\n",
    "        \n",
    "        # Concatenate heads\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                      (batch_size, -1, self.d_model))\n",
    "        \n",
    "        # Final linear projection\n",
    "        output = self.dense(concat_attention)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    \"\"\"Position-wise feed-forward network.\"\"\"\n",
    "    return keras.Sequential([\n",
    "        layers.Dense(dff, activation='relu'),\n",
    "        layers.Dense(d_model)\n",
    "    ])\n",
    "\n",
    "print(\"Transformer components defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(layers.Layer):\n",
    "    \"\"\"Single transformer encoder layer.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "        \n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dropout1 = layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = layers.Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, x, training, mask=None):\n",
    "        # Multi-head attention\n",
    "        attn_output, _ = self.mha(x, x, x, mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # Residual connection\n",
    "        \n",
    "        # Feed forward\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # Residual connection\n",
    "        \n",
    "        return out2\n",
    "\n",
    "\n",
    "class Encoder(layers.Layer):\n",
    "    \"\"\"Transformer encoder (stack of encoder layers).\"\"\"\n",
    "    \n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, \n",
    "                 input_vocab_size, maximum_position_encoding, dropout_rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "        \n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, dropout_rate) \n",
    "                           for _ in range(num_layers)]\n",
    "        \n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, x, training, mask=None):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        \n",
    "        # Embedding + positional encoding\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        \n",
    "        x = self.dropout(x, training=training)\n",
    "        \n",
    "        # Pass through encoder layers\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "        \n",
    "        return x\n",
    "\n",
    "print(\"Encoder defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(layers.Layer):\n",
    "    \"\"\"Single transformer decoder layer.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)  # Masked self-attention\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)  # Cross-attention\n",
    "        \n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "        \n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dropout1 = layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = layers.Dropout(dropout_rate)\n",
    "        self.dropout3 = layers.Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, x, enc_output, training, \n",
    "             look_ahead_mask=None, padding_mask=None):\n",
    "        # Masked multi-head attention (self-attention)\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "        \n",
    "        # Multi-head attention (cross-attention to encoder output)\n",
    "        attn2, attn_weights_block2 = self.mha2(\n",
    "            enc_output, enc_output, out1, padding_mask\n",
    "        )\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)\n",
    "        \n",
    "        # Feed forward\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)\n",
    "        \n",
    "        return out3, attn_weights_block1, attn_weights_block2\n",
    "\n",
    "\n",
    "class Decoder(layers.Layer):\n",
    "    \"\"\"Transformer decoder (stack of decoder layers).\"\"\"\n",
    "    \n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, \n",
    "                 target_vocab_size, maximum_position_encoding, dropout_rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "        \n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, dropout_rate)\n",
    "                           for _ in range(num_layers)]\n",
    "        \n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, x, enc_output, training, \n",
    "             look_ahead_mask=None, padding_mask=None):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "        \n",
    "        # Embedding + positional encoding\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        \n",
    "        x = self.dropout(x, training=training)\n",
    "        \n",
    "        # Pass through decoder layers\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](\n",
    "                x, enc_output, training, look_ahead_mask, padding_mask\n",
    "            )\n",
    "            \n",
    "            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
    "            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
    "        \n",
    "        return x, attention_weights\n",
    "\n",
    "print(\"Decoder defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(keras.Model):\n",
    "    \"\"\"Complete transformer model.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_layers, d_model, num_heads, dff,\n",
    "                 input_vocab_size, target_vocab_size, \n",
    "                 pe_input, pe_target, dropout_rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                               input_vocab_size, pe_input, dropout_rate)\n",
    "        \n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "                               target_vocab_size, pe_target, dropout_rate)\n",
    "        \n",
    "        self.final_layer = layers.Dense(target_vocab_size)\n",
    "    \n",
    "    def call(self, inputs, training):\n",
    "        inp, tar = inputs\n",
    "        \n",
    "        enc_padding_mask = None\n",
    "        look_ahead_mask = self.create_look_ahead_mask(tf.shape(tar)[1])\n",
    "        dec_padding_mask = None\n",
    "        \n",
    "        # Encoder\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
    "        \n",
    "        # Decoder\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "            tar, enc_output, training, look_ahead_mask, dec_padding_mask\n",
    "        )\n",
    "        \n",
    "        # Final linear layer\n",
    "        final_output = self.final_layer(dec_output)\n",
    "        \n",
    "        return final_output, attention_weights\n",
    "    \n",
    "    def create_look_ahead_mask(self, size):\n",
    "        \"\"\"Create mask to prevent attending to future tokens.\"\"\"\n",
    "        mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "        return mask\n",
    "\n",
    "print(\"Complete Transformer defined!\")\n",
    "print(\"\\nArchitecture Summary:\")\n",
    "print(\"- Encoder: Multi-head self-attention + Feed-forward\")\n",
    "print(\"- Decoder: Masked self-attention + Cross-attention + Feed-forward\")\n",
    "print(\"- Residual connections and layer normalization throughout\")\n",
    "print(\"- Positional encoding for sequence order\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Part 2: BERT - Bidirectional Encoder Representations\n",
    "\n",
    "**BERT** (2018) uses only the encoder part of the transformer and is pre-trained with two objectives:\n",
    "\n",
    "### Pre-training Tasks:\n",
    "1. **Masked Language Modeling (MLM)**: Predict masked tokens\n",
    "   - Input: \"The [MASK] is on the table\"\n",
    "   - Target: Predict \"cat\"\n",
    "\n",
    "2. **Next Sentence Prediction (NSP)**: Predict if sentence B follows A\n",
    "   - Input: \"The cat sat. [SEP] It was happy.\"\n",
    "   - Target: True/False\n",
    "\n",
    "### Key Features:\n",
    "- **Bidirectional**: Sees context from both directions\n",
    "- **Pre-trained**: On massive text corpora (Wikipedia, BookCorpus)\n",
    "- **Fine-tunable**: For downstream tasks (classification, NER, Q&A)\n",
    "\n",
    "### Special Tokens:\n",
    "- `[CLS]`: Classification token (first token)\n",
    "- `[SEP]`: Separator between sentences\n",
    "- `[MASK]`: Masked token for MLM\n",
    "- `[PAD]`: Padding token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate masked language modeling\n",
    "class SimpleBERT:\n",
    "    \"\"\"Simplified BERT-style model for demonstration.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model=128, num_heads=4, num_layers=2):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Build encoder-only model\n",
    "        self.encoder = Encoder(\n",
    "            num_layers=num_layers,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dff=d_model * 4,\n",
    "            input_vocab_size=vocab_size,\n",
    "            maximum_position_encoding=512,\n",
    "            dropout_rate=0.1\n",
    "        )\n",
    "        \n",
    "        # MLM head\n",
    "        self.mlm_head = keras.Sequential([\n",
    "            layers.Dense(d_model, activation='gelu'),\n",
    "            layers.LayerNormalization(epsilon=1e-6),\n",
    "            layers.Dense(vocab_size)\n",
    "        ])\n",
    "    \n",
    "    def create_model(self):\n",
    "        \"\"\"Create Keras model for BERT.\"\"\"\n",
    "        inputs = layers.Input(shape=(None,), dtype=tf.int32)\n",
    "        \n",
    "        # Encoder\n",
    "        encoder_output = self.encoder(inputs, training=True)\n",
    "        \n",
    "        # MLM predictions\n",
    "        mlm_output = self.mlm_head(encoder_output)\n",
    "        \n",
    "        model = keras.Model(inputs=inputs, outputs=mlm_output)\n",
    "        return model\n",
    "\n",
    "# Create toy BERT model\n",
    "bert = SimpleBERT(vocab_size=1000, d_model=128, num_heads=4, num_layers=2)\n",
    "bert_model = bert.create_model()\n",
    "\n",
    "print(\"BERT Model Architecture:\")\n",
    "print(bert_model.summary())\n",
    "\n",
    "print(\"\\nBERT Use Cases:\")\n",
    "print(\"- Text Classification (sentiment, topic)\")\n",
    "print(\"- Named Entity Recognition (NER)\")\n",
    "print(\"- Question Answering\")\n",
    "print(\"- Sentence Similarity\")\n",
    "print(\"- Text Embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Part 3: GPT - Generative Pre-trained Transformer\n",
    "\n",
    "**GPT** uses only the decoder part with **causal** (autoregressive) attention.\n",
    "\n",
    "### Pre-training:\n",
    "- **Causal Language Modeling**: Predict next token\n",
    "- Input: \"The cat sat on\"\n",
    "- Target: Predict \"the\"\n",
    "\n",
    "### Key Differences from BERT:\n",
    "- **Unidirectional**: Only sees previous tokens (left-to-right)\n",
    "- **Generative**: Can generate coherent text\n",
    "- **Autoregressive**: Generates one token at a time\n",
    "\n",
    "### Evolution:\n",
    "- **GPT** (2018): 117M parameters\n",
    "- **GPT-2** (2019): 1.5B parameters\n",
    "- **GPT-3** (2020): 175B parameters, few-shot learning\n",
    "- **GPT-4** (2023): Multimodal, improved reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGPT:\n",
    "    \"\"\"Simplified GPT-style model for demonstration.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model=128, num_heads=4, num_layers=2):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "    \n",
    "    def create_model(self, max_length=128):\n",
    "        \"\"\"Create Keras model for GPT.\"\"\"\n",
    "        inputs = layers.Input(shape=(None,), dtype=tf.int32)\n",
    "        \n",
    "        # Embedding\n",
    "        x = layers.Embedding(self.vocab_size, self.d_model)(inputs)\n",
    "        \n",
    "        # Positional encoding\n",
    "        pos_enc = positional_encoding(max_length, self.d_model)\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        x += pos_enc[:, :seq_len, :]\n",
    "        \n",
    "        # Decoder layers (with causal masking)\n",
    "        for _ in range(self.num_layers):\n",
    "            # Multi-head causal self-attention\n",
    "            attn_output = layers.MultiHeadAttention(\n",
    "                num_heads=self.num_heads, \n",
    "                key_dim=self.d_model // self.num_heads,\n",
    "                dropout=0.1\n",
    "            )(x, x, use_causal_mask=True)\n",
    "            \n",
    "            x = layers.LayerNormalization(epsilon=1e-6)(x + attn_output)\n",
    "            \n",
    "            # Feed-forward\n",
    "            ffn_output = layers.Dense(self.d_model * 4, activation='gelu')(x)\n",
    "            ffn_output = layers.Dense(self.d_model)(ffn_output)\n",
    "            x = layers.LayerNormalization(epsilon=1e-6)(x + ffn_output)\n",
    "        \n",
    "        # Output layer\n",
    "        outputs = layers.Dense(self.vocab_size)(x)\n",
    "        \n",
    "        model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        return model\n",
    "\n",
    "# Create toy GPT model\n",
    "gpt = SimpleGPT(vocab_size=1000, d_model=128, num_heads=4, num_layers=2)\n",
    "gpt_model = gpt.create_model()\n",
    "\n",
    "print(\"GPT Model Architecture:\")\n",
    "print(gpt_model.summary())\n",
    "\n",
    "print(\"\\nGPT Use Cases:\")\n",
    "print(\"- Text Generation (stories, articles, code)\")\n",
    "print(\"- Text Completion\")\n",
    "print(\"- Conversational AI (ChatGPT)\")\n",
    "print(\"- Few-shot Learning (with prompting)\")\n",
    "print(\"- Code Generation (GitHub Copilot)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Part 4: Using Hugging Face Transformers\n",
    "\n",
    "**Hugging Face** provides easy access to thousands of pre-trained models.\n",
    "\n",
    "### Installation:\n",
    "```bash\n",
    "pip install transformers tokenizers datasets\n",
    "```\n",
    "\n",
    "### Key Components:\n",
    "- **Models**: Pre-trained transformer models\n",
    "- **Tokenizers**: Convert text to tokens\n",
    "- **Pipelines**: High-level API for common tasks\n",
    "- **Datasets**: Curated datasets for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install if needed (uncomment):\n",
    "# !pip install transformers tokenizers datasets -q\n",
    "\n",
    "try:\n",
    "    from transformers import pipeline, AutoTokenizer, AutoModel\n",
    "    \n",
    "    print(\"Transformers library installed!\")\n",
    "    print(\"\\nAvailable Pipeline Tasks:\")\n",
    "    print(\"- text-classification: Sentiment, topic classification\")\n",
    "    print(\"- ner: Named Entity Recognition\")\n",
    "    print(\"- question-answering: Answer questions from context\")\n",
    "    print(\"- summarization: Text summarization\")\n",
    "    print(\"- translation: Machine translation\")\n",
    "    print(\"- text-generation: Generate text\")\n",
    "    print(\"- fill-mask: Fill masked tokens (BERT-style)\")\n",
    "    print(\"- zero-shot-classification: Classify without training\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"Please install transformers: pip install transformers tokenizers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Sentiment Analysis\n",
    "try:\n",
    "    print(\"Loading sentiment analysis pipeline...\")\n",
    "    sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
    "    \n",
    "    texts = [\n",
    "        \"I love this movie! It's absolutely fantastic.\",\n",
    "        \"This is the worst product I've ever bought.\",\n",
    "        \"The weather is okay today, nothing special.\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nSentiment Analysis Results:\\n\")\n",
    "    for text in texts:\n",
    "        result = sentiment_analyzer(text)[0]\n",
    "        print(f\"Text: {text}\")\n",
    "        print(f\"Sentiment: {result['label']}, Score: {result['score']:.3f}\\n\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Note: First run may download models (can be slow)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Named Entity Recognition (NER)\n",
    "try:\n",
    "    print(\"Loading NER pipeline...\")\n",
    "    ner = pipeline(\"ner\", grouped_entities=True)\n",
    "    \n",
    "    text = \"\"\"Apple Inc. was founded by Steve Jobs in Cupertino, California. \n",
    "    The company is now led by Tim Cook.\"\"\"\n",
    "    \n",
    "    entities = ner(text)\n",
    "    \n",
    "    print(\"\\nNamed Entity Recognition:\\n\")\n",
    "    print(f\"Text: {text}\\n\")\n",
    "    print(\"Entities found:\")\n",
    "    for entity in entities:\n",
    "        print(f\"  {entity['word']:20s} -> {entity['entity_group']:10s} (score: {entity['score']:.3f})\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Question Answering\n",
    "try:\n",
    "    print(\"Loading Q&A pipeline...\")\n",
    "    qa = pipeline(\"question-answering\")\n",
    "    \n",
    "    context = \"\"\"\n",
    "    The transformer architecture was introduced in the paper 'Attention is All You Need' \n",
    "    in 2017. It uses self-attention mechanisms to process sequences in parallel, \n",
    "    unlike RNNs which process sequentially. This makes transformers much faster to train \n",
    "    and better at capturing long-range dependencies.\n",
    "    \"\"\"\n",
    "    \n",
    "    questions = [\n",
    "        \"When was the transformer architecture introduced?\",\n",
    "        \"What mechanism do transformers use?\",\n",
    "        \"Why are transformers faster than RNNs?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nQuestion Answering:\\n\")\n",
    "    for question in questions:\n",
    "        result = qa(question=question, context=context)\n",
    "        print(f\"Q: {question}\")\n",
    "        print(f\"A: {result['answer']} (score: {result['score']:.3f})\\n\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Text Generation\n",
    "try:\n",
    "    print(\"Loading text generation pipeline...\")\n",
    "    generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "    \n",
    "    prompts = [\n",
    "        \"Artificial intelligence is\",\n",
    "        \"The future of machine learning\",\n",
    "        \"In a world where robots\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nText Generation (GPT-2):\\n\")\n",
    "    for prompt in prompts:\n",
    "        result = generator(prompt, max_length=50, num_return_sequences=1)[0]\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(f\"Generated: {result['generated_text']}\\n\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 5: Fill Mask (BERT-style)\n",
    "try:\n",
    "    print(\"Loading fill-mask pipeline...\")\n",
    "    unmasker = pipeline(\"fill-mask\")\n",
    "    \n",
    "    sentences = [\n",
    "        \"The [MASK] is shining brightly today.\",\n",
    "        \"Python is a popular [MASK] language.\",\n",
    "        \"Transformers use [MASK] mechanisms.\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nFill Mask (BERT):\\n\")\n",
    "    for sentence in sentences:\n",
    "        results = unmasker(sentence, top_k=3)\n",
    "        print(f\"Sentence: {sentence}\")\n",
    "        print(\"Predictions:\")\n",
    "        for i, result in enumerate(results, 1):\n",
    "            print(f\"  {i}. {result['token_str']:15s} (score: {result['score']:.3f})\")\n",
    "        print()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## Part 5: Fine-tuning BERT for Text Classification\n",
    "\n",
    "Fine-tuning adapts a pre-trained model to a specific task with your data.\n",
    "\n",
    "### Steps:\n",
    "1. Load pre-trained model\n",
    "2. Add task-specific head (e.g., classification layer)\n",
    "3. Train on your dataset with small learning rate\n",
    "4. Evaluate on validation set\n",
    "\n",
    "### Tips:\n",
    "- Use small learning rate (1e-5 to 5e-5)\n",
    "- Few epochs (2-4 usually sufficient)\n",
    "- Monitor for overfitting\n",
    "- Use learning rate warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate fine-tuning BERT for sentiment classification\n",
    "try:\n",
    "    from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "    \n",
    "    # Sample dataset\n",
    "    train_texts = [\n",
    "        \"I love this product!\",\n",
    "        \"This is terrible.\",\n",
    "        \"Amazing quality!\",\n",
    "        \"Worst purchase ever.\",\n",
    "        \"Highly recommended!\",\n",
    "        \"Complete waste of money.\"\n",
    "    ]\n",
    "    train_labels = [1, 0, 1, 0, 1, 0]  # 1 = positive, 0 = negative\n",
    "    \n",
    "    # Load tokenizer and model\n",
    "    print(\"Loading BERT model...\")\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = TFBertForSequenceClassification.from_pretrained(\n",
    "        'bert-base-uncased',\n",
    "        num_labels=2\n",
    "    )\n",
    "    \n",
    "    # Tokenize\n",
    "    encodings = tokenizer(\n",
    "        train_texts,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=128,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "    \n",
    "    # Compile\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=2e-5),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(\"\\nFine-tuning BERT...\")\n",
    "    # Note: This is just demonstration; real fine-tuning needs more data\n",
    "    history = model.fit(\n",
    "        dict(encodings),\n",
    "        np.array(train_labels),\n",
    "        epochs=3,\n",
    "        batch_size=2,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nFinal training accuracy: {history.history['accuracy'][-1]:.3f}\")\n",
    "    \n",
    "    # Test predictions\n",
    "    test_texts = [\"This is fantastic!\", \"I hate this.\"]\n",
    "    test_encodings = tokenizer(\n",
    "        test_texts,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "    \n",
    "    predictions = model.predict(dict(test_encodings), verbose=0)\n",
    "    predicted_classes = tf.argmax(predictions.logits, axis=1)\n",
    "    \n",
    "    print(\"\\nTest Predictions:\")\n",
    "    for text, pred in zip(test_texts, predicted_classes):\n",
    "        sentiment = \"Positive\" if pred == 1 else \"Negative\"\n",
    "        print(f\"'{text}' -> {sentiment}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"\\nNote: This example requires transformers with TensorFlow.\")\n",
    "    print(\"Install with: pip install transformers[tf]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## Part 6: Evaluation Metrics for Language Models\n",
    "\n",
    "Different tasks require different evaluation metrics.\n",
    "\n",
    "### Classification Metrics:\n",
    "- **Accuracy**: Fraction of correct predictions\n",
    "- **Precision**: TP / (TP + FP)\n",
    "- **Recall**: TP / (TP + FN)\n",
    "- **F1-Score**: Harmonic mean of precision and recall\n",
    "\n",
    "### Language Model Metrics:\n",
    "- **Perplexity**: exp(average cross-entropy loss)\n",
    "  - Lower is better\n",
    "  - Measures how \"surprised\" the model is\n",
    "\n",
    "### Translation Metrics:\n",
    "- **BLEU** (Bilingual Evaluation Understudy):\n",
    "  - Compares n-gram overlap with reference translations\n",
    "  - Score: 0-1 (higher is better)\n",
    "  - BLEU-4 most common (4-grams)\n",
    "\n",
    "### Summarization Metrics:\n",
    "- **ROUGE** (Recall-Oriented Understudy for Gisting Evaluation):\n",
    "  - ROUGE-N: N-gram overlap\n",
    "  - ROUGE-L: Longest common subsequence\n",
    "  - Higher is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate perplexity\n",
    "def calculate_perplexity(loss):\n",
    "    \"\"\"Calculate perplexity from cross-entropy loss.\"\"\"\n",
    "    return np.exp(loss)\n",
    "\n",
    "# Example losses and perplexities\n",
    "losses = [0.5, 1.0, 2.0, 3.0, 4.0]\n",
    "perplexities = [calculate_perplexity(loss) for loss in losses]\n",
    "\n",
    "print(\"Perplexity Examples:\\n\")\n",
    "print(\"Cross-Entropy Loss | Perplexity\")\n",
    "print(\"-\" * 35)\n",
    "for loss, ppl in zip(losses, perplexities):\n",
    "    print(f\"{loss:18.1f} | {ppl:10.2f}\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Lower perplexity = better model\")\n",
    "print(\"- Perplexity ~1.2: Very good model\")\n",
    "print(\"- Perplexity ~50: Moderate model\")\n",
    "print(\"- Perplexity >100: Poor model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple BLEU score implementation\n",
    "from collections import Counter\n",
    "\n",
    "def compute_bleu(reference, candidate, n=4):\n",
    "    \"\"\"\n",
    "    Compute BLEU score (simplified version).\n",
    "    \n",
    "    Args:\n",
    "        reference: Reference translation (string)\n",
    "        candidate: Candidate translation (string)\n",
    "        n: Maximum n-gram size (default: 4)\n",
    "    \"\"\"\n",
    "    ref_tokens = reference.lower().split()\n",
    "    cand_tokens = candidate.lower().split()\n",
    "    \n",
    "    scores = []\n",
    "    \n",
    "    for i in range(1, n + 1):\n",
    "        # Create n-grams\n",
    "        ref_ngrams = [tuple(ref_tokens[j:j+i]) for j in range(len(ref_tokens) - i + 1)]\n",
    "        cand_ngrams = [tuple(cand_tokens[j:j+i]) for j in range(len(cand_tokens) - i + 1)]\n",
    "        \n",
    "        if len(cand_ngrams) == 0:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "        \n",
    "        # Count matches\n",
    "        ref_counts = Counter(ref_ngrams)\n",
    "        cand_counts = Counter(cand_ngrams)\n",
    "        \n",
    "        matches = sum((cand_counts & ref_counts).values())\n",
    "        total = sum(cand_counts.values())\n",
    "        \n",
    "        scores.append(matches / total if total > 0 else 0)\n",
    "    \n",
    "    # Geometric mean of n-gram precisions\n",
    "    if min(scores) > 0:\n",
    "        bleu = np.exp(np.mean([np.log(s) for s in scores]))\n",
    "    else:\n",
    "        bleu = 0\n",
    "    \n",
    "    # Brevity penalty\n",
    "    bp = 1.0 if len(cand_tokens) >= len(ref_tokens) else np.exp(1 - len(ref_tokens) / len(cand_tokens))\n",
    "    \n",
    "    return bp * bleu\n",
    "\n",
    "# Test BLEU\n",
    "reference = \"The cat is sitting on the mat\"\n",
    "candidates = [\n",
    "    \"The cat is sitting on the mat\",  # Perfect match\n",
    "    \"The cat sits on the mat\",        # Close\n",
    "    \"A cat is on a mat\",               # Moderate\n",
    "    \"Dog runs in park\",                 # Poor\n",
    "]\n",
    "\n",
    "print(\"BLEU Score Examples:\\n\")\n",
    "print(f\"Reference: {reference}\\n\")\n",
    "for candidate in candidates:\n",
    "    bleu = compute_bleu(reference, candidate)\n",
    "    print(f\"Candidate: {candidate}\")\n",
    "    print(f\"BLEU: {bleu:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## Part 7: Practical Considerations\n",
    "\n",
    "### Model Selection:\n",
    "\n",
    "**Use BERT when:**\n",
    "- Classification tasks (sentiment, topic)\n",
    "- Understanding tasks (Q&A, NER)\n",
    "- You need bidirectional context\n",
    "- You have labeled data for fine-tuning\n",
    "\n",
    "**Use GPT when:**\n",
    "- Text generation tasks\n",
    "- Conversational AI\n",
    "- Few-shot learning with prompts\n",
    "- Creative writing\n",
    "\n",
    "**Use T5 when:**\n",
    "- You want a unified text-to-text approach\n",
    "- Translation, summarization\n",
    "- You need flexibility across tasks\n",
    "\n",
    "### Computational Considerations:\n",
    "- **Small models** (BERT-base, DistilBERT): Laptop-friendly\n",
    "- **Large models** (BERT-large, GPT-3): Need GPUs\n",
    "- **Inference optimization**: Quantization, distillation, ONNX\n",
    "\n",
    "### Best Practices:\n",
    "1. Start with pre-trained models\n",
    "2. Use appropriate tokenizer for your model\n",
    "3. Fine-tune with small learning rates\n",
    "4. Monitor validation metrics\n",
    "5. Use data augmentation if data is limited\n",
    "6. Consider domain adaptation\n",
    "7. Evaluate on diverse test sets\n",
    "8. Check for biases in predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model size comparison\n",
    "import pandas as pd\n",
    "\n",
    "models_data = {\n",
    "    'Model': ['DistilBERT', 'BERT-base', 'BERT-large', 'GPT-2', 'GPT-3', 'T5-small', 'T5-base'],\n",
    "    'Parameters': ['66M', '110M', '340M', '117M', '175B', '60M', '220M'],\n",
    "    'Layers': [6, 12, 24, 12, 96, 6, 12],\n",
    "    'Hidden Size': [768, 768, 1024, 768, 12288, 512, 768],\n",
    "    'Use Case': [\n",
    "        'Fast classification',\n",
    "        'General NLP',\n",
    "        'High accuracy needed',\n",
    "        'Text generation',\n",
    "        'Few-shot learning',\n",
    "        'Small tasks',\n",
    "        'General seq2seq'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(models_data)\n",
    "print(\"Popular Transformer Models:\\n\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Model Selection Guide:\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Laptop/CPU: DistilBERT, BERT-base, T5-small\")\n",
    "print(\"Single GPU: BERT-large, GPT-2, T5-base\")\n",
    "print(\"Multiple GPUs: GPT-3, Large T5 variants\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Transformers** use self-attention exclusively (no recurrence)\n",
    "2. **Multi-head attention** allows parallel processing of different aspects\n",
    "3. **Positional encoding** injects sequence order information\n",
    "4. **BERT** is bidirectional, great for understanding tasks\n",
    "5. **GPT** is autoregressive, excellent for generation\n",
    "6. **Pre-training + Fine-tuning** is the dominant paradigm\n",
    "7. **Hugging Face** provides easy access to state-of-the-art models\n",
    "8. **Transfer learning** enables training with less data\n",
    "9. **Perplexity, BLEU, ROUGE** are key evaluation metrics\n",
    "10. **Model selection** depends on task, data, and resources\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. **Complete Transformer**: Train full transformer on translation\n",
    "2. **BERT Fine-tuning**: Fine-tune BERT on custom classification task\n",
    "3. **GPT Text Generation**: Generate creative text with GPT-2\n",
    "4. **Multi-task Learning**: Train T5 on multiple tasks\n",
    "5. **Model Comparison**: Compare BERT vs. GPT on same task\n",
    "6. **Attention Visualization**: Visualize attention patterns\n",
    "7. **Custom Tokenizer**: Build domain-specific tokenizer\n",
    "8. **Model Distillation**: Compress BERT to smaller model\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Congratulations! You've completed the Introduction to AI course.\n",
    "\n",
    "### Further Learning:\n",
    "- **Advanced Transformers**: Vision Transformers, multimodal models\n",
    "- **Efficient Models**: Reformer, Linformer, Performer\n",
    "- **Prompt Engineering**: Optimizing prompts for LLMs\n",
    "- **RLHF**: Reinforcement Learning from Human Feedback\n",
    "- **RAG**: Retrieval-Augmented Generation\n",
    "- **LangChain**: Building LLM applications\n",
    "\n",
    "### Resources:\n",
    "- **Hugging Face Course**: https://huggingface.co/course\n",
    "- **Transformers Paper**: \"Attention is All You Need\"\n",
    "- **BERT Paper**: \"BERT: Pre-training of Deep Bidirectional Transformers\"\n",
    "- **GPT Paper**: \"Language Models are Few-Shot Learners\"\n",
    "\n",
    "Great work completing this course! ðŸŽ‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
