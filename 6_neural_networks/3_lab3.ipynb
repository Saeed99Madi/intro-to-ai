{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Convolutional Neural Networks (CNNs)\n",
    "\n",
    "Convolutional Neural Networks are the backbone of modern computer vision. In this lab, you'll learn how CNNs process images, build classic architectures, and apply transfer learning.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will:\n",
    "- Understand convolution operations and feature maps\n",
    "- Build CNNs from scratch\n",
    "- Implement famous architectures (LeNet, VGG, ResNet)\n",
    "- Apply transfer learning with pre-trained models\n",
    "- Use data augmentation for better generalization\n",
    "- Visualize CNN features and activations\n",
    "- Build custom image classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import VGG16, ResNet50, MobileNetV2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Convolutions\n",
    "\n",
    "A **convolution** slides a filter (kernel) over an image to produce a feature map.\n",
    "\n",
    "### Operation:\n",
    "For a 2D convolution with filter $W$ and input $X$:\n",
    "$$Y[i,j] = \\sum_m \\sum_n X[i+m, j+n] \\cdot W[m,n] + b$$\n",
    "\n",
    "### Key Parameters:\n",
    "- **Kernel size**: Size of the filter (e.g., 3×3, 5×5)\n",
    "- **Stride**: Step size when sliding the filter\n",
    "- **Padding**: Adding borders to preserve spatial dimensions\n",
    "- **Channels**: Number of filters (output feature maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize convolution operation\n",
    "from scipy.signal import convolve2d\n",
    "\n",
    "# Load sample image\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "sample_image = X_train[0].astype(float) / 255.0\n",
    "\n",
    "# Define filters\n",
    "filters = {\n",
    "    'Vertical Edge': np.array([[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]]),\n",
    "    'Horizontal Edge': np.array([[-1, -1, -1], [0, 0, 0], [1, 1, 1]]),\n",
    "    'Blur': np.ones((3, 3)) / 9,\n",
    "    'Sharpen': np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
    "}\n",
    "\n",
    "# Apply filters\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "axes[0, 0].imshow(sample_image, cmap='gray')\n",
    "axes[0, 0].set_title('Original Image')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "for idx, (name, kernel) in enumerate(filters.items(), 1):\n",
    "    filtered = convolve2d(sample_image, kernel, mode='same', boundary='symm')\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    ax.imshow(filtered, cmap='gray')\n",
    "    ax.set_title(f'{name} Detection')\n",
    "    ax.axis('off')\n",
    "\n",
    "axes[1, 2].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Convolution filters detect different features:\")\n",
    "print(\"- Edge detectors find boundaries\")\n",
    "print(\"- Blur smooths the image\")\n",
    "print(\"- Sharpen enhances details\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: CNN Architecture Components\n",
    "\n",
    "### Typical CNN Architecture:\n",
    "1. **Convolutional layers**: Extract features\n",
    "2. **Activation (ReLU)**: Non-linearity\n",
    "3. **Pooling layers**: Downsample, reduce parameters\n",
    "4. **Fully connected layers**: Classification\n",
    "\n",
    "### Pooling Operations:\n",
    "- **Max Pooling**: Takes maximum value in region\n",
    "- **Average Pooling**: Takes average value\n",
    "- **Global Average Pooling**: One value per feature map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple CNN for MNIST\n",
    "model_simple_cnn = keras.Sequential([\n",
    "    # First conv block\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Second conv block\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Third conv block\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    \n",
    "    # Dense layers\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model_simple_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare MNIST data for CNN\n",
    "X_train_cnn = X_train.reshape(-1, 28, 28, 1) / 255.0\n",
    "X_test_cnn = X_test.reshape(-1, 28, 28, 1) / 255.0\n",
    "\n",
    "# Compile and train\n",
    "model_simple_cnn.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history_cnn = model_simple_cnn.fit(\n",
    "    X_train_cnn, y_train,\n",
    "    batch_size=128,\n",
    "    epochs=10,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "test_loss, test_acc = model_simple_cnn.evaluate(X_test_cnn, y_test, verbose=0)\n",
    "print(f\"\\nTest Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Famous CNN Architectures\n",
    "\n",
    "### LeNet-5 (1998)\n",
    "- First successful CNN\n",
    "- Handwritten digit recognition\n",
    "- 7 layers\n",
    "\n",
    "### AlexNet (2012)\n",
    "- Won ImageNet competition\n",
    "- Deeper network with ReLU\n",
    "- Dropout for regularization\n",
    "\n",
    "### VGG (2014)\n",
    "- Very deep (16-19 layers)\n",
    "- Small 3×3 filters throughout\n",
    "- Simple, uniform architecture\n",
    "\n",
    "### ResNet (2015)\n",
    "- Residual connections\n",
    "- Enables training of 100+ layer networks\n",
    "- Skip connections solve vanishing gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LeNet-5 implementation\n",
    "def create_lenet():\n",
    "    model = keras.Sequential([\n",
    "        layers.Conv2D(6, kernel_size=(5, 5), activation='tanh', input_shape=(28, 28, 1)),\n",
    "        layers.AveragePooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(16, kernel_size=(5, 5), activation='tanh'),\n",
    "        layers.AveragePooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(120, activation='tanh'),\n",
    "        layers.Dense(84, activation='tanh'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# VGG-like (simplified)\n",
    "def create_vgg_style():\n",
    "    model = keras.Sequential([\n",
    "        # Block 1\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(28, 28, 1)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        # Block 2\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        # Dense layers\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# ResNet-style block\n",
    "def resnet_block(x, filters, kernel_size=3, stride=1):\n",
    "    # Main path\n",
    "    fx = layers.Conv2D(filters, kernel_size, strides=stride, padding='same')(x)\n",
    "    fx = layers.BatchNormalization()(fx)\n",
    "    fx = layers.Activation('relu')(fx)\n",
    "    fx = layers.Conv2D(filters, kernel_size, padding='same')(fx)\n",
    "    fx = layers.BatchNormalization()(fx)\n",
    "    \n",
    "    # Shortcut\n",
    "    if stride != 1 or x.shape[-1] != filters:\n",
    "        x = layers.Conv2D(filters, 1, strides=stride, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    # Add and activate\n",
    "    out = layers.Add()([fx, x])\n",
    "    out = layers.Activation('relu')(out)\n",
    "    return out\n",
    "\n",
    "def create_resnet_style():\n",
    "    inputs = keras.Input(shape=(28, 28, 1))\n",
    "    \n",
    "    x = layers.Conv2D(64, 3, padding='same')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    \n",
    "    x = resnet_block(x, 64)\n",
    "    x = resnet_block(x, 128, stride=2)\n",
    "    x = resnet_block(x, 256, stride=2)\n",
    "    \n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    outputs = layers.Dense(10, activation='softmax')(x)\n",
    "    \n",
    "    return keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "print(\"Created architectures: LeNet, VGG-style, and ResNet-style\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare architectures\n",
    "architectures = {\n",
    "    'LeNet': create_lenet(),\n",
    "    'VGG-style': create_vgg_style(),\n",
    "    'ResNet-style': create_resnet_style()\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in architectures.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train_cnn[:10000], y_train[:10000],  # Subset for speed\n",
    "        batch_size=128,\n",
    "        epochs=5,\n",
    "        validation_split=0.2,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    results[name] = history\n",
    "    \n",
    "    # Count parameters\n",
    "    params = model.count_params()\n",
    "    print(f\"{name}: {params:,} parameters, Final val acc: {history.history['val_accuracy'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Data Augmentation\n",
    "\n",
    "**Data augmentation** artificially increases training data by applying transformations:\n",
    "- Rotation\n",
    "- Translation\n",
    "- Scaling\n",
    "- Flipping\n",
    "- Color jittering\n",
    "- Cropping\n",
    "\n",
    "### Benefits:\n",
    "- Better generalization\n",
    "- Reduces overfitting\n",
    "- Acts as regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data augmentation pipeline\n",
    "data_augmentation = keras.Sequential([\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomTranslation(0.1, 0.1),\n",
    "    layers.RandomZoom(0.1),\n",
    "])\n",
    "\n",
    "# Visualize augmentations\n",
    "sample = X_train_cnn[0:1]\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    if i == 0:\n",
    "        augmented = sample\n",
    "        title = 'Original'\n",
    "    else:\n",
    "        augmented = data_augmentation(sample, training=True)\n",
    "        title = f'Augmented {i}'\n",
    "    \n",
    "    ax.imshow(augmented[0, :, :, 0], cmap='gray')\n",
    "    ax.set_title(title)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with augmentation\n",
    "model_with_aug = keras.Sequential([\n",
    "    data_augmentation,\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model_with_aug.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history_aug = model_with_aug.fit(\n",
    "    X_train_cnn, y_train,\n",
    "    batch_size=128,\n",
    "    epochs=10,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Data augmentation helps prevent overfitting!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Transfer Learning\n",
    "\n",
    "**Transfer learning** uses pre-trained models as starting points:\n",
    "\n",
    "### Strategies:\n",
    "1. **Feature extraction**: Freeze pre-trained layers, train only new layers\n",
    "2. **Fine-tuning**: Unfreeze some layers and train with small learning rate\n",
    "\n",
    "### Benefits:\n",
    "- Faster training\n",
    "- Better performance with less data\n",
    "- Leverage knowledge from ImageNet (1.4M images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 for transfer learning demo\n",
    "(X_train_cifar, y_train_cifar), (X_test_cifar, y_test_cifar) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize\n",
    "X_train_cifar = X_train_cifar / 255.0\n",
    "X_test_cifar = X_test_cifar / 255.0\n",
    "y_train_cifar = y_train_cifar.squeeze()\n",
    "y_test_cifar = y_test_cifar.squeeze()\n",
    "\n",
    "cifar_classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "                'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# Visualize samples\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    ax.imshow(X_train_cifar[i])\n",
    "    ax.set_title(cifar_classes[y_train_cifar[i]])\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"CIFAR-10: {X_train_cifar.shape[0]} training images, {X_test_cifar.shape[0]} test images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer learning with MobileNetV2\n",
    "# Load pre-trained model (without top classification layer)\n",
    "base_model = MobileNetV2(\n",
    "    input_shape=(32, 32, 3),\n",
    "    include_top=False,\n",
    "    weights='imagenet'\n",
    ")\n",
    "\n",
    "# Freeze base model\n",
    "base_model.trainable = False\n",
    "\n",
    "# Add custom classification head\n",
    "inputs = keras.Input(shape=(32, 32, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "outputs = layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "model_transfer = keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile\n",
    "model_transfer.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(f\"Total parameters: {model_transfer.count_params():,}\")\n",
    "print(f\"Trainable parameters: {sum([tf.size(v).numpy() for v in model_transfer.trainable_variables]):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with transfer learning (feature extraction)\n",
    "history_transfer = model_transfer.fit(\n",
    "    X_train_cifar[:5000], y_train_cifar[:5000],  # Subset for demo\n",
    "    batch_size=64,\n",
    "    epochs=5,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fine-tuning: Unfreeze some layers\n",
    "base_model.trainable = True\n",
    "# Freeze early layers, train later ones\n",
    "for layer in base_model.layers[:-20]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Recompile with lower learning rate\n",
    "model_transfer.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0001),  # Lower LR\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(f\"\\nFine-tuning: Trainable parameters: {sum([tf.size(v).numpy() for v in model_transfer.trainable_variables]):,}\")\n",
    "\n",
    "# Continue training (fine-tuning)\n",
    "history_finetune = model_transfer.fit(\n",
    "    X_train_cifar[:5000], y_train_cifar[:5000],\n",
    "    batch_size=64,\n",
    "    epochs=5,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTransfer learning stages:\")\n",
    "print(\"1. Feature extraction: Train only new layers\")\n",
    "print(\"2. Fine-tuning: Unfreeze and train some base layers with low LR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Visualizing CNN Features\n",
    "\n",
    "Understanding what CNNs learn helps interpret and debug models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize intermediate activations\n",
    "layer_outputs = [layer.output for layer in model_simple_cnn.layers[:6]]  # First 6 layers\n",
    "activation_model = keras.Model(inputs=model_simple_cnn.input, outputs=layer_outputs)\n",
    "\n",
    "# Get activations for a test image\n",
    "test_image = X_test_cnn[0:1]\n",
    "activations = activation_model.predict(test_image, verbose=0)\n",
    "\n",
    "# Visualize first conv layer activations\n",
    "first_layer_activation = activations[0]\n",
    "n_features = min(16, first_layer_activation.shape[-1])\n",
    "\n",
    "fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    if i < n_features:\n",
    "        ax.imshow(first_layer_activation[0, :, :, i], cmap='viridis')\n",
    "        ax.set_title(f'Filter {i}')\n",
    "    ax.axis('off')\n",
    "plt.suptitle('First Convolutional Layer Activations')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Each filter learns to detect different features!\")\n",
    "print(\"Early layers: edges, textures\")\n",
    "print(\"Later layers: complex patterns, objects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: PyTorch CNN Implementation\n",
    "\n",
    "Let's implement a CNN in PyTorch for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch CNN\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 3 * 3, 64)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(-1, 64 * 3 * 3)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_pytorch = SimpleCNN().to(device)\n",
    "\n",
    "print(model_pytorch)\n",
    "print(f\"\\nDevice: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Convolutions** detect spatial patterns through learned filters\n",
    "2. **Pooling** reduces spatial dimensions and parameters\n",
    "3. **CNNs** learn hierarchical features (edges → textures → objects)\n",
    "4. **Famous architectures** provide proven building blocks\n",
    "5. **Data augmentation** improves generalization\n",
    "6. **Transfer learning** leverages pre-trained models\n",
    "7. **Feature extraction** trains only new layers\n",
    "8. **Fine-tuning** adapts pre-trained features\n",
    "9. **Visualization** helps understand learned features\n",
    "10. **Both Keras and PyTorch** are excellent for CNNs\n",
    "\n",
    "## CNN Design Best Practices\n",
    "\n",
    "1. **Start with transfer learning** if possible\n",
    "2. **Use 3×3 convolutions** (VGG insight)\n",
    "3. **Add batch normalization** after conv layers\n",
    "4. **Use ReLU** activation\n",
    "5. **Apply data augmentation** for limited data\n",
    "6. **Global average pooling** instead of flatten when possible\n",
    "7. **Gradually increase channels** as you go deeper\n",
    "8. **Add dropout** before final layers\n",
    "9. **Use residual connections** for deep networks\n",
    "10. **Monitor validation metrics** to prevent overfitting\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. **Custom Architecture**: Design your own CNN architecture for CIFAR-10\n",
    "2. **Visualization**: Visualize filters from different layers\n",
    "3. **Grad-CAM**: Implement class activation maps to see what the model looks at\n",
    "4. **Transfer Learning**: Try different pre-trained models (ResNet, EfficientNet)\n",
    "5. **Object Detection**: Explore YOLO or Faster R-CNN basics\n",
    "6. **Style Transfer**: Implement neural style transfer\n",
    "7. **Custom Dataset**: Build a classifier for your own images\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In Lab 4, we'll explore:\n",
    "- Recurrent Neural Networks (RNNs)\n",
    "- Long Short-Term Memory (LSTM)\n",
    "- Attention mechanisms\n",
    "- Sequence-to-sequence models\n",
    "\n",
    "Great work! You now understand how CNNs revolutionized computer vision."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
