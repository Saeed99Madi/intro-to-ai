{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Advanced Architectures\n",
    "\n",
    "In this final neural networks lab, we'll explore architectures for sequential data, attention mechanisms, and generative models.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will:\n",
    "- Understand and implement Recurrent Neural Networks (RNNs)\n",
    "- Build LSTM and GRU networks\n",
    "- Apply attention mechanisms\n",
    "- Understand transformer basics\n",
    "- Implement autoencoders\n",
    "- Work with sequence-to-sequence models\n",
    "- Build practical applications with sequential data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Recurrent Neural Networks (RNNs)\n",
    "\n",
    "RNNs process sequential data by maintaining hidden state.\n",
    "\n",
    "### Basic RNN:\n",
    "$$h_t = \\tanh(W_{hh} h_{t-1} + W_{xh} x_t + b_h)$$\n",
    "$$y_t = W_{hy} h_t + b_y$$\n",
    "\n",
    "### Problems:\n",
    "- **Vanishing gradients**: Hard to learn long-term dependencies\n",
    "- **Exploding gradients**: Unstable training\n",
    "\n",
    "### Solution: LSTM and GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple sequence prediction task\n",
    "def generate_sequence_data(n_samples=1000, seq_length=10):\n",
    "    \"\"\"\n",
    "    Generate sequences where target is sum of inputs.\n",
    "    \"\"\"\n",
    "    X = np.random.rand(n_samples, seq_length, 1)\n",
    "    y = np.sum(X, axis=1)\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = generate_sequence_data(1000)\n",
    "X_test, y_test = generate_sequence_data(200)\n",
    "\n",
    "print(f\"Input shape: {X_train.shape}\")\n",
    "print(f\"Output shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple RNN model\n",
    "model_rnn = keras.Sequential([\n",
    "    layers.SimpleRNN(32, activation='tanh', input_shape=(10, 1)),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "model_rnn.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "history_rnn = model_rnn.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=32,\n",
    "    epochs=20,\n",
    "    validation_split=0.2,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "test_loss = model_rnn.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Simple RNN Test MAE: {test_loss[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: LSTM (Long Short-Term Memory)\n",
    "\n",
    "LSTM uses gates to control information flow:\n",
    "\n",
    "### Gates:\n",
    "1. **Forget gate**: $f_t = \\sigma(W_f [h_{t-1}, x_t] + b_f)$\n",
    "2. **Input gate**: $i_t = \\sigma(W_i [h_{t-1}, x_t] + b_i)$\n",
    "3. **Output gate**: $o_t = \\sigma(W_o [h_{t-1}, x_t] + b_o)$\n",
    "\n",
    "### Cell state update:\n",
    "$$\\tilde{C}_t = \\tanh(W_C [h_{t-1}, x_t] + b_C)$$\n",
    "$$C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t$$\n",
    "$$h_t = o_t \\odot \\tanh(C_t)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM model\n",
    "model_lstm = keras.Sequential([\n",
    "    layers.LSTM(32, input_shape=(10, 1)),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "model_lstm.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "history_lstm = model_lstm.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=32,\n",
    "    epochs=20,\n",
    "    validation_split=0.2,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# GRU model (simplified LSTM)\n",
    "model_gru = keras.Sequential([\n",
    "    layers.GRU(32, input_shape=(10, 1)),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "model_gru.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "history_gru = model_gru.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=32,\n",
    "    epochs=20,\n",
    "    validation_split=0.2,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Compare\n",
    "print(f\"LSTM Test MAE: {model_lstm.evaluate(X_test, y_test, verbose=0)[1]:.4f}\")\n",
    "print(f\"GRU Test MAE: {model_gru.evaluate(X_test, y_test, verbose=0)[1]:.4f}\")\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history_rnn.history['val_loss'], label='RNN')\n",
    "plt.plot(history_lstm.history['val_loss'], label='LSTM')\n",
    "plt.plot(history_gru.history['val_loss'], label='GRU')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.title('RNN vs LSTM vs GRU')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Text Generation with LSTM\n",
    "\n",
    "Let's build a character-level text generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text\n",
    "text = \"\"\"Deep learning is a subset of machine learning that uses neural networks with multiple layers. \n",
    "These networks can learn hierarchical representations of data, making them powerful for tasks like \n",
    "image recognition, natural language processing, and more.\"\"\"\n",
    "\n",
    "# Create character mappings\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_idx = {c: i for i, c in enumerate(chars)}\n",
    "idx_to_char = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "print(f\"Unique characters: {len(chars)}\")\n",
    "print(f\"Text length: {len(text)}\")\n",
    "\n",
    "# Create sequences\n",
    "seq_length = 40\n",
    "step = 3\n",
    "sequences = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(text) - seq_length, step):\n",
    "    sequences.append(text[i:i + seq_length])\n",
    "    next_chars.append(text[i + seq_length])\n",
    "\n",
    "print(f\"Number of sequences: {len(sequences)}\")\n",
    "\n",
    "# Vectorize\n",
    "X = np.zeros((len(sequences), seq_length, len(chars)), dtype=bool)\n",
    "y = np.zeros((len(sequences), len(chars)), dtype=bool)\n",
    "\n",
    "for i, seq in enumerate(sequences):\n",
    "    for t, char in enumerate(seq):\n",
    "        X[i, t, char_to_idx[char]] = 1\n",
    "    y[i, char_to_idx[next_chars[i]]] = 1\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LSTM model for text generation\n",
    "model_text = keras.Sequential([\n",
    "    layers.LSTM(128, input_shape=(seq_length, len(chars))),\n",
    "    layers.Dense(len(chars), activation='softmax')\n",
    "])\n",
    "\n",
    "model_text.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy'\n",
    ")\n",
    "\n",
    "# Train\n",
    "model_text.fit(X, y, batch_size=32, epochs=50, verbose=0)\n",
    "\n",
    "# Generate text\n",
    "def generate_text(model, seed_text, length=200, temperature=1.0):\n",
    "    generated = seed_text\n",
    "    \n",
    "    for _ in range(length):\n",
    "        # Prepare input\n",
    "        x_pred = np.zeros((1, seq_length, len(chars)))\n",
    "        for t, char in enumerate(seed_text[-seq_length:]):\n",
    "            x_pred[0, t, char_to_idx[char]] = 1\n",
    "        \n",
    "        # Predict next character\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        preds = np.log(preds + 1e-7) / temperature\n",
    "        exp_preds = np.exp(preds)\n",
    "        preds = exp_preds / np.sum(exp_preds)\n",
    "        \n",
    "        next_idx = np.random.choice(len(chars), p=preds)\n",
    "        next_char = idx_to_char[next_idx]\n",
    "        \n",
    "        generated += next_char\n",
    "        seed_text += next_char\n",
    "    \n",
    "    return generated\n",
    "\n",
    "# Generate with different temperatures\n",
    "seed = \"Deep learning is a subset of machine l\"\n",
    "\n",
    "for temp in [0.5, 1.0, 1.5]:\n",
    "    print(f\"\\n--- Temperature: {temp} ---\")\n",
    "    print(generate_text(model_text, seed, length=100, temperature=temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Attention Mechanism\n",
    "\n",
    "Attention allows models to focus on relevant parts of the input.\n",
    "\n",
    "### Attention Formula:\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "Where:\n",
    "- $Q$: Query\n",
    "- $K$: Key\n",
    "- $V$: Value\n",
    "- $d_k$: Dimension of key vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple attention layer\n",
    "class SimpleAttention(layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(SimpleAttention, self).__init__()\n",
    "        self.W = layers.Dense(units)\n",
    "        self.V = layers.Dense(1)\n",
    "    \n",
    "    def call(self, hidden_states):\n",
    "        # hidden_states shape: (batch, time, features)\n",
    "        score = self.V(tf.nn.tanh(self.W(hidden_states)))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * hidden_states\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "# Model with attention\n",
    "inputs = keras.Input(shape=(10, 1))\n",
    "lstm_out = layers.LSTM(32, return_sequences=True)(inputs)\n",
    "context, attention_weights = SimpleAttention(32)(lstm_out)\n",
    "outputs = layers.Dense(1)(context)\n",
    "\n",
    "model_attention = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model_attention.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "history_attention = model_attention.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=32,\n",
    "    epochs=20,\n",
    "    validation_split=0.2,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(f\"LSTM with Attention Test MAE: {model_attention.evaluate(X_test, y_test, verbose=0)[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Autoencoders\n",
    "\n",
    "Autoencoders learn compressed representations by reconstructing inputs.\n",
    "\n",
    "### Architecture:\n",
    "- **Encoder**: Compress input to latent representation\n",
    "- **Latent space**: Bottleneck layer\n",
    "- **Decoder**: Reconstruct from latent representation\n",
    "\n",
    "### Applications:\n",
    "- Dimensionality reduction\n",
    "- Denoising\n",
    "- Anomaly detection\n",
    "- Feature learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST for autoencoder\n",
    "(X_train_ae, _), (X_test_ae, _) = keras.datasets.mnist.load_data()\n",
    "X_train_ae = X_train_ae.astype('float32') / 255.0\n",
    "X_test_ae = X_test_ae.astype('float32') / 255.0\n",
    "X_train_ae = X_train_ae.reshape(-1, 784)\n",
    "X_test_ae = X_test_ae.reshape(-1, 784)\n",
    "\n",
    "# Build autoencoder\n",
    "latent_dim = 32\n",
    "\n",
    "# Encoder\n",
    "encoder_input = keras.Input(shape=(784,))\n",
    "x = layers.Dense(128, activation='relu')(encoder_input)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "latent = layers.Dense(latent_dim, activation='relu', name='latent')(x)\n",
    "\n",
    "encoder = keras.Model(encoder_input, latent, name='encoder')\n",
    "\n",
    "# Decoder\n",
    "decoder_input = keras.Input(shape=(latent_dim,))\n",
    "x = layers.Dense(64, activation='relu')(decoder_input)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "decoder_output = layers.Dense(784, activation='sigmoid')(x)\n",
    "\n",
    "decoder = keras.Model(decoder_input, decoder_output, name='decoder')\n",
    "\n",
    "# Autoencoder\n",
    "autoencoder_input = keras.Input(shape=(784,))\n",
    "encoded = encoder(autoencoder_input)\n",
    "decoded = decoder(encoded)\n",
    "\n",
    "autoencoder = keras.Model(autoencoder_input, decoded, name='autoencoder')\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train autoencoder\n",
    "history_ae = autoencoder.fit(\n",
    "    X_train_ae, X_train_ae,  # Input = Output for autoencoder\n",
    "    batch_size=256,\n",
    "    epochs=10,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Visualize reconstructions\n",
    "n_samples = 10\n",
    "decoded_imgs = autoencoder.predict(X_test_ae[:n_samples], verbose=0)\n",
    "\n",
    "fig, axes = plt.subplots(2, n_samples, figsize=(15, 4))\n",
    "\n",
    "for i in range(n_samples):\n",
    "    # Original\n",
    "    axes[0, i].imshow(X_test_ae[i].reshape(28, 28), cmap='gray')\n",
    "    axes[0, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[0, i].set_ylabel('Original', fontsize=12)\n",
    "    \n",
    "    # Reconstructed\n",
    "    axes[1, i].imshow(decoded_imgs[i].reshape(28, 28), cmap='gray')\n",
    "    axes[1, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[1, i].set_ylabel('Reconstructed', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Latent dimension: {latent_dim}\")\n",
    "print(f\"Compression: {784} → {latent_dim} (x{784/latent_dim:.1f} smaller)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Sequence-to-Sequence (Seq2Seq)\n",
    "\n",
    "Seq2Seq models transform one sequence into another:\n",
    "\n",
    "### Architecture:\n",
    "1. **Encoder**: Processes input sequence → context vector\n",
    "2. **Decoder**: Generates output sequence from context\n",
    "\n",
    "### Applications:\n",
    "- Machine translation\n",
    "- Text summarization\n",
    "- Chatbots\n",
    "- Speech recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple seq2seq example: reverse sequences\n",
    "def generate_seq2seq_data(n_samples=10000, seq_length=10):\n",
    "    \"\"\"\n",
    "    Generate sequences where target is reversed input.\n",
    "    \"\"\"\n",
    "    X = np.random.randint(0, 10, (n_samples, seq_length))\n",
    "    y = np.flip(X, axis=1)\n",
    "    return X, y\n",
    "\n",
    "X_seq, y_seq = generate_seq2seq_data()\n",
    "X_seq_train, X_seq_test = X_seq[:8000], X_seq[8000:]\n",
    "y_seq_train, y_seq_test = y_seq[:8000], y_seq[8000:]\n",
    "\n",
    "# One-hot encode\n",
    "def one_hot_encode_seq(X, num_classes=10):\n",
    "    return tf.keras.utils.to_categorical(X, num_classes)\n",
    "\n",
    "X_seq_train_oh = one_hot_encode_seq(X_seq_train)\n",
    "y_seq_train_oh = one_hot_encode_seq(y_seq_train)\n",
    "X_seq_test_oh = one_hot_encode_seq(X_seq_test)\n",
    "y_seq_test_oh = one_hot_encode_seq(y_seq_test)\n",
    "\n",
    "print(f\"Input shape: {X_seq_train_oh.shape}\")\n",
    "print(f\"Output shape: {y_seq_train_oh.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple seq2seq model\n",
    "model_seq2seq = keras.Sequential([\n",
    "    layers.LSTM(128, input_shape=(10, 10), return_sequences=True),\n",
    "    layers.LSTM(128, return_sequences=True),\n",
    "    layers.TimeDistributed(layers.Dense(10, activation='softmax'))\n",
    "])\n",
    "\n",
    "model_seq2seq.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history_seq2seq = model_seq2seq.fit(\n",
    "    X_seq_train_oh, y_seq_train_oh,\n",
    "    batch_size=128,\n",
    "    epochs=20,\n",
    "    validation_split=0.2,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "test_loss, test_acc = model_seq2seq.evaluate(X_seq_test_oh, y_seq_test_oh, verbose=0)\n",
    "print(f\"\\nSequence Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Test predictions\n",
    "sample_input = X_seq_test[0:5]\n",
    "sample_pred = model_seq2seq.predict(X_seq_test_oh[0:5], verbose=0)\n",
    "sample_pred = np.argmax(sample_pred, axis=-1)\n",
    "\n",
    "print(\"\\nExample predictions:\")\n",
    "for i in range(5):\n",
    "    print(f\"Input:  {sample_input[i]}\")\n",
    "    print(f\"Target: {y_seq_test[i]}\")\n",
    "    print(f\"Pred:   {sample_pred[i]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **RNNs** process sequential data with hidden state\n",
    "2. **LSTMs** solve vanishing gradient problem with gates\n",
    "3. **GRUs** are simpler, faster alternative to LSTMs\n",
    "4. **Attention** allows focusing on relevant parts\n",
    "5. **Autoencoders** learn compressed representations\n",
    "6. **Seq2Seq** transforms sequences (translation, summarization)\n",
    "7. **Transformers** (not covered) use self-attention exclusively\n",
    "8. Choose architecture based on task requirements\n",
    "\n",
    "## When to Use What?\n",
    "\n",
    "**LSTMs/GRUs:**\n",
    "- Time series forecasting\n",
    "- Text generation\n",
    "- Speech recognition\n",
    "- Video analysis\n",
    "\n",
    "**Attention/Transformers:**\n",
    "- Machine translation\n",
    "- Question answering\n",
    "- Document understanding\n",
    "- Modern NLP tasks\n",
    "\n",
    "**Autoencoders:**\n",
    "- Dimensionality reduction\n",
    "- Anomaly detection\n",
    "- Denoising\n",
    "- Feature learning\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. **Time Series**: Forecast stock prices using LSTM\n",
    "2. **Sentiment Analysis**: Classify movie reviews with LSTM\n",
    "3. **Attention Visualization**: Visualize attention weights\n",
    "4. **Variational Autoencoder**: Implement VAE for generation\n",
    "5. **Bidirectional LSTM**: Compare with unidirectional\n",
    "6. **Custom Seq2Seq**: Build chatbot with seq2seq\n",
    "7. **Transformer**: Implement simple transformer encoder\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "You've completed Week 6! Continue with:\n",
    "- Week 7: Language - Advanced NLP and Transformers\n",
    "- Explore: GANs, Reinforcement Learning, Graph Neural Networks\n",
    "- Read: Attention Is All You Need paper\n",
    "- Practice: Build projects on Hugging Face\n",
    "\n",
    "Congratulations! You now have a comprehensive understanding of neural network architectures!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
