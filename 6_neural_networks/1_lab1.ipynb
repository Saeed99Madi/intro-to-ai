{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Introduction to Neural Networks\n",
    "\n",
    "Welcome to the exciting world of neural networks! In this lab, we'll build neural networks from scratch to understand how they work, then use modern frameworks for practical applications.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will:\n",
    "- Understand the mathematical foundations of neural networks\n",
    "- Implement forward propagation from scratch\n",
    "- Implement backpropagation from scratch\n",
    "- Build multi-layer perceptrons (MLPs)\n",
    "- Use activation functions effectively\n",
    "- Train neural networks with gradient descent\n",
    "- Use Keras and PyTorch for practical applications\n",
    "- Recognize and solve handwritten digits (MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Tuple\n",
    "import pandas as pd\n",
    "\n",
    "# Deep learning frameworks\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Set style and seeds\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"GPU available (TF): {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
    "print(f\"GPU available (PyTorch): {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Artificial Neuron\n",
    "\n",
    "An artificial neuron is inspired by biological neurons. It:\n",
    "1. Receives inputs $x_1, x_2, ..., x_n$\n",
    "2. Applies weights $w_1, w_2, ..., w_n$ and bias $b$\n",
    "3. Computes weighted sum: $z = \\sum_{i} w_i x_i + b$\n",
    "4. Applies activation function: $a = \\sigma(z)$\n",
    "\n",
    "### Common Activation Functions\n",
    "\n",
    "**Sigmoid:** $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ (outputs 0 to 1)\n",
    "\n",
    "**Tanh:** $\\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$ (outputs -1 to 1)\n",
    "\n",
    "**ReLU:** $\\text{ReLU}(z) = \\max(0, z)$ (most popular for hidden layers)\n",
    "\n",
    "**Softmax:** $\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}}$ (for multi-class output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions\n",
    "class ActivationFunctions:\n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(z):\n",
    "        s = ActivationFunctions.sigmoid(z)\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh(z):\n",
    "        return np.tanh(z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh_derivative(z):\n",
    "        return 1 - np.tanh(z) ** 2\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu(z):\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu_derivative(z):\n",
    "        return (z > 0).astype(float)\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "# Visualize activation functions\n",
    "z = np.linspace(-5, 5, 100)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Sigmoid\n",
    "axes[0, 0].plot(z, ActivationFunctions.sigmoid(z), linewidth=2, label='sigmoid')\n",
    "axes[0, 0].plot(z, ActivationFunctions.sigmoid_derivative(z), linewidth=2, label='derivative')\n",
    "axes[0, 0].set_title('Sigmoid')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Tanh\n",
    "axes[0, 1].plot(z, ActivationFunctions.tanh(z), linewidth=2, label='tanh')\n",
    "axes[0, 1].plot(z, ActivationFunctions.tanh_derivative(z), linewidth=2, label='derivative')\n",
    "axes[0, 1].set_title('Tanh')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# ReLU\n",
    "axes[1, 0].plot(z, ActivationFunctions.relu(z), linewidth=2, label='ReLU')\n",
    "axes[1, 0].plot(z, ActivationFunctions.relu_derivative(z), linewidth=2, label='derivative')\n",
    "axes[1, 0].set_title('ReLU')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Comparison\n",
    "axes[1, 1].plot(z, ActivationFunctions.sigmoid(z), linewidth=2, label='Sigmoid')\n",
    "axes[1, 1].plot(z, ActivationFunctions.tanh(z), linewidth=2, label='Tanh')\n",
    "axes[1, 1].plot(z, ActivationFunctions.relu(z), linewidth=2, label='ReLU')\n",
    "axes[1, 1].set_title('Comparison')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Properties:\")\n",
    "print(\"- Sigmoid: Smooth, bounded [0,1], prone to vanishing gradients\")\n",
    "print(\"- Tanh: Smooth, bounded [-1,1], zero-centered\")\n",
    "print(\"- ReLU: Simple, no upper bound, dead ReLU problem possible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Simple Neural Network from Scratch\n",
    "\n",
    "Let's build a 2-layer neural network (1 hidden layer) from scratch.\n",
    "\n",
    "### Architecture:\n",
    "- Input layer: $n$ features\n",
    "- Hidden layer: $h$ neurons with ReLU activation\n",
    "- Output layer: $k$ neurons with softmax activation\n",
    "\n",
    "### Forward Propagation:\n",
    "1. $Z^{[1]} = W^{[1]} X + b^{[1]}$\n",
    "2. $A^{[1]} = \\text{ReLU}(Z^{[1]})$\n",
    "3. $Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$\n",
    "4. $A^{[2]} = \\text{softmax}(Z^{[2]})$\n",
    "\n",
    "### Loss Function (Cross-Entropy):\n",
    "$$L = -\\frac{1}{m} \\sum_{i=1}^{m} \\sum_{k=1}^{K} y_k^{(i)} \\log(\\hat{y}_k^{(i)})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    Simple feedforward neural network from scratch.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int,\n",
    "                learning_rate: float = 0.01):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Initialize weights (He initialization for ReLU)\n",
    "        self.W1 = np.random.randn(hidden_size, input_size) * np.sqrt(2.0 / input_size)\n",
    "        self.b1 = np.zeros((hidden_size, 1))\n",
    "        self.W2 = np.random.randn(output_size, hidden_size) * np.sqrt(2.0 / hidden_size)\n",
    "        self.b2 = np.zeros((output_size, 1))\n",
    "        \n",
    "        self.loss_history = []\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "        X shape: (input_size, m) where m is batch size\n",
    "        \"\"\"\n",
    "        # Layer 1\n",
    "        self.Z1 = self.W1.dot(X) + self.b1\n",
    "        self.A1 = ActivationFunctions.relu(self.Z1)\n",
    "        \n",
    "        # Layer 2\n",
    "        self.Z2 = self.W2.dot(self.A1) + self.b2\n",
    "        self.A2 = ActivationFunctions.softmax(self.Z2.T).T\n",
    "        \n",
    "        return self.A2\n",
    "    \n",
    "    def compute_loss(self, Y, A2):\n",
    "        \"\"\"\n",
    "        Compute cross-entropy loss.\n",
    "        \"\"\"\n",
    "        m = Y.shape[1]\n",
    "        loss = -np.sum(Y * np.log(A2 + 1e-8)) / m\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, X, Y):\n",
    "        \"\"\"\n",
    "        Backward propagation.\n",
    "        \"\"\"\n",
    "        m = X.shape[1]\n",
    "        \n",
    "        # Output layer gradients\n",
    "        dZ2 = self.A2 - Y\n",
    "        dW2 = (1/m) * dZ2.dot(self.A1.T)\n",
    "        db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "        \n",
    "        # Hidden layer gradients\n",
    "        dA1 = self.W2.T.dot(dZ2)\n",
    "        dZ1 = dA1 * ActivationFunctions.relu_derivative(self.Z1)\n",
    "        dW1 = (1/m) * dZ1.dot(X.T)\n",
    "        db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "        \n",
    "        return dW1, db1, dW2, db2\n",
    "    \n",
    "    def update_parameters(self, dW1, db1, dW2, db2):\n",
    "        \"\"\"\n",
    "        Update weights using gradient descent.\n",
    "        \"\"\"\n",
    "        self.W1 -= self.learning_rate * dW1\n",
    "        self.b1 -= self.learning_rate * db1\n",
    "        self.W2 -= self.learning_rate * dW2\n",
    "        self.b2 -= self.learning_rate * db2\n",
    "    \n",
    "    def train(self, X, Y, epochs: int = 1000, print_every: int = 100):\n",
    "        \"\"\"\n",
    "        Train the network.\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            A2 = self.forward(X)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = self.compute_loss(Y, A2)\n",
    "            self.loss_history.append(loss)\n",
    "            \n",
    "            # Backward pass\n",
    "            dW1, db1, dW2, db2 = self.backward(X, Y)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.update_parameters(dW1, db1, dW2, db2)\n",
    "            \n",
    "            if (epoch + 1) % print_every == 0:\n",
    "                predictions = np.argmax(A2, axis=0)\n",
    "                labels = np.argmax(Y, axis=0)\n",
    "                accuracy = np.mean(predictions == labels)\n",
    "                print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions.\n",
    "        \"\"\"\n",
    "        A2 = self.forward(X)\n",
    "        return np.argmax(A2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on synthetic data\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate data\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,\n",
    "                          n_redundant=5, n_classes=3, random_state=42)\n",
    "\n",
    "# Split and scale\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert to one-hot encoding\n",
    "def one_hot_encode(y, num_classes):\n",
    "    m = len(y)\n",
    "    one_hot = np.zeros((num_classes, m))\n",
    "    one_hot[y, np.arange(m)] = 1\n",
    "    return one_hot\n",
    "\n",
    "Y_train = one_hot_encode(y_train, 3)\n",
    "Y_test = one_hot_encode(y_test, 3)\n",
    "\n",
    "# Transpose for our implementation\n",
    "X_train_T = X_train.T\n",
    "X_test_T = X_test.T\n",
    "\n",
    "# Create and train network\n",
    "nn = NeuralNetwork(input_size=20, hidden_size=64, output_size=3, learning_rate=0.1)\n",
    "nn.train(X_train_T, Y_train, epochs=1000, print_every=200)\n",
    "\n",
    "# Evaluate\n",
    "predictions = nn.predict(X_test_T)\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Plot loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(nn.loss_history)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Time')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Understanding Backpropagation\n",
    "\n",
    "Backpropagation uses the chain rule to compute gradients efficiently.\n",
    "\n",
    "### Chain Rule Example:\n",
    "If $L = f(g(h(x)))$, then:\n",
    "$$\\frac{dL}{dx} = \\frac{dL}{df} \\cdot \\frac{df}{dg} \\cdot \\frac{dg}{dh} \\cdot \\frac{dh}{dx}$$\n",
    "\n",
    "### For Our Network:\n",
    "We compute gradients layer by layer, moving backward:\n",
    "1. Output layer: $\\frac{\\partial L}{\\partial W^{[2]}}$, $\\frac{\\partial L}{\\partial b^{[2]}}$\n",
    "2. Hidden layer: $\\frac{\\partial L}{\\partial W^{[1]}}$, $\\frac{\\partial L}{\\partial b^{[1]}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize gradient flow\n",
    "def visualize_gradient_flow(nn, X_sample):\n",
    "    \"\"\"\n",
    "    Visualize how gradients flow through the network.\n",
    "    \"\"\"\n",
    "    # Forward pass\n",
    "    nn.forward(X_sample)\n",
    "    \n",
    "    # Get weight magnitudes\n",
    "    W1_mag = np.abs(nn.W1).mean()\n",
    "    W2_mag = np.abs(nn.W2).mean()\n",
    "    \n",
    "    # Get activation magnitudes\n",
    "    A1_mag = np.abs(nn.A1).mean()\n",
    "    A2_mag = np.abs(nn.A2).mean()\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Weight magnitudes\n",
    "    axes[0].bar(['W1', 'W2'], [W1_mag, W2_mag])\n",
    "    axes[0].set_ylabel('Mean Absolute Weight')\n",
    "    axes[0].set_title('Weight Magnitudes')\n",
    "    axes[0].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Activation magnitudes\n",
    "    axes[1].bar(['Input', 'Hidden (A1)', 'Output (A2)'], \n",
    "               [np.abs(X_sample).mean(), A1_mag, A2_mag])\n",
    "    axes[1].set_ylabel('Mean Absolute Activation')\n",
    "    axes[1].set_title('Activation Magnitudes')\n",
    "    axes[1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_gradient_flow(nn, X_train_T[:, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: MNIST with Keras\n",
    "\n",
    "Now let's use Keras to build a neural network for the classic MNIST handwritten digit dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST\n",
    "(X_train_mnist, y_train_mnist), (X_test_mnist, y_test_mnist) = keras.datasets.mnist.load_data()\n",
    "\n",
    "print(f\"Training data shape: {X_train_mnist.shape}\")\n",
    "print(f\"Test data shape: {X_test_mnist.shape}\")\n",
    "\n",
    "# Visualize samples\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    ax.imshow(X_train_mnist[i], cmap='gray')\n",
    "    ax.set_title(f\"Label: {y_train_mnist[i]}\")\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "X_train_mnist = X_train_mnist.reshape(-1, 784) / 255.0  # Flatten and normalize\n",
    "X_test_mnist = X_test_mnist.reshape(-1, 784) / 255.0\n",
    "\n",
    "# Build model with Keras\n",
    "model_keras = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(784,)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile\n",
    "model_keras.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Print model summary\n",
    "model_keras.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "history = model_keras.fit(\n",
    "    X_train_mnist, y_train_mnist,\n",
    "    batch_size=128,\n",
    "    epochs=10,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "test_loss, test_acc = model_keras.evaluate(X_test_mnist, y_test_mnist, verbose=0)\n",
    "print(f\"\\nTest Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history.history['loss'], label='Training')\n",
    "axes[0].plot(history.history['val_loss'], label='Validation')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Loss Over Time')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history.history['accuracy'], label='Training')\n",
    "axes[1].plot(history.history['val_accuracy'], label='Validation')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Accuracy Over Time')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "predictions = model_keras.predict(X_test_mnist[:20])\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "fig, axes = plt.subplots(2, 10, figsize=(15, 3))\n",
    "for i in range(20):\n",
    "    ax = axes[i // 10, i % 10]\n",
    "    ax.imshow(X_test_mnist[i].reshape(28, 28), cmap='gray')\n",
    "    color = 'green' if predicted_labels[i] == y_test_mnist[i] else 'red'\n",
    "    ax.set_title(f\"P:{predicted_labels[i]}\\nA:{y_test_mnist[i]}\", color=color, fontsize=8)\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Green = Correct, Red = Incorrect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: MNIST with PyTorch\n",
    "\n",
    "Now let's implement the same network in PyTorch for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model in PyTorch\n",
    "class MNISTNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # No softmax (included in loss)\n",
    "        return x\n",
    "\n",
    "# Create model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_torch = MNISTNet().to(device)\n",
    "\n",
    "print(model_torch)\n",
    "print(f\"\\nTraining on: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for PyTorch\n",
    "X_train_torch = torch.FloatTensor(X_train_mnist)\n",
    "y_train_torch = torch.LongTensor(y_train_mnist)\n",
    "X_test_torch = torch.FloatTensor(X_test_mnist)\n",
    "y_test_torch = torch.LongTensor(y_test_mnist)\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = TensorDataset(X_train_torch, y_train_torch)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_torch.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model_torch.train()\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model_torch(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        epoch_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    accuracy = correct / total\n",
    "    train_losses.append(avg_loss)\n",
    "    train_accuracies.append(accuracy)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Evaluate\n",
    "model_torch.eval()\n",
    "with torch.no_grad():\n",
    "    X_test_device = X_test_torch.to(device)\n",
    "    y_test_device = y_test_torch.to(device)\n",
    "    outputs = model_torch(X_test_device)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    test_accuracy = (predicted == y_test_device).sum().item() / y_test_device.size(0)\n",
    "    print(f\"\\nTest Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PyTorch training\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(train_losses)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('PyTorch Training Loss')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(train_accuracies)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('PyTorch Training Accuracy')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Neural networks** are composed of layers of artificial neurons\n",
    "2. **Forward propagation** computes predictions layer by layer\n",
    "3. **Backpropagation** efficiently computes gradients using the chain rule\n",
    "4. **Activation functions** introduce non-linearity (ReLU is most common)\n",
    "5. **Weight initialization** matters for training stability\n",
    "6. **Learning rate** controls update step size\n",
    "7. **Keras** provides high-level API for quick prototyping\n",
    "8. **PyTorch** offers more flexibility and control\n",
    "9. Both frameworks handle **GPU acceleration** automatically\n",
    "10. **MNIST** is a great starting point for image classification\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. **Activation Comparison**: Train networks with different activation functions (sigmoid, tanh, ReLU) and compare\n",
    "2. **Architecture Search**: Try different numbers of layers and neurons\n",
    "3. **Learning Rate**: Experiment with different learning rates (0.001, 0.01, 0.1, 1.0)\n",
    "4. **Batch Size**: Compare training with different batch sizes\n",
    "5. **Regularization**: Add L2 regularization to prevent overfitting\n",
    "6. **Dropout**: Implement dropout in both frameworks\n",
    "7. **Custom Dataset**: Apply these techniques to Fashion-MNIST or CIFAR-10\n",
    "8. **Visualization**: Visualize learned weights in the first layer\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In Lab 2, we'll explore:\n",
    "- Advanced optimization techniques\n",
    "- Batch normalization\n",
    "- Dropout and regularization\n",
    "- Deep network architectures\n",
    "- Training best practices\n",
    "\n",
    "Great job! You've built your first neural networks from scratch and with modern frameworks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
