{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Deep Learning Fundamentals\n",
    "\n",
    "In this lab, we'll explore the techniques that make training deep neural networks possible and effective. You'll learn about normalization, regularization, optimization, and architectural patterns that are essential for modern deep learning.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will:\n",
    "- Understand and apply batch normalization\n",
    "- Implement dropout for regularization\n",
    "- Use advanced optimizers (Adam, RMSprop, AdamW)\n",
    "- Apply learning rate scheduling\n",
    "- Handle vanishing/exploding gradients\n",
    "- Build deep networks with residual connections\n",
    "- Master weight initialization strategies\n",
    "- Train robust deep learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping, ModelCheckpoint\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Batch Normalization\n",
    "\n",
    "**Batch Normalization** normalizes layer inputs during training, providing several benefits:\n",
    "- Faster training\n",
    "- Higher learning rates possible\n",
    "- Less sensitive to initialization\n",
    "- Acts as regularization\n",
    "\n",
    "### Algorithm:\n",
    "For a mini-batch $\\mathcal{B} = \\{x_1, ..., x_m\\}$:\n",
    "\n",
    "1. Compute batch mean: $\\mu_{\\mathcal{B}} = \\frac{1}{m} \\sum_{i=1}^{m} x_i$\n",
    "2. Compute batch variance: $\\sigma^2_{\\mathcal{B}} = \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\mu_{\\mathcal{B}})^2$\n",
    "3. Normalize: $\\hat{x}_i = \\frac{x_i - \\mu_{\\mathcal{B}}}{\\sqrt{\\sigma^2_{\\mathcal{B}} + \\epsilon}}$\n",
    "4. Scale and shift: $y_i = \\gamma \\hat{x}_i + \\beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Fashion-MNIST\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# Normalize\n",
    "X_train = X_train.reshape(-1, 784) / 255.0\n",
    "X_test = X_test.reshape(-1, 784) / 255.0\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model WITHOUT batch normalization\n",
    "model_no_bn = keras.Sequential([\n",
    "    layers.Dense(256, activation='relu', input_shape=(784,)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Model WITH batch normalization\n",
    "model_with_bn = keras.Sequential([\n",
    "    layers.Dense(256, input_shape=(784,)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Dense(128),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Dense(64),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile both\n",
    "for model in [model_no_bn, model_with_bn]:\n",
    "    model.compile(\n",
    "        optimizer='sgd',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train both models\n",
    "history_no_bn = model_no_bn.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=128,\n",
    "    epochs=15,\n",
    "    validation_split=0.1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "history_with_bn = model_with_bn.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=128,\n",
    "    epochs=15,\n",
    "    validation_split=0.1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Compare\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(history_no_bn.history['loss'], label='Without BN')\n",
    "axes[0].plot(history_with_bn.history['loss'], label='With BN')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Training Loss')\n",
    "axes[0].set_title('Training Loss Comparison')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(history_no_bn.history['val_accuracy'], label='Without BN')\n",
    "axes[1].plot(history_with_bn.history['val_accuracy'], label='With BN')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Validation Accuracy')\n",
    "axes[1].set_title('Validation Accuracy Comparison')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Batch Normalization benefits:\")\n",
    "print(\"- Faster convergence\")\n",
    "print(\"- More stable training\")\n",
    "print(\"- Better final accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Dropout Regularization\n",
    "\n",
    "**Dropout** randomly drops neurons during training to prevent overfitting.\n",
    "\n",
    "- During training: Randomly set activations to 0 with probability $p$\n",
    "- During inference: Use all neurons (scaled appropriately)\n",
    "\n",
    "### Benefits:\n",
    "- Prevents co-adaptation of neurons\n",
    "- Ensemble effect (training many sub-networks)\n",
    "- Strong regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with dropout\n",
    "model_dropout = keras.Sequential([\n",
    "    layers.Dense(256, activation='relu', input_shape=(784,)),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model_dropout.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Model without dropout (for comparison)\n",
    "model_no_dropout = keras.Sequential([\n",
    "    layers.Dense(256, activation='relu', input_shape=(784,)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model_no_dropout.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train both\n",
    "history_dropout = model_dropout.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=128,\n",
    "    epochs=20,\n",
    "    validation_split=0.1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "history_no_dropout = model_no_dropout.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=128,\n",
    "    epochs=20,\n",
    "    validation_split=0.1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Compare overfitting\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Without dropout - training vs validation\n",
    "axes[0].plot(history_no_dropout.history['accuracy'], label='Training')\n",
    "axes[0].plot(history_no_dropout.history['val_accuracy'], label='Validation')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Without Dropout (Overfitting)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# With dropout - training vs validation\n",
    "axes[1].plot(history_dropout.history['accuracy'], label='Training')\n",
    "axes[1].plot(history_dropout.history['val_accuracy'], label='Validation')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('With Dropout (Better Generalization)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice how dropout reduces the gap between training and validation accuracy!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Advanced Optimizers\n",
    "\n",
    "Modern optimizers adapt learning rates during training:\n",
    "\n",
    "### SGD with Momentum\n",
    "$$v_t = \\beta v_{t-1} + (1-\\beta) \\nabla L$$\n",
    "$$\\theta_t = \\theta_{t-1} - \\alpha v_t$$\n",
    "\n",
    "### RMSprop\n",
    "$$s_t = \\beta s_{t-1} + (1-\\beta) (\\nabla L)^2$$\n",
    "$$\\theta_t = \\theta_{t-1} - \\frac{\\alpha}{\\sqrt{s_t + \\epsilon}} \\nabla L$$\n",
    "\n",
    "### Adam (Adaptive Moment Estimation)\n",
    "Combines momentum and RMSprop:\n",
    "$$m_t = \\beta_1 m_{t-1} + (1-\\beta_1) \\nabla L$$\n",
    "$$v_t = \\beta_2 v_{t-1} + (1-\\beta_2) (\\nabla L)^2$$\n",
    "$$\\hat{m}_t = \\frac{m_t}{1-\\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1-\\beta_2^t}$$\n",
    "$$\\theta_t = \\theta_{t-1} - \\frac{\\alpha}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare optimizers\n",
    "optimizers_to_test = {\n",
    "    'SGD': keras.optimizers.SGD(learning_rate=0.01),\n",
    "    'SGD + Momentum': keras.optimizers.SGD(learning_rate=0.01, momentum=0.9),\n",
    "    'RMSprop': keras.optimizers.RMSprop(learning_rate=0.001),\n",
    "    'Adam': keras.optimizers.Adam(learning_rate=0.001),\n",
    "    'AdamW': keras.optimizers.AdamW(learning_rate=0.001)\n",
    "}\n",
    "\n",
    "histories = {}\n",
    "\n",
    "for name, optimizer in optimizers_to_test.items():\n",
    "    print(f\"Training with {name}...\")\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=(784,)),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train[:10000], y_train[:10000],  # Subset for speed\n",
    "        batch_size=128,\n",
    "        epochs=20,\n",
    "        validation_split=0.2,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    histories[name] = history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for name, history in histories.items():\n",
    "    axes[0].plot(history.history['loss'], label=name)\n",
    "    axes[1].plot(history.history['val_accuracy'], label=name)\n",
    "\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Training Loss')\n",
    "axes[0].set_title('Optimizer Comparison - Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Validation Accuracy')\n",
    "axes[1].set_title('Optimizer Comparison - Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTypical observations:\")\n",
    "print(\"- Adam converges fastest (good default choice)\")\n",
    "print(\"- SGD with momentum is more stable than plain SGD\")\n",
    "print(\"- AdamW adds weight decay regularization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Learning Rate Scheduling\n",
    "\n",
    "Learning rate schedules adjust the learning rate during training:\n",
    "\n",
    "- **Step Decay**: Reduce LR by factor every N epochs\n",
    "- **Exponential Decay**: $\\alpha_t = \\alpha_0 e^{-kt}$\n",
    "- **Cosine Annealing**: $\\alpha_t = \\alpha_{min} + \\frac{1}{2}(\\alpha_{max} - \\alpha_{min})(1 + \\cos(\\frac{t\\pi}{T}))$\n",
    "- **Warmup**: Start with small LR, gradually increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate schedules\n",
    "def step_decay(epoch, lr):\n",
    "    drop_rate = 0.5\n",
    "    epochs_drop = 5\n",
    "    return lr * (drop_rate ** (epoch // epochs_drop))\n",
    "\n",
    "def exponential_decay(epoch, lr):\n",
    "    k = 0.1\n",
    "    return lr * np.exp(-k * epoch)\n",
    "\n",
    "def cosine_annealing(epoch, lr, total_epochs=20):\n",
    "    min_lr = 1e-5\n",
    "    max_lr = 0.001\n",
    "    return min_lr + 0.5 * (max_lr - min_lr) * (1 + np.cos(np.pi * epoch / total_epochs))\n",
    "\n",
    "# Visualize schedules\n",
    "epochs = np.arange(0, 20)\n",
    "initial_lr = 0.001\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(epochs, [step_decay(e, initial_lr) for e in epochs], label='Step Decay', linewidth=2)\n",
    "plt.plot(epochs, [exponential_decay(e, initial_lr) for e in epochs], label='Exponential Decay', linewidth=2)\n",
    "plt.plot(epochs, [cosine_annealing(e, initial_lr) for e in epochs], label='Cosine Annealing', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Learning Rate Schedules')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with learning rate schedule\n",
    "model_lr_schedule = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(784,)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model_lr_schedule.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Add callbacks\n",
    "lr_scheduler = LearningRateScheduler(lambda epoch, lr: cosine_annealing(epoch, lr))\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "history_scheduled = model_lr_schedule.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=128,\n",
    "    epochs=20,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[lr_scheduler, early_stopping],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(f\"Final validation accuracy: {max(history_scheduled.history['val_accuracy']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Weight Initialization\n",
    "\n",
    "Proper initialization is crucial for training deep networks:\n",
    "\n",
    "### Xavier/Glorot Initialization (for Sigmoid/Tanh)\n",
    "$$W \\sim \\mathcal{N}(0, \\sqrt{\\frac{2}{n_{in} + n_{out}}})$$\n",
    "\n",
    "### He Initialization (for ReLU)\n",
    "$$W \\sim \\mathcal{N}(0, \\sqrt{\\frac{2}{n_{in}}})$$\n",
    "\n",
    "### Why it matters:\n",
    "- Poor initialization → vanishing/exploding gradients\n",
    "- Good initialization → stable training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare initializations\n",
    "initializers = {\n",
    "    'Zeros': keras.initializers.Zeros(),\n",
    "    'Random Normal': keras.initializers.RandomNormal(stddev=0.01),\n",
    "    'Xavier': keras.initializers.GlorotNormal(),\n",
    "    'He': keras.initializers.HeNormal()\n",
    "}\n",
    "\n",
    "init_histories = {}\n",
    "\n",
    "for name, initializer in initializers.items():\n",
    "    if name == 'Zeros':  # Skip zeros (won't learn)\n",
    "        continue\n",
    "        \n",
    "    print(f\"Training with {name} initialization...\")\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(128, activation='relu', kernel_initializer=initializer, input_shape=(784,)),\n",
    "        layers.Dense(64, activation='relu', kernel_initializer=initializer),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train[:5000], y_train[:5000],\n",
    "        batch_size=128,\n",
    "        epochs=10,\n",
    "        validation_split=0.2,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    init_histories[name] = history\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "for name, history in init_histories.items():\n",
    "    plt.plot(history.history['val_accuracy'], label=name, linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.title('Effect of Weight Initialization')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nHe initialization typically works best for ReLU networks!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Residual Connections (ResNet-style)\n",
    "\n",
    "**Residual connections** (skip connections) help train very deep networks:\n",
    "\n",
    "$$y = F(x) + x$$\n",
    "\n",
    "Where $F(x)$ is the residual mapping to be learned.\n",
    "\n",
    "### Benefits:\n",
    "- Easier gradient flow\n",
    "- Enables training of very deep networks (100+ layers)\n",
    "- Identity mapping as fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual block in Keras (Functional API)\n",
    "def residual_block(x, units):\n",
    "    \"\"\"\n",
    "    Create a residual block.\n",
    "    \"\"\"\n",
    "    # Main path\n",
    "    out = layers.Dense(units)(x)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.Activation('relu')(out)\n",
    "    out = layers.Dense(units)(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    \n",
    "    # Shortcut connection\n",
    "    if x.shape[-1] != units:\n",
    "        x = layers.Dense(units)(x)  # Project to match dimensions\n",
    "    \n",
    "    # Add and activate\n",
    "    out = layers.Add()([out, x])\n",
    "    out = layers.Activation('relu')(out)\n",
    "    \n",
    "    return out\n",
    "\n",
    "# Build ResNet-style model\n",
    "inputs = keras.Input(shape=(784,))\n",
    "x = layers.Dense(128)(inputs)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation('relu')(x)\n",
    "\n",
    "# Add residual blocks\n",
    "x = residual_block(x, 128)\n",
    "x = residual_block(x, 128)\n",
    "x = residual_block(x, 64)\n",
    "\n",
    "# Output\n",
    "x = layers.Dropout(0.3)(x)\n",
    "outputs = layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "model_resnet = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model_resnet.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model_resnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ResNet model\n",
    "history_resnet = model_resnet.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=128,\n",
    "    epochs=15,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "test_loss, test_acc = model_resnet.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\nTest Accuracy with Residual Connections: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Complete Modern Architecture\n",
    "\n",
    "Let's build a complete model with all best practices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modern best-practice model\n",
    "def create_modern_model():\n",
    "    inputs = keras.Input(shape=(784,))\n",
    "    \n",
    "    # Initial layer\n",
    "    x = layers.Dense(256, kernel_initializer='he_normal')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    # Residual block 1\n",
    "    residual = x\n",
    "    x = layers.Dense(256, kernel_initializer='he_normal')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Dense(256, kernel_initializer='he_normal')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Add()([x, residual])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    # Transition\n",
    "    x = layers.Dense(128, kernel_initializer='he_normal')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    # Residual block 2\n",
    "    residual = x\n",
    "    x = layers.Dense(128, kernel_initializer='he_normal')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Dense(128, kernel_initializer='he_normal')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Add()([x, residual])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    # Output\n",
    "    outputs = layers.Dense(10, activation='softmax')(x)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Create and compile\n",
    "model_modern = create_modern_model()\n",
    "model_modern.compile(\n",
    "    optimizer=keras.optimizers.AdamW(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    LearningRateScheduler(lambda epoch, lr: cosine_annealing(epoch, lr, total_epochs=20)),\n",
    "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "    ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True)\n",
    "]\n",
    "\n",
    "# Train\n",
    "history_modern = model_modern.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=128,\n",
    "    epochs=20,\n",
    "    validation_split=0.1,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Final evaluation\n",
    "test_loss, test_acc = model_modern.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize final results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(history_modern.history['loss'], label='Training')\n",
    "axes[0].plot(history_modern.history['val_loss'], label='Validation')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Modern Architecture - Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(history_modern.history['accuracy'], label='Training')\n",
    "axes[1].plot(history_modern.history['val_accuracy'], label='Validation')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Modern Architecture - Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Batch Normalization** accelerates training and improves stability\n",
    "2. **Dropout** prevents overfitting through regularization\n",
    "3. **Adam optimizer** is a great default choice\n",
    "4. **Learning rate scheduling** improves convergence\n",
    "5. **He initialization** works well for ReLU networks\n",
    "6. **Residual connections** enable very deep networks\n",
    "7. **Combine techniques** for best results\n",
    "8. **Early stopping** prevents overtraining\n",
    "9. **Model checkpointing** saves best weights\n",
    "10. **Monitor validation metrics** to detect overfitting\n",
    "\n",
    "## Modern Deep Learning Recipe\n",
    "\n",
    "1. Use **He initialization** for weights\n",
    "2. Add **Batch Normalization** after dense layers (before activation)\n",
    "3. Use **ReLU** or variants (LeakyReLU, ELU) for hidden layers\n",
    "4. Add **Dropout** for regularization (0.2-0.5)\n",
    "5. Use **Adam** or **AdamW** optimizer\n",
    "6. Apply **learning rate scheduling** (cosine annealing)\n",
    "7. Use **residual connections** for deep networks\n",
    "8. Monitor with **early stopping**\n",
    "9. Save best model with **checkpointing**\n",
    "10. Use **data augmentation** when possible\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. **Ablation Study**: Remove techniques one-by-one and measure impact\n",
    "2. **Hyperparameter Search**: Find optimal dropout rate and layer sizes\n",
    "3. **Deep Network**: Build a 10-layer network with residual connections\n",
    "4. **Custom Scheduler**: Implement warmup + cosine decay\n",
    "5. **L2 Regularization**: Add kernel regularization and compare with dropout\n",
    "6. **Gradient Clipping**: Implement gradient clipping for stability\n",
    "7. **Mixed Precision**: Use mixed precision training for speed\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In Lab 3, we'll explore:\n",
    "- Convolutional Neural Networks (CNNs)\n",
    "- Computer vision applications\n",
    "- Transfer learning\n",
    "- Famous CNN architectures\n",
    "\n",
    "Excellent work! You now understand the key techniques for training deep neural networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
